<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
<head>
<title></title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<link rel="stylesheet" type="text/css" href="style.css" />
</head>

<body class="body">
<div class="main">
<div class="caption">Linux firewall своими руками</div>
<div class="content">
<small>
автор: gapsf@yandex.ru<br>
начато: 25.07.2010<br>
последнее изменение: 01.09.2013<br>
<a href="http://gapsf.github.io/hlf">http://gapsf.github.io/hlf</a><br>
<a href="http://handmade-linux-firewall.narod.ru">http://handmade-linux-firewall.narod.ru</a><br>
</small>

<h1>Содержание</h1>
<a style="margin-left:0em" href="#foreword">Вступление</a><br>
<br>
<a style="margin-left:0em" href="#part1">Часть 1. Концепции</a><br>
  <a style="margin-left:2em" href="#tasks">Что мы хотим</a><br>
  <a style="margin-left:2em" href="#networks">Схема сети</a><br>
    <a style="margin-left:4em" href="#networks_">Почему рассматривается именно этот вариант схемы?</a><br>
  <br>
  <a style="margin-left:2em" href="#problems">Проблемы</a><br>
  <a style="margin-left:2em" href="#conception">Основные концепции: потоки, линки, цепочки</a><br>
    <a style="margin-left:4em" href="#rules_listing">Листинг правил iptables для рассматриваемой схемы сети</a><br>
    <a style="margin-left:4em" href="#interfaces">Конфигурация интерфейсов шлюза</a><br>
  <br>
  <a style="margin-left:2em" href="#filter">Фильтрация</a><br>
  	 <a style="margin-left:4em" href="#filter_what">Сетевые сервисы или чего не должно быть на шлюзе/firewall'е</a><br>
     <a style="margin-left:4em" href="#access_control">Контроль доступа</a><br>
     <a style="margin-left:4em" href="#filter_rules">Цепочки фильтрации</a><br>
     		<a style="margin-left:6em" href="#filter_LAN_GW">Локальная сеть <-> шлюз</a><br>
     		<a style="margin-left:6em" href="#filter_INET_GW">Интернет <-> шлюз</a><br>
     		<a style="margin-left:6em" href="#filter_LAN_INET">Локальная сеть <-> Интернет</a><br>
     		<a style="margin-left:6em" href="#filter_INET_DMZ">Интернет <-> DMZ</a><br>
     		<a style="margin-left:6em" href="#filter_LAN_DMZ">Локальная сеть <-> DMZ</a><br>
     		<a style="margin-left:6em" href="#filter_ct">Фильтрация и отслеживание соединений (connection tracking)</a><br>
  <br>
  <a style="margin-left:2em" href="#routing">Маршрутизация (routing)</a><br>
    <a style="margin-left:4em" href="#routing_internet">Немного об устройстве сети Интернет. Автономные системы</a><br>
    <a style="margin-left:4em" href="#routing_conn">Варианты организации подключения к сети Интернет</a><br>
    <a style="margin-left:4em" href="#routing_gen">Маршрутизация при различных вариантах подключения</a><br>
	  <a style="margin-left:4em" href="#routing_example">Маршрутизация в рассматриваемом примере</a><br>
   		<a style="margin-left:6em" href="#routing1">Одно внешнее подключение (один провайдер)</a><br>
   		<a style="margin-left:6em" href="#routing2">Два внешних подключения (два разных провайдера)</a><br>
   		  <a style="margin-left:8em" href="#routing2policy">Policy routing</a><br>
   		  <a style="margin-left:8em" href="#routing2firewall">fwmark: NETFILTER+routing</a><br>
   		  <a style="margin-left:8em" href="#routing2steps">Этапы прохождения пакета через NETFILTER</a><br>
   		  <a style="margin-left:8em" href="#routing2in">Входящий трафик через двух провайдеров</a><br>
   <a style="margin-left:4em" href="#routingn_hwredundancy">Отказоустойчивость: резервирование шлюза</a><br>
  <br>
  <a style="margin-left:2em" href="#router_hardware">Немного о железе для шлюза</a><br>
  <br>
  <a style="margin-left:2em" href="#nat_share">Доступ из внутренних сетей к сети Интернет через один публичный адрес (NAT - трансляция адресов)</a><br>
  <br>
  <a style="margin-left:2em" href="#tc">Управление трафиком (traffic control: shaping, policing и т.д.)</a><br>
    <a style="margin-left:4em" href="#tc_egress">Управление исходящим трафиком: shaping, scheduling</a><br>
    <a style="margin-left:4em" href="#tc_ingress">Входящий трафик и почему им невозможно управлять. Policing</a><br>
  <br>
  <a style="margin-left:2em" href="#tc_netfilter">Управление трафиком в рассматриваемом примере</a><br>
    <a style="margin-left:4em" href="#tc_ifb">Объединённое управление трафиком на нескольких интерфейсах (IFB, IMQ - одна дисциплина на несколько интерфейсов)</a><br>
    <a style="margin-left:4em" href="#tc_downstream">Управление трафиком в downstream-канале</a><br>
    <a style="margin-left:6em" href="#tc_ifb0">Обьединение трафика на виртуальном IFB-интерфейсе</a><br>
    <a style="margin-left:6em" href="#tc_ifb0">Настройка дисциплины IFB-интерфейса и создание иерархии классов</a><br>
    <a style="margin-left:6em" href="#tc_ifb0_2isp">Иерархия классов для двух каналов/провайдеров</a><br>
    <a style="margin-left:6em" href="#tc_ifb0">Классификация трафика при помощи NETFILTER</a><br>
    <a style="margin-left:4em" href="#tc_squid">Управление трафиком и SQUID</a><br>
      <a style="margin-left:6em" href="#tc_squid_v1">Вариант №1: использовать возможности SQUID (версии 2.x) по управлению трафиком</a><br>
      <a style="margin-left:6em" href="#tc_squid_v2">Вариант №2: косвенное влияние на использование канала SQUID'ом плюс частичная "интеграция" с traffic control</a><br>
  <br>
  <a style="margin-left:2em" href="#ta">Учет (подсчет) трафика (traffic accounting)</a><br>
    <a style="margin-left:4em" href="#ta_conntrackd">Подсчет трафика с помощью conntrackd</a><br>
    <a style="margin-left:4em" href="#ta_ulogd">Подсчет трафика с помощью ulogd</a><br>
    <a style="margin-left:4em" href="#ta_squid">Учет трафика и SQUID</a><br>
  <br>
  <a style="margin-left:2em" href="#rw">Roadwarriors: мобильные клиенты. Доступ к внутренним сетям и VPN.</a><br>
    <a style="margin-left:4em" href="#rw_vpn">Функционирование  VPN с точки зрения фаервола</a><br>
    <a style="margin-left:4em" href="#rw_route">VPN и маршрутизация</a><br>
    <a style="margin-left:4em" href="#rw_choice">Выбор VPN</a><br>
  <br>
  <a style="margin-left:2em" href="#ipset">Использование <b>ipset</b>. Оптимизация набора правил</a><br>
  <a style="margin-left:2em" href="#iptables-restore"><b>iptables-restore</b>: атомарная загрузка набора правил</a><br>
  <a style="margin-left:2em" href="#nftables"><b>nftables</b>: преемник iptables и будущее Linux firewall</a><br>

<br><br>
<a style="margin-left:0em" href="#part2">Часть 2. Реализация</a><br>
  <a style="margin-left:2em" href="#shorewall">Чем не устроил Shorewall</a><br>
  <a style="margin-left:2em" href="#history">Предыдущие варианты реализации или "как не надо делать"</a><br>
  <a style="margin-left:2em" href="#sources">Исходные тексты фаервола</a><br>
  <a style="margin-left:2em" href="#why_perl">Почему perl, а не bash, python и т.д.?</a><br>
  <a style="margin-left:2em" href="#structure_db">Почему конфигурация не хранится в СУБД или LDAP</a><br>
  <br>
  <a style="margin-left:2em" href="#structure">Устройство фаервола</a><br>
		<a style="margin-left:4em" href="#structure_fs">Структура каталогов</a><br>
		  <a style="margin-left:6em" href="#structure_multiconfig">Поддержка нескольких конфигураций</a><br>
			<a style="margin-left:6em" href="#structure_multiconfig">Общие модули</a><br>
		<a style="margin-left:4em" href="#structure_cfg">Конфигурационные файлы</a><br>
  		<a style="margin-left:6em" href="#structure_cfg_net">Файл описания шлюза и сетей</a><br>
    		<a style="margin-left:8em" href="#structure_cfg_net_syntax">Синтаксис файла</a><br>
  		<a style="margin-left:6em" href="#structure_cfg_hosts">Файл описания правил для хостов и сетей</a><br>
    		<a style="margin-left:8em" href="#structure_cfg_hosts_syntax">Синтаксис файла</a><br>
    <a style="margin-left:4em" href="#structure_cfg_main">Конфигурация <b>main</b></a><br>
  		<a style="margin-left:6em" href="#structure_cfg_mian_startup">Процесс загрузки конфигурация main</a><br>
  		<a style="margin-left:6em" href="#structure_cfg_mian_funcs">Подробнее о функциях <b>firewall.pm</b></a><br>
<a style="margin-left:0em" href="#tc_faq">Вопросы без ответов</a><br>
<a style="margin-left:0em" href="#todo">TODO</a><br>

<br>
<br>
<h1 id="foreword">Вступление</h1>
В данном тексте показан один из возможных вариантов построения интернет-шлюза 
(небольшой организации) на базе GNU/Linux с функциями фаервола, маршрутизатора.
В качестве дистрибутива используется Debian GNU/Linux.
<p>
<b>Представлена целостная картина, основанная на реально эксплуатируемой системе.</b> 
<br>Изложен <b>один из возможных принципов организации правил</b> NETFILTER, позволяющие относительно легко:
</p>

<ul>
<li>разработать набор правил</li>
<li>осуществлять визуальный анализ их логики (по выводу <i>iptables -nvL</i>)</li>
<li>вносить изменения в конфигурацию фаервола</li>
<li>автоматизировать фаервол.</li>
</ul>
<p>
Также рассмотрены: маршрутизация (напр. работа через двух провайдеров),
управление трафиком, учет трафика, разъяснены некоторые неочевидные моменты, 
с которыми пришлось столкнуться на практике. 
</p>
<p>
Текст ориентирован на тех, кто хочет увидеть пример реально эксплуатирумого набора
правил iptabels/tc (плох он или хорош - это другой вопрос), разобраться в работе и использовании 
подсистем фаервола и управления трафиком в Linux с целью самостоятельного конфигурирования.
Подразумевается, что вы представляете, как продвинутый пользователь/администратор,
что такое iptables, знаете синтаксис команд iptables, в общих чертах возможности NETFILTER 
(подсистема ядра для перехвата пакетов и выполнения манипуляций над ними), 
сетевого стека Linux в целом, знаете о IP, UDP/TCP и т.д. и т.п.<br>
<b>Т.е. у вас есть знания о составных частях, но вы смутно представляете себе, как 
из этого всего собрать, нечто, решающее сетевые задачи в комплексе.</b>
</p>
<p>
Текст состоит из двух частей.
<br>В первой части описаны предлагаемые принципы организации правил для фаервола (NETFILTER) и 
управления трафиком (TRAFFIC CONTROL), а также взаимодействие этих подсистем. 
</p>

<p>
Во второй части представлена и описана реализация фаервола на
основе принципов, изложенных в первой части. <b>Представлен и описан реально эксплуатируемый 
исходный код</b>, который, посредством настройки 
и/или относительно легкой модифицикации, можно подстроить под конкретные нужды.
</p>
<p>
<b>Если необходимо готовое, широко используемое решение</b> - смотрите <b>Shorewall</b> 
<a target="_blank" href="http://www.shorewall.net">http://www.shorewall.net</a> (есть в большинстве дистрибутивов)
или специализированные дистрибутивы-фаерволы/маршрутизаторы: <b>IPCop</b>, <b>IPFire</b> и т.п.
(<a target="_blank" href="http://en.wikipedia.org/wiki/List_of_router_or_firewall_distributions">List of router or firewall distributions</a>)
</p>
<p>
<b>Если вы недостаточно понимаете возможности iptables - есть отличная документация</b> по возможностям 
и синтаксису, правда на английском - <br>
<b>"Iptables Tutorial"</b>, <a target="_blank" href="http://www.frozentux.net/iptables-tutorial/iptables-tutorial.html">http://www.frozentux.net/iptables-tutorial/iptables-tutorial.html</a>.
</p>

<p>
Есть русский перевод устаревшей версии: <a target="_blank" href="http://www.opennet.ru/docs/RUS/iptables/">http://www.opennet.ru/docs/RUS/iptables/</a>
</p>

<br>
<br>
<h1 id="part1">Часть 1. Концепции</h1>
<h2 id="tasks">Что мы хотим</h2>
<p>
Условимся подконтрольные нам сети называть <b>"внутренними сетями"</b>, а сети, которые
не находятся под нашим управлением - <b>"внешними сетями"</b>.
В простейшем случае: все что в офисе - "внутренняя сеть", Интернет - "большая внешняя сеть".
</p>
<p>
В рассматриваемом случае шлюз  <b>выполняет роль фаервола, маршрутизатора и</b>, возможно, 
<b>на нём функционируют какие-либо сетевые сервисы</b>. 
</p>

<p>
Итак, есть общая задача - организовать одновременный доступ нескольких 
узлов (хостов, компьютеров, устройств) из внутренних сетей к сети Интернет  
<b>через один публичный IP-адрес</b>. 
Также, необходимо организовать инфраструктуру под собственные web/ftp/smtp/pop-сервера 
с возможностью доступа к ним из внешних сетей.
Дополнительно, небходимо обеспечить возможность доступа к другим внешним сетям через 
дополнительные интерфейсы шлюза и доступ в сеть Интернет через второго провайдера.
</p>

<p>
Уточним требования к шлюзу и задачи которые мы хотим решить:
<ul>
		<li>задачи:</li>
		<ul>
		  <li>организовать доступ из внутренних сетей к сети Интернет (и другим внешним сетям) через один публичный адрес (а-ля <i>"чтобы все в офисе могли лазить по Интернету"</i>)</li> 
			<li>в случае двух внешних сетей, имеющих выход в Интернет, маршрутизировать (направлять) исходящий трафик (согласно каким-то критериям) по разным путям (через разных провайдеров) (а-ля <i>"чтобы Вася лазил в Интернет через провайдера X, а Петя, в то же время, через провайдера Y"</i>)</li>
			<li>контролировать доступ из внутренних сетей к узлам внешних сетей на уровне пользователей, адресов узлов, протоколов, tcp/udp-портов (а-ля <i>"Петя может лазить везде,Васе только ICQ, а Маше вообще в Инете делать нечего"</i>)</li>
	    <li>ограничивать скорость передачи данных между внешними и внутренними сетями (<i>download</i> и <i>upload</i>) на уровне пользователей, адресов узлов, протоколов, tcp/udp-портов (а-ля <i>"Петя может качать со скоростью 100 кбайт/с, а Вася 200 кбайт/с"</i>)</li>
	    <li>организовать доступ из внешних сетей к внутренним сетям с контролем доступа  на уровне пользователей, адресов узлов, протоколов, tcp/udp-портов (а-ля <i>"чтобы сотрудник мог подключиться к нашей сети из дома/командировки (через VPN)"</i> и <i>"чтобы люди могли заходить на наш сайт, почта была у нас и "ходила" от нас и к нам"</i>)</li>
		</ul>

    <li>требования:</li>
  	<ul>
		  <li>относительно простой визуальный анализ логики правил</li>
		  <li>относительно простое (не требующее длительного анализа последствий, предсказуемое) внесение изменений в конфигурацию фаервола</li>
		  <li>возможность перезапуска "по частям", т.е. чтобы при изменении конфигурации 
          можно было обновлять не все правила, а только часть правил, соответствующих изменениям 
          в конфигурации</li>
			<b>следующие требования относятся в большей степени к реализации как таковой:</b> 
		  <li>интеграция решаемых задач (контроль доступа и управление трафиком) в единое целое для упрощения администрирования и наглядности конфигурации</li>
		  <li>желательно наличие конфигурационных файлов, позволяющих задать необходимые метаданные (имена узлов, ФИО сотрудников и т.п.)</li>
		</ul>
	</ul>
</p>

<br>
<br>
<h2 id="networks">Схема сети</h2>
<p>
Ниже приведена схема сети, которая будет рассматривать в дальнейшем:
</p>
<center><img src="img/networks.png"/></center>
<br>
<center>Рис. 1 Схема сети</center>
<p>
В общем, схема представляет собой классический <i>"three-legged"</i> (<i>"Т-образный"</i>) firewall с выделением сервисов,
доступных извне в отдельную сеть <b>DMZ</b> (от <i>"Демилитаризованная зона"</i>),
которая подключена к отдельному интерфейсу (т.е. сетевой карте).
</p>

<p>
Чтобы лучше была видна <i>"Т-образность"</i> фаервола, ниже приведена слегка упрощенная схема (без подключения ко второму провайдеру).
</p>

<center><img src="img/networks-simple.png"/></center>
<br>
<center>Рис. 2 Упрощенная схема сети</center>
<p>

<p>
Теперь рассмотрим схему сети на <b>Рис.1</b> подробнее:
</p>
<p>
Территориально в <b>Здании 1</b> расположены:
</p>

<ul>
	<li><b>LAN</b> - наша локальная сеть, в которой расположено все наше оборудование
	 (компьютеры), которое должно быть <b>недоступно</b> напрямую (по IP-адресам) извне;</li>
	<li><b>PPTP</b> - виртуальная сеть поверх физичеcкой сети <b>LAN</b> - служит для контроля доступа
	 из внутренних сетей во внешние: чтобы "выйти в Интернет" надо установить PPP-туннель к шлюзу;</li>
	<li><b>DMZ</b> - демилитаризованная сеть, в которой расположено оборудование,
	  которое должно быть <b>доступно</b> извне (<i>web/ftp/почтовый сервера</i>).<br>
    Возможны два варианта:</li>
    <ul>
	    <li style="color:red">сеть DMZ имеет публичные адреса выделенные провайдером - нужна только маршрутизация;</li>
	    <li style="color:red">сеть DMZ имеет private адреса - нужны: маршрутизация + SNAT + DNAT + port forwarding;</li>
	  </ul>
	<li><b>ADSL модем №1</b>, подключеный одним портом к шлюзу <b>Gateway</b>, а другим - к сети провайдера <b>ISP1</b>, 
	  например по телефонной линии;</li>
	<li><b>Gateway</b> - собственно шлюз: компьютер с ОС Linux, 4-мя сетевыми интерфейсами (сетевыми Ethernet-картами).
	  Шлюз выполняет роль маршрутизатора, фаервола, а также, на нем могут функционировать некоторые сервисы.
		<b>Именно через него проходит весь трафик между сетями</b> и именно здесь
    сосредоточены функции управления и контроля.  На шлюзе:</li>
		<ul>
			<li>к интерфейсу <b>eth0</b> через коммутатор (switch) подключена сеть <b>LAN</b>;</li>
			<li>к интерфейсу <b>eth1</b> подключен (своим Ethernet-портом) <b>ADSL модем №1</b>,
       <b>Интерфейс имеет статический IP-адрес</b>, выданный провайдером ISP1;</li>
			<li>к интерфейсу <b>eth2</b> через коммутатор (switch) подключен сеть <b>DMZ</b>;</li>
			<li>к интерфейсу <b>eth3</b> через коммутатор (switch) подключена сеть <b>LAN2</b>;</li>
		</ul>
</ul>
<p>
Территориально в <b>Здании 2</b> расположены:
</p>
<ul>
	<li><b>неподконтрольная, но доступная нам </b> локальная сеть <b>LAN2</b>:
	нам выделен один адрес для подключения шлюза <b>Gateway</b>.
	Сеть <b>LAN2</b> имеет выход в Интернет через <b>ADSL модем №2</b></li>
	<li><b>ADSL модем №2</b>, подключенный одним портом к сети <b>LAN2</b>, а другим - к
	 к провайдеру <b>ISP2</b> (например по телефонной линии).
   <b>Внешний интерфейс имеет динамический IP-адрес</b>, выдаваемый провайдером ISP2 при подключении.
   Есть возможность напрямую или через администратора сети <b>LAN2</b> настраивать
   маршрутизацию на <b>ADSL модеме №2</b>;</li>
  <li><b>roadworrior</b> - мобильные сотрудники, находящиеся вне внутренних сетей
	 (командировка, дом) и желающие получить доступ в сеть <b>LAN</b> (и возможно в <b>LAN2</b>).</li>
</ul>

<h2 id="networks_">Почему рассматривается именно этот вариант схемы?</h2>
<p>
Естественно реальная сеть, являющася прототипом рассматриваемой здесь сети, развивалась от простого к более сложному.
Сначала была сеть <b>LAN</b> и один провайдер <b>ISP1</b>. Далее потребовалось контролировать 
доступ из сети <b>LAN</b> в Интернет - был выбран PPTP и появились <i>pppN</i> интерфейсы.
Затем появилась необходимость в размещении
собственных сервисов - для этого появилась сеть <b>DMZ</b>. Позже появилось подключение к сети <b>LAN2</b>
и выход через неё на второго провайдера <b>ISP2</b>. Соответственно, росло и количество интерфейсов.
</p>

<p>
С другой стороны, рассматриваемый вариант удобен тем, что он имеет достаточную сложность, 
чтобы проявилась значительная часть вопросов и проблем, возникающих при управлении фаерволом на таком шлюзе. 
 
</p>

<br>
<h2 id="problems">Проблемы</h2>
<p>
Как организовать последовательность правил для фаервола? По какому принципу?
Чтобы это было наглядно и удобно.
В <i>NETFILTER</i> есть пять встроенных цепочек:
</p>
<ul>
	<li><b>INPUT</b> - для всех входящих (через любой интерфейс) пакетов,
    <i>предназначенных для данного узла</i> (т.е. после процесса маршрутизации);</li>
	<li><b>OUTPUT</b> - для всех исходящих через все интерфейсы пакетов
    <i>ещё до процесса маршрутизации</i>;</li>
	<li><b>FORWARD</b> - для всех проходящих (из любого - в любой интерфейс) пакетов;</li>
	<li><b>PREROUTING</b> - для всех входящих (через любой интерфейс) пакетов,
    <i>ещё до процесса маршрутизации</i>;</li>
	<li><b>POSTROUTING</b> - для всех исходящих (через любой интерфейс) пакетов
    уже <i>после процесса маршрутизации</i>.</li>
</ul>
<p>
Чаще всего, в примерах и HOWTO встречается такой подход: правила фильтрации
помещаются в эти цепочки "все в кучу" - одно за одним:
все правила для фильтрации входящих пакетов на все интерфейсы - в INPUT,
все правила для фильтрации исходящих пакетов из всех интерфейсов - в OUTPUT.
Или: все разрешающие правила - в одну пользовательскую цепочку, все запрещающие - в другую цепочку.
Или: все правила для tcp - в одну цепочку, для udp - в другую, для icmp - в третью и т.п. (см. тот же "Iptables Tutorial").
Изредка вводятся какие-то цепочки для решения каких-то частных задач и только.
Достаточно универсальная методика не предлагается - в основном какие-то фрагменты,
из которых трудно "сложить" что-то "внятное".
</p>

<p>
Такой подход к организации последовательности правил приводит к ряду сложностей:
<ul>
	<li><b>правила изначально трудно планировать и разрабатывать</b>: неочевидно какого принципа придерживаться,
      какие правила должны идти раньше, какие - позже; со временем это 100%-но превращается в неуправляемый "бардак" и поэтому;</li>
	<li><b>правила трудно анализировать</b>: выяснить (вспомнить) <i>"что я тут накрутил?"</i>;</li>
	<li><b>правила трудно модифицировать</b>: вам надо внести изменения, и для этого вам сначала надо вспомнить <i>"что я тут накрутил?";</i>,
    потом долго думать <i>"куда же мне воткнуть эти новые правила?"</i>, а затем еще дольше анализировать:
    <i>"будет ли это работать нужным образом после внесения изменений?"</i> и <i>"не нарушу ли я то, что раньше работало?"</i>;</li>                        
	<li><b>правила трудно автоматизровать</b>: для бессиcтемных наборов правил труднее написать некий скрипт,
  автоматизирующий создание нужной логики на основе неких конфигурационных файлов.</li>
</ul>
Т.о. нужен некий, в меру универсальный, принцип организации и комбинирования правил
фаервола.
</p>

<br>
<br>
<h2 id="conception">Основные концепции: потоки, линки, цепочки</h2>
<div class="excl">
За основу взято предположение о симметричной маршрутизации. Для шлюза это значит,
что ответные IP-датаграммы приходят на тот же интерфейс, с которого ушли "прямые".
В случае форвардинга - что ответные IP-датаграммы проходят через ту же пару 
интерфейсов, что и IP-датаграммы в прямом направлении. 
</div>
<p>
Косвенно о применимости данного подхода намекает cуществование 
<b>"Strict Reverse Path Forwarding"</b> 
в <a href="http://tools.ietf.org/html/rfc3704#page-5">
RFC 3704 - Ingress Filtering for Multihomed Networks</a>:
</p>

<pre class="cmd">
Strict Reverse Path Forwarding is a very reasonable approach in front
of any kind of edge network; 
...
First, the test is only applicable in places where routing is
symmetrical - <b>where IP datagrams in one direction and responses</b> from
the other deterministically <b>follow the same path</b>.  <b>While this is
common at edge network interfaces to their ISP</b>, it is in no sense
common between ISPs, which normally use asymmetrical "hot potato"
routing.
</pre>

<p>
В целом, в RFC 3704, речь идет о защите от трафика с поддельных адресов путем 
проверки  на маршрутизаторе маршрутизации до адреса-источника.
Адрес-источника в получаемом пакете проверяется по FIB и если пакет был
получен через тот же интерфейс, через который будет маршрутизироваться ответ к 
этому адресу, то всё ок. Иначе пакет можно игнорировать. 
Естественно это работает только при симметричной маршрутизации, 
которая используется при подключении клиента к провайдеру (т.е. наш случай).
</p>

<dl>
<dt><b>Потоком</b> (<b>flow</b>)</dt>
<dd>
условимся называть <i>однонаправленный логический</i> канал передачи данных между
двумя сетями или сетью и шлюзом. С учетом того, что:
<ul>
<li>физически трафик между сетями проходит не напрямую, а через шлюз;</li>
<li>в одну и ту же сеть (напр. <b>INET</b>) трафик может идти через разные интерфейсы шлюза;</li> 
<li>трафик в разные сети может идти через один и тот же интерфейс шлюза (напр. в <b>LAN2</b> и <b>INET</b> через <i>eth3</i>),</li>
</ul>
поток будет идентифицироваться, в общем случае   четырья элементами:
<ol>
<li>cеть источник,</li>
<li>входной интерфейс на шлюзе,</li>
<li>выходной интерфейс на шлюзе,</li>
<li>сеть приёмник.</li>
</ol>
При этом <i>сетью</i> может быть совокупность нескольких IP-сетей.
В рассматриваемой схеме, IP-сети <i>192.168.0.0/24</i> и
192.168.0.1/24  трактуются как одна сеть <b>LAN</b>, а не как две разные сети.
Так как по смыслу трафик этих сетей необходимо фильтровать
и маршрутизировать по единым правилам, то в терминах потоков 
удобнее рассматривать эти две сети как единое целое.    
<br>Далее по тексту потоки будут обозначаться следующим образом:<br>
<ul>
<li><b>сеть_источник>вх_интерфейс-исх_интерфейс>сеть_приёмник</b> обозначает 
один конкретный поток. Например <b>LAN>eth0-eth3>INET</b>, <b>LAN>eth0-GW</b>;</li>
<li><b>сеть_источник>сеть_приёмник</b> обозначает все потоки (через все интерфейсы) между двумя сетями, 
идущие в одном направлении.
Например <b>LAN>INET</b>, <b>INET>GW</b>;</li> 
<li><b>сеть_источник-сеть_приёмник</b> обозначает все потоки 
(в прямом и обратном направлениях) между двумя сетями;
Например <b>LAN-INET</b>, <b>INET-GW</b>.
</li> 
</ul>
<dt>
Из двух потоков, поток считающийся прямым, выбирается произвольно. Обратным же 
для прямого, будет поток, в идентификаторе которого входящие и исходящие сеть и 
интерфейс зеркально переставлены местами.   
</dt>
</dd>
</dl>

<dl>
<dt><b>Линком, связью</b> (<b>link</b>)</dt>
<dd>
в данном тексте условимся называть совокупность всех потоков
(прямых и обратных через все интерфейсы) между двумя сетями или сетью и шлюзом.
</dl>

<dl>
<dt><b>Цепочкой потока (flow chain)</b></dt>
<dd>
будем называть пользовательскую цепочку, в которой сгруппированы правила, 
предназначенные для управления пакетами данного потока.
</dd>
</dl>

<p>
На рис.3 показаны <b>основные</b> потоки. Потоки относящиеся к одному линку имеют один цвет.
Неиспользуемые или второстепенные потоки не показаны, чтобы не загромождать схему.
Потоки между <b>VPN</b> и другими сетями показаны в разделе 
<a href="#rw_vpn">Функционирование  VPN с точки зрения фаервола</a>.
Каждая стрелка соответствует потоку,
а название стрелки - имени цепочки в правилах фильтрации. 
</p>

<p>
<center><img src="img/networks-flows.png"/></center>
<br>
<center>Рис. 3 Потоки (основные)</center>
</p>

<p>
Из схемы видно, что, например, линк <b>LAN-DMZ</b> состоит из двух потоков:
<ol>
<li><b>LAN</b>><i>eth0</i>-<i>eth2</i>><b>DMZ</b> - прямого и </li>
<li><b>DMZ</b>><i>eth2</i>-<i>eth0</i>><b>LAN</b> - обратного.</li>
</ol> 
Линк <b>LAN-INET</b> состоит из восьми потоков:
<ol>
<li><b>LAN</b>><i>eth0</i>-<i>eth1</i>><b>INET</b> - прямой,</li>
<li><b>LAN</b>><i>eth0</i>-<i>eth3</i>><b>INET</b> - прямой,</li>
<li><b>INET</b>><i>eth1</i>-<i>eth0</i>><b>LAN</b> - обратный,</li>
<li><b>INET</b>><i>eth3</i>-<i>eth0</i>><b>LAN</b> - обратный,</li>
<li><b>LAN</b>><i>ppp+</i>-<i>eth1</i>><b>INET</b> - прямой,</li>
<li><b>LAN</b>><i>ppp+</i>-<i>eth3</i>><b>INET</b> - прямой,</li>
<li><b>INET</b>><i>eth1</i>-<i>ppp+</i>><b>LAN</b> - обратный,</li>
<li><b>INET</b>><i>eth3</i>-<i>ppp+</i>><b>LAN</b> - обратный.</li>
</ol>
</p>

<p>
В случаях когда сетью источником/приемником является сам шлюз,то входной/выходной
интерфейсы не имеют смысла, например для линка <b>LAN-GW</b>: 
<ol>
<li><b>LAN</b>><i>eth0</i>-<b>GW</b> - прямой поток,</li>
<li><b>GW</b>-<i>eth0</i>><b>LAN</b> - обратный.</li>
</ol> 
</p>

<p>
Выше была показана только часть существующих между сетями потоков.
<b>Посмотрим, сколько всего потоков в рассматриваемой конфигурации.</b> 
Представим схему сети в виде графа (рис. 4), в котором шлюз и сети представлены вершинами.
Соединим каждую вершину со всеми остальными - получим полный граф со всеми 
возможными линками (ребра графа) между пятью сетями. Цвет линков на рис. 4
соответствует цветам на рис. 3. Серым цветом показаны неиспользуемые линки - 
трафик между этими сетями нам не нужен и поэтому не разрешён.  
Кол-во линков будет равно N*(N-1)/2, где N - кол-во вершин в графе. 
Для рассматриваемой сети N=6 и <b>кол-во линков равно <i>15</i></b>.<br> 
<b>Т.к. линк состоит как минимум из двух потоков - <i>прямого</i> и <i>обратного</i>, то
минимальное кол-во всех потоков будет равно <i>30</i>.</b>
</p>

<p>
<center><img src="img/links_flows.png"/></center>
<br>
<center>Рис. 4 Полный ориентированный граф сети - потоки и связи</center>
</p>

<p>
Однако это еще не всё. Сеть LAN у нас "синтетическая" и фактически 
объединяет две разных IP-сети на двух разных интерфесах - <i>eth0</i> и <i>ppp+</i>.
С сетью INET похожая ситуация - доступ в сеть INET тоже возможен через два
интерфейса - <i>eth1</i> и <i>eth3</i>.
Поэтому если мы хотим посчитать абсолютно все варианты обмена трафиком,
то граф должен содержать все сочетания <i>сеть-интерфейс</i>:
<b>LAN-eth0, LAN-ppp+, INET-eth1, INET-eth3, LAN2-eth3, VPN-tun1, DMZ-eth2, GW</b> или
что то же самое все интерфейсы шлюза плюс сам шлюз.
Получаем 7 интерфейсов плюс сам шлюз и полное кол-во потоков 7*8=<b>56</b>.      
Максимальное же кол-во потоков для графа на рис. 3 будет равно <b>52</b>, т.е.
меньше на 4 потока: LAN>eth0-ppp+>LAN, LAN>ppp+-eth0>LAN и INET>eth1-eth3+>INET, INET>eth3-eth1>INET.
</p>

<p>
Теперь переходим к предлагаемому приниципу организации правил.
</p>

<div class="excl">
<p>
<b>Основной подход в организации правил</b> - вместо линейной последовательности правил 
<b>использовать ветвление: каждому потоку - своя пользовательская цепочка (цепочка потока)</b>.
Пакет "направляется на проверку" в ту или иную пользовательскую цепочку на основании 
принадлежности к одному из потоков.
</div>
</p>
<p>
Схематично это можно изобразить следующим образом:
</p>


<br>
<p>
<center><img src="img/linear_branched.png"/></center>
<br>
<center>Рис. 5 Линейная организация правил (слева) и организация правил с ветвлениями 
на основе интрефейсов и адресов сетей (справа).</center>
</p>

<div class="excl">
Т.о. краткий алгоритм организации правил таков:
<ul>
	<li>идентифицируем все возможные <b>связи</b> между сетями и между сетями и шлюзом</li>
	<li>для каждого реально используемого соединения создаем <b>по две</b> (для прямого и обратного <b>потока</b>) 
	   <b>отдельных цепочки</b> с информативными именами</li>
	<li>правила, относящиеся к соответствующему потоку помещаем в соответствующую цепочку (т.е. в <b>цепочку потока</b>)</li>
	<li>во встроенных цепочках <i>INPUT, OUTPUT, FORWARD</i> создаем только правила переходов в
  соответствующие цепочки потоков</li>
	<li>используем данный подоход как для фильтрации (в filter table), 
  так и модификации (напр. MARK, CLASSIFY) пакетов (mangle table)</li>
</ul>
</div>

<p>
В случае необходимости иметь единый набор правил для разных потоков достаточно 
направить пакеты, принадлежащие разным потокам, в одну и ту же цепочку потока. 
Например, в рассматриваемом примере, правила фильтрации одинаковы для трафика между 
сетями <b>LAN</b> и <b>Интернет</b> вне зависимости от того каким путём он 
идет: через <b>eth0</b> или <b>ppp+</b> и <b>eth1</b> или <b>eth3</b>.  
Или, например, трафик между сетями <b>LAN</b>, <b>PPTP</b> и шлюзом <b>GW</b> будет 
обрабатываться одинаково, как будто <b>LAN</b>, <b>PPTP</b>  - одна сеть. 
</p>

<p id="rules_listing">
Ниже приведены ссылки на листинги правил фаервола для данной схемы, которые будут 
рассматриваться далее по тексту.
<ul>
<li><a style="font-size:20px" href="iptables_filter.html" target="_blank">Листинг <i>iptables -nvL -t filter</i></a></li>
<li><a style="font-size:20px" href="iptables_mangle.html" target="_blank">Листинг <i>iptables -nvL -t mangle</i></a></li>
<li><a style="font-size:20px" href="iptables_nat.html" target="_blank">Листинг <i>iptables -nvL -t nat</i></a></li>
</ul>
</p>


<p id="interfaces">
Конфигурация интерфейсов из файла <i>/etc/network/interfaces</i>
<pre class="cmd">
<i>/etc/network/interfaces</i>

allow-hotplug eth0 eth1 eth2 eth3

### LAN
auto eth0
iface eth0 inet static
	address 192.168.0.1
	netmask 255.255.255.0
	network 192.168.0.0
	broadcast 192.168.0.255

### INET
auto eth1
iface eth1 inet static
	address 198.51.100.2
	netmask 255.255.255.252
	network 198.51.100.0
	broadcast 198.51.100.3
	gateway 198.51.100.1

### DMZ
auto eth2
iface eth2 inet static
	address 203.0.113.1
	netmask 255.255.255.248
	network 203.0.113.0
	broadcast 203.0.113.7
	
### LAN2
auto eth3
iface eth3 inet static
	address 192.168.5.1
	netmask 255.255.255.0
	network 192.168.5.0
	broadcast 192.168.5.255
</pre>
</p>

<p id="conception_plus">
Какие плюсы и минусы имеет такой принцип организации правил?
На мой взгляд следующие:
</p>

<ul>
	<li>сразу просматривается некая логика которой можно следовать на протяжении всего жизненного цикла фаервола;</li>
	<li>довольно легко разработать, анализировать и модифицировать правила - просто
	  выбираем, по какому потоку мы хотим что-то сделать - и "лезем" только туда,
		практически не боясь затронуть логику, действующую по другим потокам;
		меньше вероятность ошибок - мы знаем, что в данной цепочке будут проверяться
		только пакеты относящиеся к данному потоку;</li>
	<li>появляется возможность легко выборочно перезапускать firewall, т.е. обновлять
	  правила "на лету" только по выбранному потоку, не затрагивая остальные;</li>
	<li>предположу, что такая организация правил ускоряет проверку пакетов:
	  вместо проверки пакета по линейному списку, состоящему из всех правил, 
		происходит определение нужной группы правил на основании сетей и интерфейсов пакета
		и, как следствие, существенное сужение списка проверяемых правил; хотя тут многое
		зависит от того сколько проверок потребуется для данного конкретного пакета и от
		соотношения количества цепочек, определяющих принадлежность и количества правил в
		цепочках потоков;
		</li>
	<li>единственным "недостатком" можно считать то, что количество потоков быстро растет
   с увеличением количества интерфейсов на шлюзе и контролируемых сетей, однако
	 этот вопрос легко решается автоматизацией - описываем сети, а скрипт создает
	 нужные цепочки; 4-ре интерфейса, пожалуй, разумный максимум для полностью
   ручного написания и модификаций правил фаервола;</li>
</ul>

<br>
<br>
<h2 id="filter">Фильтрация</h2>
<p>
Одна из основных задач фаервола - пропускать разрешенный трафик и блокировать
нежелательный, т.е. фильтровать входящий, исходящий, проходящий трафик.
Для правил фильтрации в <i>NETFILTER</i> есть
таблица <i>filter</i> - именно в неё добавляются правила, если в команде <i>iptables</i>
явно не указана желаемая таблица.
Однако, технически, правила фильтрации можно размещать в любых таблицах.
В этой части будут показаны реально используемые правила фильтрации, но сначала важно
разобраться, какие сетевые сервисы могут присутствовать на шлюзе, а какие там совершенно неуместны. 
</p>

<h3 id="filter_what">Сетевые сервисы или чего не должно быть на шлюзе</h3>
<p>
<div class="excl">
<b>В норме</b> на шлюзе не должно быть ни одного
сетевого сервиса и соответственно открытых портов! Из этого принципа надо
исходить изначально.
</div>
</p>

<p>
Максимум, что можно допустить на шлюзе - аутентификация межсетевого доступа
пользователей. И даже это желательно выносить на отдельный сервер аутентификации. 
<b>Все сетевые сервисы потенциально уязвимы и через них шлюз может быть взломан и как
результат - может быть получен контроль над шлюзом и доступ во все сети</b>.
Для уменьшения вероятности взлома шлюза через сетевые сервисы их (сервисы) выносят
на отдельные узлы (серверы) в отдельную, частично изолированную, сеть - <b>DMZ</b>.
</p>

<p>
<b>На шлюзе правила фильтрации настраивают таким образом, что подключения, инициируемые
из сети <i>LAN</i> в <i>DMZ</i> - разрешены, а вот соединения инициируемые из сети <i>DMZ</i> в <i>LAN</i> - блокируются!</b>
Таким образом, при взломе сервиса, работающего на сервере в <b>DMZ</b>, уменьшаются
последствия взлома - напрямую со взломанного сервера невозможно будет подключиться
к изолированой сети <b>LAN</b>.
Это и есть функция сети <b>DMZ</b>.
</p>

<p>
Однако в реальности, по разным организационно-финансовым причинам, не всегда есть
возможность вынести сетевые сервисы (<i>web/ftp/smtp/pop3/dns</i> и т.п.) в 
отдельную сеть. Поэтому не остается ничего другого, как ставить их на единственном
сервере - шлюзе.
</p>

<p>
<b>Размещать (все) сервисы непосредственно на шлюзе можно, если
вы считаете что риски, связанные с таким подходом, являются приемлемыми.
</b>
Например, вы считаете вероятность взлома низкой, или потери от утечек и восстановления
работоспособности не существенными.
</p>

<p>
Конкретно в нашем примере, сервисы расположены на шлюзе и только web-сервер вынесен в <b>DMZ</b>. 
Поэтому на шлюзе есть открытые порты.
</p>

<h3 id="access_control">Контроль доступа</h3>
<p>
Для контроля доступа на уровне адресов хостов, протоколов, tcp/udp-портов достаточно
возможностей одного <i>NETFILTER</i>.
Контроль сводится к фильтрации (блокированию) нежелательного трафика по IP и/или MAC адресам.
</p>
<p>
Но надо понимать, что и IP-адрес и MAC-адрес любого устройства в сети могут быть изменены
на адреса, которым разрешен доступ в контролируемую сеть (т.е. подделаны). 
Таким образом контроль
доступа только по адресам не является абсолютно надежным, но на практике часто
этого достаточно. Для более надежного контроля надо использовать какие-либо варианты
аутентификации и авторизации трафика, проходящего через шлюз. Для этого можно
использовать <b>VPN</b> или более специализированные решение, например <b>NuFW</b>.
</p>

<h4>PPTP VPN</h4>
<p>
В качестве VPN довольно удобно использовать <b>PPTP</b>: сервер есть под Linux, 
а клиент встроен во все версии Windows.
</p>
<p>
Для выхода в контролируемую сеть (например из <b>LAN</b> в <b>Internet</b>), пользователь должен создать туннель между
своим устройством (компьютером) и сервером PPTP на шлюзе. Для этого он должен
пройти аутентификацию по логину/паролю: получить доступ в контролируемую
сеть без аккаунта невозможно. Естественно на шлюзе правила фаервола запрещают
прохождение пакетов c адресов <i>192.168.0.0/24</i> сети <i>LAN</i> в контролируемую сеть. Зато там есть разрешающие
правила для клиентских адресов pptp-тунеллей <i>192.168.1.xxx</i>. Если пользователь пройдет процесс
аутентификации, то будет создан туннель между его компьютером и шлюзом.
На стороне шлюза туннелю будет присвоен адрес <i>192.168.3.1</i>, а клиентской
части туннеля - некий адрес, привязанный к логину и задаваемый в например <i>/etc/ppp/chap-config</i>.
Т.о. создается устойчивое соотвествие <i>"пользователь"</i> - <i>"IP адрес туннеля"</i> и
на шлюзе появляется возможность управлять доступом и другими параметрами подключения
на основе IP-адреса клиентской стороны туннеля с помощю <i>NETFILTER</i>.
Т.е. контроль на шлюзе и в этом случае происходит "естественным способом" - по IP-адресам.
Только в этом случае адрес "защищен от подмены" логином/паролем.  
</p>

<h4>OpenVPN</h4>
<p>
Для тех же целей можно использовать и OpenVPN.
Можно отключить шифрование, уменьшив нагрузку на шлюз. Недостатком этого варианта
является необходимость установки OpenVPN-клиента на клиентское устройство.
Для мобильных устройств с определенными ОС это может быть проблемой. 
<br>Также см.  <a href="#rw_choice">Выбор VPN</a> 
</p>

<h4>NuFW</h4>
<p>
Работа <a href="http://nufw.org/">NuFW</a> основана на другом принципе. 
На firewall устанавливается <b>Nufw-сервер</b>, который принимает пакеты от клиентов.
На этом же хосте (или на другом) ставится сервер аутентификации <b>Nuauth</b>.
На каждом клиентском хосте (компьютере) ставится <b>Nufw-клиент</b>. Каждый пакет
исходящий от клиента принимается Nufw-сервером и ставится в очередь в ожидании
аутентификации. Об этом пакете информируется Nuauth, который, в свою очередь,
запращивает у соответствующего Nufw-клиента аутентификационные данные.
Nufw-клиент запрашивает прохождение данного пакета у Nuauth и если пакет авторизован - 
передает разрешение на Nufw-сервер, который и "пропускает" пакет.
</p>

<p>
Nufw для своей работы использует <i>NETFILTER</i>: пакеты, подлежащие авторизации, просто
направляются в определенную <i>userspace (пространство пользователя)</i> очередь при помощи <i>target NFQUEUE</i>, т.е. в очередь которой
управляет Nufw-сервер. В случае удочной авторизации пакета, Nufw-сервер просто реинжектит
пакет из очереди обратно в сетевой стек. В реальности достатчно отправлять на
на авторизацию только <i>NEW-пакеты</i>, а остальные будут контролироваться в 
рамках сединения с помощью <i>conntrack</i>.
</p>
<p>
Как и в случае с OpenVPN, необходимость установки Nufw-клиента может стать
непреодолимым припятствием при использвании устройств с ОС, под которые не 
существует клиента. В первую очередь это относится к мобильным устройствам.
</p>
<p>
В 2012 году проект был в "дауне". На начало 2013 похоже проект "восстал из пепла"
с новым именем - <a href="http://www.ufwi.org">UFWI</a>
</p>

<div class="excl">
В нашем примере одновременно используются аутентификация по <b>IP-адресам</b> устройств и
<b>PPTP VPN</b>.
</div>

<p>
<b>NuFW</b> более громоздок и на момент написания первоначального варианта фаервола
не было бесплатного <b>NuFW-клиента</b> под Windows, поэтому в свое время был выбран вариант с PPTP VPN.
</p>

<h3 id="filter_rules">Цепочки фильтрации</h3>
<p>
Теперь рассмотрим непосредственно сами цепочки фильтрации и правила в них.
Однако, в отличии от большинства руководств, вместо анализа последовательности команд iptables,
мы будем анализировать непосредственно результат их работы, т.е. вывод команды
<i>iptables -nvL</i>.
</p>

<ul>
<li><a style="font-size:20px" href="iptables_filter.html" target="_blank">Листинг <i>iptables -nvL -t filter</i></a></li>
</ul>

<p>
Как видно илз листинга, цепочки имеют "говорящие" имена и пронумерованы. Это позволяет легко
ориентироваться в листинге таблицы <i>filter</i>. К тому же цепочки
сгруппированы по потокам и всегда выводятся в предсказуемом порядке, что удобно.
Рассмотрим эти цепочки.
</p>

<p>
<h4>INPUT, OUTPUT, FORWARD</h4>
Листинг начинается со встроенных стандартных
цепочек <i>INPUT, OUTPUT, FORWARD</i>.
В эти цепочки помещены правила, которые "определяют принадлежность"
(по интерфейсам и адресам сетей) пакета к какому-либо логическому потоку и
переключают логику дальнейших проверок на последовательность
правил в пользовательской цепочке, соответствующей потоку.
<b>Пакеты, не соответствующие ни одному из правил в этих цепочках
(пакеты не соответствующие ни одному из логических потоков) игнорируются.</b>
Достигается это установкой политик по умолчанию (см. <i>Chain XXXX (policy DROP...</i>) 
Порядок, в котором размещены правила в цепочках <i>INPUT, OUTPUT, FORWARD</i>,
косвенно влияет на скорость обработки пакетов: первыми желательно размещать правила,
под которые будет, предположительно, подходить основная масса пакетов. 
</p>
<div class="excl">
Т.е. рекомендуемый подход в организации правил - <b>"все что явно не разрешено - запрещено"!</b>.
Изначально, политкой фаервола, все запрещено и суть настройки сводится к явному 
описанию разрешающих правил.
</div>

<p>
Наборы правил, сгруппированые по потокам в пользовательские цепочки, 
описаны далее в порядке номеров на схеме.
</p>

<p>
Таким образом, в дальнейшем (после правил в цепочках <i>INPUT, OUTPUT, FORWARD</i> ), 
пакет проверяется на соответствие только "своим"
(относящимся только к данному потоку) правилам, <b>а не всем подряд правилам, имеющимся в firewall'е</b>.
Это и даёт преимущества, <a href="#conception_plus">указанные выше</a>.  
</p>

<p>
<b>Примечание.</b> <span style="font-size:13px">В англоязычных источниках процесс проверки пакета на соответствие какому-то из
правил так и описывается: будто бы пакет "движется" по набору правил,
от правилу к правилу, "прыгает" в другие цепочки, "выпрыгивает" из них обратно и т.д.
Насколько я понимаю, это условность - в реальности пакет никуда не "движется" (по набору правил),
но так очень удобно описывать процесс, в котором ядро, для каждого пакета,
перебирает правила, имеющиеся в firewall'e, проверяет, соответствует ли пакет данному правилу 
и решает что с эти пакетом делать:
проверить следующее правило, переключиться (-j ИМЯ_ЦЕПОЧКИ) на другую последовательность (цепочку)
правил проверки, принять пакет (-j ACCEPT) или отвергнуть его (-j REJECT/DENY) и т.п.
Конечно, взаимосвязь между этапами обработки пакетов в сетевом стеке и таблицами/встроенными цепочками есть, но не более того. Я по тексту буду тоже "грешить" таким подходом, чтобы не ломать язык.
</span>
</p>

<p>
Далее рассмотрим цепочки потоков и наборы правил в них подробнее, ориентируясь по схеме на <b>рис. 3</b>.
<br>Расцветка правил, соответствует цветам потоков на схеме. 
</p>

<p id="filter_LAN_GW">
<h4><span style="background-color:#8FCC8F">
01_LAN_GW, 02_GW_LAN: Локальная сеть - шлюз</span></h4>

Начнём со взаимодействия локальной сети <b>LAN</b> и шлюза <b>Gateway</b>.
Для цепочки <b>01_LAN_GW</b> мы "отбираем" только пакеты пришедшие из сети
<i>192.168.0.0/24</i> на адрес <i>192.168.0.1</i> интерфейса <i>eth0</i>, а также 
пакеты пришедшие из сети <i>192.168.1.0/24</i> на адрес <i>192.168.3.1</i> любого ppp-интерфейса.
Это соответственно правила №2 и №3 в цепочке <b>INPUT</b>.
<i>192.168.3.1</i> - это адрес серверных концов всех ppp-туннелей, он одинаков
для всех и устанавливается в <i>/etc/pptpd.conf</i>.

<br>Для цепочки <b>02_GW_LAN</b> всё наоборот: пакеты уходящие с <i>192.168.0.1</i> <i>eth0</i>
в сеть <i>192.168.0.0/24</i> и пакеты уходящие со <i>192.168.3.1</i> любого ppp-интерфейса
в сеть <i>192.168.1.0/24</i>. 
Это и есть потоки между локальной сетью и шлюзом.
</p>

<p>
<b id="filter_note1">Примечание</b>. <span style="font-size:13px">Возможно, вам покажется, излишним указывать в правилах одновременно
интерфейс и его адрес (или адрес сети), но это не так. Дело в том, что на любой интерфейс
может прийти пакет с абсолютно любым адресом назначения (и отправителя тоже).
Если вы укажите только имя интерфейса и не укажете IP-адрес(а), то даже пакет с адресом
получателя, не совпадающим с адресом интерфейса, будет принят на дальнейшую обработку
и может "проскочить" ваш firewall. 
<b>Поэтому ни в коем случае нельзя полагаться на то что</b>, раз у вас внутренняя сеть
имеет адрес <i>192.168.0.0/24</i>, т<b>о в ней не могут появиться пакеты с адресами принадлежащими
другим IP-сетям</b>. Такие ситуации могут возникнуть как в результате попыток обойти firewall
так и в штатных режимах, например при маршрутизации (policy routing) или использовании туннелей (IPSec/NETKEY). 
Тоже самое относится и к отправляемым пакетам: например <i>eth1</i> имеет
адрес <i>198.51.100.2</i> но это не значит, что все пакеты отсылаемые провайдеру ISP1
через этот интерфейс обязательно будут иметь адрес отправителя <i>198.51.100.2</i>.
И уж тем более это очевидно в случае, когда интерфейс имеет несколько IP-адресов
(да, интерфейс может иметь несолько IP-адресов одновременно).
</span>
</p>

<p>
Итак мы рассмотрели, какие пакеты попадают на проверку в цепочки <b>01_LAN_GW, 02_GW_LAN</b>.
Теперь посмотрим, что у нас в цепочке <b>01_LAN_GW</b>.
Первым же правилом мы отправляем пакет в цепочку со счетчиками <b>01_LAN_GW_CNT</b>,
в которой просто перечислены интересующие нас правила с пустым <i>target</i> (в соответствующенй команде нет ключа -j).
Это правило идет самым первым, потому что иначе не будут считаться пакеты уже
установленных соединений.
<div class="excl">
<b>Обратите внимание, что нигде в цепочках потоков нам уже не надо обязательно
"отслеживать" входные-выходные интерфейсы и сети, так как мы уже "определились"  с ними ранее,
в правилах цепочек <i>INPUT, OUTPUT, FORWARD</i> и их указание будет просто излишним.</b>
</div>  
<br>
В <b>01_LAN_GW_CNT</b>, у подходящих правил, счетчики увеличиваются на размер пакета
и мы снова возвращаемся в <b>01_LAN_GW</b> на второе правило. Все "счетчики" являются
необязательными и если они вам не нужны - просты выкиньте соответствующие правила и цепочки.
Вторым правилом мы принимаем все пакеты, относящиеся к уже установленным
соединениям (т.е. те о которых уже "знает" conntrack).
Это правило идет вторым и перед правилами, разрешающими подключения на определенные порты,
потому что основная масса пакетов будет относиться к уже установленным соединениям.
И поэтому нет смысла пакеты "прогонять" сначала через кучу правил для новых соединений и
только потом ставить правило <i>RELATED,ESTABLISHED</i>.

Следующее правило перекидывает нас в цепочку <b>01_LAN_GW_SKIP_OR_DENY</b>, в которой
мы помещаем правила, запрещающие прохождение каких-либо пакетов и правила, позволяющие
сделать исключения из этих запретов. Такая комбинация практически универсальна и позволяет
реализовать большинство вариантов запретов и исключений. Остановимся на этом моменте подробнее. 
</p>

<p>
Рассмотрим пример, правда несколько искусственный.
Итак, вам надо открыть доступ к порту <i>tcp 873</i> для всей сети <i>192.168.0.0/24</i>, за исключением
хостов с адресами из диапазона <i>192.168.0.10-192.168.0.20</i>, но некий узел <i>192.168.0.13</i>,
из этого запрещенного диапазона, тоже должен иметь доступ на <i>tcp 873</i>.
</p>
<p>
Ясно, что запрещающее правило (<i>192.168.0.10-192.168.0.20 tcp dport 873 DROP</i>) должно идти перед
разрешающим (<i>192.168.0.0/24 tcp 873 tcp dport 873 ACCEPT</i>). А теперь, как из запретного
диапазона исключить <i>192.168.0.13 tcp dport 87</i>3? Надо поставить это правило перед правилом
<i>192.168.0.10-192.168.0.20 tcp dport 873 DROP</i>. А если у нас будет много разных
запрещающих условий и исключений из них? Надо будет следить за порядком следования
этих правил и это будет слишком громоздко. <br>
<b>Поэтому удобнее разнести запрещающие и исключающие правила в разные цепочки 
по нижеизложенному принципу</b>.
<p>
</p>
Как мы уже видели, в основной цепочке потока <b>01_LAN_GW</b> первым же делом мы принимаем (<i>ACCEPT</i>)
все пакеты, относящиеся к уже установленным (<i>RELATED,ESTABLISHED</i>) соединениям.
Таким образом, после этого правила, все проверки будут проводиться только над пакетами
<i>"в состоянии NEW"</i> (ну еще может быть <i>INVALID</i>), что, по идее, экономит ресурсы. 
<b>Поэтому далее идет "работа" только с <i>"NEW-пакетами"</i></b>.
<br>
Далее проверка из <b>01_LAN_GW</b> переходит в цепочку
<b>01_LAN_GW_SKIP_OR_DENY</b>, где <b>первыми</b> размещены все исключающие правила с target <i>-j RETURN</i>,
а самым последним стоит правило направляющее проверку в список запрещающих правил
<b>01_LAN_GW_DENY</b>. Таким образом пакеты, подпадающие под исключения из запретов,
"выскочат сухими" из <b>01_LAN_GW_SKIP_OR_DENY</b> обратно в <b>01_LAN_GW</b>,
так и не дойдя до цепочки <b>01_LAN_GW_DENY</b>. А пакеты, не соответствующие
исключениям, уйдут на проверку в <b>01_LAN_GW_DENY</b> и, возможно там и "умрут" (<i>DROP</i>).
"Выжившие" (не подошедшие ни под один <i>DROP</i>) "выскочат" сначала в <b>01_LAN_GW_SKIP_OR_DENY</b>,
так как цепочка <b>01_LAN_GW_DENY</b> закончилась, а затем и в <b>01_LAN_GW</b>.
И если эти пакеты не будут соответствовать разрешающим правилам (<i>ля-ля-ля state NEW -j ACCEPT</i>) в этой цепочке,
они будут "убиты" самым последним правилом в цепочке <b>01_LAN_GW</b> (<i>DROP</i>),
которое, в некторой степени, является перестраховочным, но полезным. Оно скорейшим
образом убъет "неугодный" пакет, не давая ему "выпрыгнуть" обратно в <i>INPUT</i> и "гулять"
по остальным правилам пока его не <i>DROP</i>'нут по <i>default</i>'у.

</p>
<p>
Таким образом, достаточно универсальную логику для одного потока можно построить по следующей схеме:
</p>

<p>
<center><img src="img/rules_flow.png"/></center>
<br>
<center>Рис. 6 Логика "Разрешения+исключения из разрешений (запрет)+исключения из запретов"</center>
</p>

<p>
Т.е. разрешающие правила располагаются после всех запрещающе-исключающих правил.
<div class="excl">
<b>По только что описанному принципу (рис. 6) построены цепочки и правила фильтрации для всех потоков!</b>
<br>Да, конечно можно, вместо использования цепочек <b>LAN_GW_DENY</b> и <b>LAN_GW_ALLOW</b>, просто выстроить правила в нужном порядке: 
сначала <i>RELATED/ESTABLISHED</i>, затем все запрещающие и затем все разрешающие - эффект будет (почти) тот же.
"Почти" потому что, теряется возможность вносить исключения в запреты посредством добавления правил в <b>LAN_GW_SKIP_OR_DENY</b> - 
без использования цепочки подобный "пропуск" группы правил невозможен. 
Также теряется простой (путем очистки соответствующей цепочки) способ обновить часть правил: 
только разрешающие или только запрещающие. Т.е. возможности оптимизации зависят от
наличия или отсутствия тех или иных правил в конкретном потоке. 
Эту задачу можно решить с помощью скриптов.
</div>
</p>

<p>
Назначение большей части открытых портов в <b>01_LAN_GW</b> думаю очевидна.
Специально поясню только два правила:
</p>
<ul>
	<li>tcp dpt:1723 - разрешаем подключения к демону pptp, так как контроль доступа
	  из внутренних во внешние сети построен на использовании PPTP-туннелей</li> 
	<li>proto 47 - он же GRE, в него упаковываются ppp-пакеты PPTP-туннелей, поэтому нам тоже его нужно разрешить;
	  протокол GRE что называется <i>stateless</i>, поэтому нет особого смысла указывать
		<i>state NEW</i>. Здесь он указан, потому что правила созданы автоматически скриптом.</li> 
</ul>

<p>
Теперь о цепочке <b>02_GW_LAN</b>. Здесь мы описываем, что сам шлюз может посылать
в сеть <b>LAN</b>. Здесь все тоже построено по вышеописанному принципу.
Так же, вторым правилом после счетчиков, идет <i>... state RELATED,ESTABLISHED...ACCEPT</i>,
для того чтобы принимать пакеты ранее установленных соединенений.
</p>

<p>
Порты в <b>02_GW_LAN</b>.
</p>
<ul>
	<li><i>tcp spt:5222, tcp spt:5223</i> - протокол XMPP; в сети LAN есть Jabber-сервер, на него
	  шлюз отсылает некую информацию (оповещения от разных скриптов) со шлюза, которая оперативно
		видна в IM-клиенте</li>
	<li><i>tcp spt:20</i> - на шлюзе есть ftp-сервер (для технических целей), подключение к
		нему идет в active-режиме <b>через SSL</b>, поэтому модуль ядра <i>ip_conntrack_ftp</i> не сможет распознать
		в зашифрованном потоке служебную информацию FTP-протокола 
		и отнести новое соединение, которое сервер пытается установить с клиентом, к состоянию <i>RELATED</i>;
		таким образом правило <i>ACCEPT ... state RELATED,ESTABLISHED</i> для таких соединений срабатывать
		не будет и выход только один: явно разрешить ftp-серверу
		подключаться c tcp порта 20 шлюза <b>на любой адрес и порт сети LAN</b>; см. схему для Active FTP, например здесь 
		<a target="_blank" href="http://slacksite.com/other/ftp.html#active">Active FTP vs. Passive FTP, a Definitive Explanation</a></li>
</ul>

<h4 id="related">Почему <i>RELATED,ESTABLISHED</i> нужен в обоих направлениях</h4>
<p>
Помню, когда я начинал писать свои первые правила фаервола, я не до конца понимал,
почему <i>RELATED,ESTABLISHED</i> нужен в обоих направлениях, даже если я разрешаю
соединения только в одну сторону.
</p>

<p>
Чтобы лучше понять, почему, я рисовал диаграмки на бумажках, а здесь привожу следующую
диаграмму. Думаю, с ней всё станет очевидно.
</p>

<p>
<center><img src="img/re_in_both.png"/></center>
<br>
<center>Рис. 7 Connection tracking: почему <i>RELATED,ESTABLISHED</i> нужен в обоих направлениях</center>
</p>

<p  id="filter_INET_GW">
<h4><span style="background-color:#CC8F8F">
03_INET_GW, 04_GW_INET: Интернет - шлюз</span></h4>
Это потоки между самим шлюзом и сетью Интернет. Как видно из схемы, существует
два пути для трафика между шлюзом и Интернетом: через провайдера <b>ISP1</b> и через
провайдера <b>ISP2</b>. Соответственно для проверок, в эту цепочку отбираются следующие пакеты
(смотрим цепочки <i>INPUT, OUTPUT</i>):
<ul>
для пути через провайдера <b>ISP1</b>
		<ul>
		<li>входящие на адрес <i>198.51.100.2</i> интерфейса <i>eth1</i></li>  
		<li>выходящие из интерфейса <i>eth1</i> с адресом источника <i>198.51.100.2</i></li>
		</ul>
для пути через провайдера <b>ISP2</b>
		<ul>
		<li>входящие на адрес <i>192.168.5.1</i> интерфейса <i>eth3</i> не из сети <i>192.168.0.0/16</i> (в том числе не из сети <i>192.168.5.0/24</i>)</li>  
		<li>выходящие из интерфейса <i>eth3</i> с адресом источника <i>192.168.5.1</i> не в сеть <i>192.168.0.0/16</i></li>
		</ul>
</ul>
Каким же образом определяется, по какому пути пойдет со шлюза конкретный пакет в данный момент?
Этот вопрос решается с помощью средств маршрутизации.
<b>Фильтрация, как таковая, здесь может лишь выполнять вспомогательную функцию -
маркировку пакетов (напр. MARK).</b>
Поэтому ответ на этот вопрос будет дан в разделе <a href="#routing">Маршрутизация (routing)</a>, а сейчас рассмотрим только фильтрацию.
</p>

<p>
Выданный провайдером <b>ISP1</b>, адрес <i>198.51.100.2</i> на <i>eth1</i>, является <b>статическим</b>,
поэтому внешние сервисы (<i>ftp/smtp/dns/pop3</i>) "слушают" на этом адресе и интерфейсе. Именно этот адрес является "адресом шлюза в сети Интернет".
А вот адрес <i>192.168.5.1</i> на интерфейсе <i>eth3</i> является внутренним,
а внешний адрес присваивается внешнему интерфейсу модема <b>ADSL №2</b> <b>динамически</b>.
Т.е. шлюз своим интерфейсом <i>eth3</i> непосредственно в Интернете "не виден".
Поэтому организовать на нем внешние сервисы возможно, но на
практике это связано с различными проблемами, связанными как с динамическим
адресом модема (обновление DNS, RR в обратной зоне для SMTP-сервера и т.п.), так
и необходимостью использования DNAT на модеме для осуществления <i>проброса портов</i>
(<i>port forwarding</i>) от модема на <i>eth3</i> шлюза.
Т.о., в нашем примере, для интерфейса <i>eth1</i> надо открыть порты внешних сервисов, а вот для <i>eth3</i>
это делать не надо.
</p>

<p>
Поэтому, специально для трафика, приходящего из Интернета на <i>eth3</i> сделана отдельная 
цепочка <b>03_INET_GW_ISP2</b>, которая фактичеcки является копией <b>03_INET_GW</b>, но без <i>"NEW-правил"</i>.
В ней мы только считаем и разрешаем трафик от уже установленных соединений.
</p>

<p>
Почему в <b>INPUT</b> в правиле для <b>03_INET_GW_ISP2</b> мы не указали <i>destination 192.168.5.1</i>?
Это связано с тем опять же с маршрутизацией через двух провайдеров: на интерфейс
<i>eth3</i> могут приходить и пакеты с адресом назначения <i>198.51.100.2</i>, 
а не только <i>192.168.5.1</i>. Причины этого рассмотрены в разделе 
<a href="#routing_eth3">"Этапы прохождения пакета через NETFILTER"</a>
</p>

<p>
Схема же построения правил в цепочках <b>03_INET_GW, 04_GW_INET</b> совпадает со схемой 
правил в цепочках <b>01_LAN_GW, 02_GW_LAN</b> и не имеет каких-то отличительных особенностей.
</p>

<br>
<p  id="filter_LAN_INET">
<h4><span style="background-color:#8FADCC">
05_LAN_INET, 06_INET_LAN: Локальная сеть - Интернет</span></h4>
Переходим к рассмотрению несколько более интересной ситуации: трафик между локальной сетью <b>LAN</b>
и сетью <b>Интернет</b>. <b>Здесь также два пути прохода трафика</b>: через провайдера <b>ISP1</b>
и провайдера <b>ISP2</b>.
</p>

<p>
В данном примере цепочки <b>05_LAN_INET, 06_INET_LAN</b> универсальны и служат для
фильтрации по обоим маршрутам, т.е. правила фильтрации одни и те же 
для трафика через обоих провайдеров.
</p>

<p>
Эти цепочки используются для проверки следующих пакетов (смотрим схему и правила):
<ul>
для трафика через провайдера <b>ISP1</b>:
		<ul>
		<li>входящих из сети <i>192.168.0.0/24</i> в интерфейс <i>eth0</i> и выходящих через <i>eth1</i>;</li>  
		<li>входящих из сети <i>192.168.1.0/24</i> в любой ppp-интерфейс и
		    выходящих также через <i>eth1</i> (для хостов, получающих доступ в 
        Интернет через ppp-соединение);</li>
		</ul>
для трафика через провайдера <b>ISP2</b>:
		<ul>
		<li>входящих из сети <i>192.168.0.0/24</i> в интерфейс <i>eth0</i> и выходящих
		  через <i>eth3</i>  на адреса не из сети <i>192.168.0.0/16</i>.
      <b>Дополнительный контроль целевой сети нужен для того, чтобы отличить трафик 
      идущий конкретно в сеть <i>192.168.5.0/24</i> (т.е. <i>LAN</i>-><i>LAN2</i>) 
      от любого другого</b> (подразумевается, что для
			локальных сетей выбраны адреса из диапазона <i>192.168.0.0/16</i>)</li>  
		<li>входящих из сети <i>192.168.1.0/24</i> в любой ppp-интерфейс и выходящих
		  через <i>eth3</i>  на адреса не из сети <i>192.168.0.0/16</i> (принцип тот же).</li>
		</ul>
</ul>
и в обратном направлении соответственно (просто меняем source и destination местами).
</p>

<p>
Смотрим как устроена цепочка <b>05_LAN_INET</b>. Принцип тот же, что и в цепочках,
рассмотренных выше. Но есть некоторые отличия.
</p>

<p>
Первое отличие - наличие цепочки <b>05_LAN_INET_LIMIT</b>. Если в цепочках "сеть-шлюз"
органичение на скорость создания новых подключений ограничивалась непосредственно
в разрешающем правиле, то здесь удобно иметь глобальные (на всю сеть <b>LAN</b>) ограничения. 
Поясню как это работает. Но сначало о том, что привело к появлению цепочки <b>05_LAN_INET_LIMIT</b>.
</p>

<p>
Если в сети <b>LAN</b> появятся зараженные компьютеры, то с высокой вероятностью
они станут рассылать спам и прочую ерунду посредством SMTP-протокола, напрямую
подключаясь к различным SMTP-серверам в Интернете. Не полагайтесь на наличие антивируса:
всегда найдется машина, не защищенная в данный момент антивирусом (сотрудник или
клиент принес свой ноутбук) либо найдется вирус, который
не дететктируется установленным антивирусом. 100%-ый выход один - заблокировать
подключение на <i>tcp 25</i> на сетевом уровне. Однако при этом пользователи сети теряют
возможность отправлять почту через внешние SMTP-сервера посредством почтовых агентов.
Если это не проблема для сотрудников и/или у вас в сети есть свой собственный SMTP-сервер
(естественно с SMTP-аутентификацией для защиты от неавторизованной рассылки), то можно решить эту проблему и так.
А можно поступить чуть менее жестко. 
</p>

<p>
Можно вместо жесткого запрета, сделать исключения для некоторых, нужных вам серверов,
например популярных почтовых сервисов. Как раз правила в <b>05_LAN_INET_SKIP_OR_DENY</b>
и разрешают доступ к <i>25 tcp</i> портам некоторых почтовых сервисов. А в <b>05_LAN_INET_DENY</b>
есть запрещающее правило на <i>dst tcp 25</i>. Таким образом мы запрещаем подключения на
<i>25 tcp</i> порт всех хостов в Интернете за исключением перечисленных в <b>05_LAN_INET_SKIP_OR_DENY</b>.
</p>

<p>
Второй вариант - лимитировать кол-во подключений на <i>25 tcp</i> в единицу времени.
Так как задача вируса или бота - разослать как можно быстрее и больше, то можно
заблокировать слишком часто идущие попытки подключения на <i>25 tcp</i>.
Правда это подразумевает, что бот остылает одно (или мало) сообщений через одно
подключение (SMTP-сессию). 
</p>

<p>
Для этого можно воспользоваться модулями iptables <i>limit</i>, <i>hashlimit</i> или <i>recent</i>.
</p>

<p>
В нашем примере использован <i>limit</i> - разберем сначала его.
В <b>05_LAN_INET_LIMIT</b> первое правило говорит: <b>"Разрешить новые подключения на tcp порт <i>25</i>
не чаще 2-х раз в минуту для каждой комбинации <i>"адрес источника-адрес приемника"</i>
и допускается "сжечь"(использовать) до пяти подключений без контроля (промежутка времени)"</b>.
</p>
<p style="font-size:13px">
Перескажу своими словами, как работает этот алгоритм:
в некий <i>"счетчик"</i> (<i>"корзину" - bucket</i>) емкостью (<i>burst</i>) 5, с постоянной частотой 2 раза
в минуту <i>"капают"</i> (поступают) разрешения (<i>"билеты"</i>) на прохождение пакета (т.е. чтобы "корзина"
заполнилась "до краев" надо около 2,5 минут). После заполнения "до верху" разрешения уже не капают,
а ждут пока в "корзине" появится свободное место. Свободное место появляется при
выдаче разрешения на прохождение одного пакета: одно разрешение - один пакет.
Теперь, пусть клиент из сети пытается подключиться к 25-му порту 10 раз в течении 10 секунд
(отправить 10 "NEW-пакетов" за 10 секунд).
Произойдет следующее: накопившиеся в корзине 5 разрешений будут "мгновенно" использованы
и 5 подключений будут сделаны в кратчайшие сроки и корзина опустошится: разрешениий
на прохождение пакетов больше нет. Теперь оставшиеся 5 пакетов смогут уйти,
только если в корзине появятся разрешения для них (по одному на пакет): поэтому
клиенту для отправки следующего (6-го) пакета придется ждать пока в корзину "капнет" очередное разрешение.
А так как капают они по 2 в минуту, то чаще двух подключений в минуту
клиент уже сделать не сможет. Если он снизит частоту своих подключений ниже 2/мин,
то разрешения снова смогут накапливаться до 5 (емкости корзины).
<br>
<a target="_blank" href="http://en.wikipedia.org/wiki/Token_bucket">http://en.wikipedia.org/wiki/Token_bucket</a>
</p>

<p>
Подключения, идущие чаще 2/минуту, не будут разрешены первым правилом и проверка
"проваливается" дальше: на второе и третье правила.
Ну а третье правило просто "убивает" пакет, который бы привысил заданную частоту.
А вот второе правило записывает в системный лог (и возможно выдает на консоль) 
сообщение, но тоже с частотой не превышающей
определенной заданной. Дело в том что, если пакеты, не подпадающие под лимит
будут идти с большой частотой, то в лог (и на консоль!) будут, с этой же частотой,
добавляться записи о каждом пакете (а их может быть ОЧЕНЬ много).
Для ограничения записи в лог используется тот же модуль <i>limit</i>.
Т.е. в логе мы увидим запись не о каждом пакет, превысившим частоту предыдущего правила,
а только о части, не превысившей ограничений команды <i>...-j LOG</i>   
Таким образом мы можем ограничить частоту подключений до желаемой величины, практически
не даваю боту из сети забивать ваш канал и "сильно беспокоить" SMTP-сервера.
<b>Однако соединения хоть и редко, но будут осуществляться.
Вообще этот модуль служит для защиты от DOS-атак, когда не надо полностью блокировать
доступ.</b>
</p>
<p>
Можно применить оба похода одновременно, что и видно на примере: ограничены сервера,
на которые можно пдключаться и частота, с которой к ним можно подключаться.
</p>
<div class="excl">
Обратите внимаение, что лимитровать надо <b>только частоту <i>NEW</i>-пакетов</b> (они же SYN для tcp),
а не частоту пакетов уже установленных соединений!
</div>

<p>
Вместо модуля <i>limit</i> можно использовать использовать модуль <i>recent</i>.
Для этого вместо трех команд, приведенных в примере, можно использовать такие: 
<pre class="cmd">
iptables -A 05_LAN_INET_LIMIT -p tcp --dport 25 -m recent --update --seconds 120 --hitcount 5 --name LAN_INET_25 --rdest \
  -m limit --limit 1/min --limit-burst 5 -j LOG --log-prefix `LAN->INET_SMTP_limit: '  
iptables -A 05_LAN_INET_LIMIT -p tcp --dport 25 -m recent --update --seconds 120 --hitcount 5 --name LAN_INET_25 --rdest -j DROP 
iptables -A 05_LAN_INET_LIMIT -p tcp --dport 25 -m recent --set --name LAN_INET_25 --rsource -j ACCEPT
</pre>
что даст нам следующее
<pre class="cmd">
0     0 LOG        tcp  --  *      *       0.0.0.0/0            0.0.0.0/0           tcp dpt:25 recent: UPDATE seconds: 120 hit_count: 5 name: LAN_INET_25_LOG side: dest limit: avg 6/hour burst 5 LOG flags 0 level 4 prefix `LAN->INET_SMTP_limit: '
0     0 DROP       tcp  --  *      *       0.0.0.0/0            0.0.0.0/0           tcp dpt:25 recent: UPDATE seconds: 120 hit_count: 5 name: LAN_INET_25 side: dest
0     0 ACCEPT     tcp  --  *      *       0.0.0.0/0            0.0.0.0/0           tcp dpt:25 recent: SET name: LAN_INET_25 side: source  
</pre>
и означает: <b>"если с определённого адреса за 120 секунд или менее будет происходить более 5
попыток подключения" - "убивать" пакеты с этого адреса в дальнейшем до тех пор,
 пока частота не снизится до <i>"не более 5-ти пакетов за 120 секунд"</i></b>.
Это условие описывается двумя правилами, а не одним (подробнее читайте документацию recent) 
</p>

<p>
Какой эфект даст <i>recent</i> в сравнении с <i>limit</i>?
После момента превышения частоты и до её спада ниже заданного уровня <i>recent</i>
<b>полность болкирует трафик</b>, а <i>limit</i> "цедит в час по чайной ложке" как
указано в параметре <i>--limit</i>. Но в <i>limit</i> можно указать отслеживание лимита
по комбинации <i>"источник-приёмник"</i>, а у <i>recent</i> либо только <i>"источник"</i>,
либо только <i>"примёник"</i>: c <i>limit</i> можно ограничить каждую пару IP (source-destination)
в отдельности, а с <i>recent</i> - только всю сеть, как единое целое.
</p>

<p id="filter_INET_DMZ">
<h4><span style="background-color:#E5D8A1">
07_INET_DMZ, 08_DMZ_INET: Интернет - DMZ</h4>
Как уже говорилось ранее сеть <b>DMZ</b> предназначена для размещения внешних сервисов.
В нашем примере часть сервисов расположена на самом шлюзе (хотя это и является плохой практикой!), а в <b>DMZ</b> расположены
два web-сервера: один с адресом <i>203.0.113.2</i> и другой с адресом <i>203.0.113.3</i>. Предполагается, что доступ к серверам в <b>DMZ</b> извне осуществляется
только через провайдера <b>ISP1</b> по внешнему адресу 198.51.100.2,
по причинам, описанным  в разделе <a href="#filter_INET_GW">Интернет - шлюз</a>.  
Рассмотрим соответствующие цепочки. 
</p>

<p>
Ситуация похожа на рассмотренную "трафик между двумя сетями, идущий через шлюз".
Смотрим на схему и листинг цепочек и видим, что для этих цепочек мы отбираем следующие пакеты.

</p>
<ul>
Для пути через провайдера <b>ISP1</b>:
		<ul>
		<li>входящие в интерфейс <i>eth1</i> и выходящие через <i>eth2</i> с адресом 
    сети назначения <i>203.0.113.0/29</i>;</li>  
		<li>входящие из сети <i>203.0.113.0/29</i> на интерфейс <i>eth2</i> и 
    выходящие через <i>eth1</i>.</li>
		</ul>
Для пути через провайдера <b>ISP2</b>:
		<ul>
		<li>входящие в интерфейс <i>eth3</i> и выходящие через <i>eth2</i> с адресом 
      сети назначения <i>203.0.113.0/29</i>;</li>  
		<li>входящие из сети <i>203.0.113.0/29</i> на интерфейс <i>eth2</i> и 
      выходящие через <i>eth3</i>.</li>
		</ul>
</ul>

<p>
Принцип построения цепочек такой же как и в остальных.
Особо можно отметить, что здесь контролируются не только входящие порты (т.е.
на какие порты можно подключиться извне), но и исходящие порты: куда и на какие
порты в Интернете могут подключаться сервера из <b>DMZ</b>.
</p>

<p>
<div class="excl">
Кроме того, очевидно, что контролировать доступ можно в двух местах: как на шлюзе
<b>Gateway</b>, так и на самих серверах в <b>DMZ</b>. Контроль на шлюзе имеет определенные
преимущества: централизация, в случае взлома/заражения сервера <b>DMZ</b>, его собственный контроль
уже, возможно, не будет функционировать. Шлюз же в этом случае будет продолжать
блокировать нежелательный трафик. Поэтому рекомендую, как основной, контроль именно на шлюзе.
</div>
</p>

<p>
Так же как и в цепочках сединения <a href="#filter_INET_GW">Интернет - шлюз</a>,
для входящих на <i>eth3</i> через <b>ISP2</b> пакетов создана специальная цепочка
<b>07_INET_DMZ_ISP2</b>, в которой нет разрешений для установления новых соединений из сети Интернет.
</p>

<p id="filter_LAN_DMZ">
<h4><span style="background-color:#9F9BAD">
09_LAN_DMZ, 10_DMZ_LAN: Локальная сеть - DMZ</span></h4>
Основная особенность у этих цепочек - <b>однонаправленность для новых соединений</b>:  
хосты из сети <b>LAN</b> должны иметь возможность подключаться (устанавливать новые 
соединения) к хостам сети <b>DMZ</b>, а вот наборот: хосты из сети <b>DMZ</b> к хостам сети <b>LAN</b> - нет.
</p>

<p>
Для этих цепочек мы отбираем пакеты:
		<ul>
		<li>входящие в интерфейс <i>eth0</i> из сети <i>192.168.0.0/24</i> и выходящие
		   через <i>eth2</i> с адресом сети назначения <i>203.0.113.0/29</i></li>  
		<li>входящие из сети <i>192.168.1.0/24</i> на любой ppp-интерфейс и выходящие
		   через <i>eth2</i> с адресом сети назначения <i>203.0.113.0/29</i>
		</ul>
и в обратном направлении, соответственно.
</p>

<p>
<div class="excl">
Обратите внимание на цепочку <b>10_DMZ_LAN</b> - видно, что там разрешены только
<i>RELATED,ESTABLISHED-пакеты</i>, т.е. из <b>DMZ</b> невозможно установить новое соединение
с хостами сети <b>LAN</b>, что и требуется.
</p>
</div>

<p>
Второй момент, на который можно обратить внимания - правила, где есть  <i>tcp spts:60000:61000</i>.
Они связаны с протоколом FTP через SSL 
(как и tcp <i>spt:20</i> разделе <a href="#filter_LAN_GW">Локальная сеть - шлюз</a>).
Только приём с открытием 20 порта здесь "не пройдет" из-за <i>active mode</i> протокола FTP!
<br>
Ведь в этом случае мы будем вынуждены разрешить подключения из <b>DMZ</b> в <b>LAN</b>,
чего мы крайне не хотим!
</p>
<p>
Выход один - использовать пассивный режим (passive mode) в котором все соединения
инициирует только клиент  
(см. <a target="_blank" href="http://slacksite.com/other/ftp.html#passive">Active FTP vs. Passive FTP, a Definitive Explanation</a>).
Для этого на серверах в  <b>DMZ</b> надо сконфигурировать ftp-сервер на прослушивание
фиксированного диапазона портов, на которые клиент будет подключаться для передачи данных.
Вот как раз прохождение пакетов на этот диапазон портов и нужно будет разрешить на шлюзе
в потоках <b>LAN</b>-><b>DMZ</b>.  
</p>

<h4 id="filter_ct">Фильтрация и отслеживание соединений (connection tracking)</h4>
<p>
В рассматриваемом варианте организации правил используются возможности NETFILTER 
по отслеживанию состояния соединений (stateful packet inspection), что необходимо
учитывать в ситуциях, подобной описанной ниже.   
</p>
<p>
Пусть для некоего хоста в данный момент разрешено прохождение пакетов:
установление новых соединений разрешают правила типа <i>-m state NEW</i>,
а прохождение остальных пакетов этих соединений разрешаются правилами типа <i>-m state RELATED, ESTABLISHED</i>.  
Пусть, начиная с какого-то момента исчезает необходимость в разрешении трафика 
данному хосту: для него планируется полностью удалить все правила
(удалить хост из конфигурации), а не просто запретить трафик,
явно добавив запрещающие правила. 
При этом подразумевается, что после удаления всех, в т.ч. разрешающих, правил,
обмен пакетами для данного хоста тут же станет невозможным.
</p>
<p>
Однако только удаления рарзрешающих правил в такой ситуации недостаточно.
Т.к. остаются правила типа <i>-m state RELATED, ESTABLISHED</i>,
то хост сможет получать/отправлять пакеты в рамках ранее установленных соединений
до момента их разрыва: явного или по таймауту. Например, если в момент удаления
из конфигурации хост загружал некий файл, то и после удаления правил хост сможет
продолжать загружать данный файл, пока он полностью не будет загружен.       
</p>

<p>
Чтобы одновременно с удалением правил для хоста из конфигурации сразу же блокировался
и весь связанный с хостом трафик, необходимо либо явно удалить все соединения
данного хоста из таблицы connection tracking, либо перед удалением хоста из 
конфигурации предварительно ввести для него запрещающие правила и дождаться разрыва
всех соединений по таймауту и только потом удалять все правила.
</p>
<p>
Для удаления отслеживаемых соединений а также других манипуляций с соединениями
необходима утилита <i>conntrack</i> из пакета <i>conntrack-tools</i>.
</p>

<p>
Для работы утилиты conntrack необходимо предварительно загрузить модуль ядра <i>ip_conntrack</i>. 
<pre class="cmd">
modprobe ip_conntrack # загружаем модуль ядра
</pre>
И далее, например, чтобы удалить все соединения, установленные хостом с адресом 192.168.1.3
надо выполнить команду: 
<pre class="cmd">
conntrack -D -s 192.168.1.3 
</pre>
</p>

</p>
По этой же причине, для того чтобы запрещающие правила действовали и на уже 
установленные соединения, их необходимо вводить перед <i>RELATED, ESTABLISHED</i>-правилами. 
См. цепочку <b>05_LAN_INET</b> в листинге таблицы <i>filter</i>.
</p>
<p>
Если же вам достаточно запрета только новых соединений, то запрещающие правила 
оптимальней вводить после <i>RELATED, ESTABLISHED</i> для уменьшения кол-ва
правил, по которым проверяется каждый пакет данного потока.
</p>

<br>
<br>
<h2 id="routing">Маршрутизация (routing)</h2>
<p>
Маршрутизация - процесс в котором определяется путь, по которому пойдет трафик.
Поэтому для любого трафика должен быть каким-то образом определен
(задан) маршрут, каким он пойдет до цели.
</p>

<div class="excl">
В данном примере мы сосредоточимся только на процессе маршрутизации на границе сети -
между нашим шлюзом и провайдером(ами).
Собственно в рассматриваемом примере есть только один маршрутизатор - шлюз, 
на котором и происходит весь процесс маршрутизации.
</div>

<p>
Посему, сначала, кратко рассмотрим устройство сети Интернет и варианты подключения к нему.
</p>

<br>
<h3 id="routing_internet">Немного об устройстве сети Интернет. Автономные системы</h3>
<p>
Сеть Интернет состоит из автономных систем (Autonomous System, AS).
AS представляет собой группу адресов (префиксов, т.е. фактически IP-подсетей) с единой административной
политикой маршрутизации. AS являются (самыми) крупномасштабными блоками из
которых "собран" Интернет. AS не имеют ничего общего с физической инфраструктурой - 
это логическая структура, поверх физической, функционирующая на третьем, сетевом уровне модели OSI, 
на котором осуществляется маршрутизация. 
В этом собственно и суть Интернета - объединение разнородных физических сетей
на более высоком уровне едиными протоколами адресации (IP) и маршрутизации (BGP).
</p>
<p>
Например, провайдер вместе со всеми своими клиентами может быть одной такой AS.
У Яндекса есть своя AS (AS13238 YANDEX Yandex LLC,RU), у Гугла несколько (напр. AS15169 GOOGLE - Google Inc.,US),
у различных операторов сетей, коммерческих организаций и пр. и пр. 
Их много - несколько десятков тысяч (см. например здесь 
<a href="http://bgp.potaroo.net/cidr/autnums.html">http://bgp.potaroo.net/cidr/autnums.html</a>)
Клиент, подключившись к провайдеру, становится частью его AS, если он не получил свой
номер AS у регионального оператора и не организовал свою AS.
Вот собственно между AS (и через них) трафик и "ходит". Как трафик "ходит" внутри AS
дргуие AS (т.е. весь остальной Интернет) "не волнует" - это забота администраторов сетей, 
составляющих данную AS. Т.е. пограничные маршрутизаторы AS являются границой 
между "внутренним миром" данной AS и всем остальным миром, аналогично тому как шлюз(ы)
и фаерволы вашей сети отделяют её от окружающего мира. 
AS с публичным номером имеет, как минимум, два подключения к двум другми AS. 
Т.о. Интернет представляет собой буквально сеть из множества соединений между тысячами AS.
</p>
<p>
Маршрутизация между AS (т.е. определение каким путем, через какие промежуточные AS, трафик попадет
из одной AS в другую) осуществляется пограничными маршрутизаторами этих AS c использованием
протоколов внешней маршрутизации. На 2014 г это BGP4. 
То, какие сети (IP-адреса) входят в в данную AS передается (анонсируется) всем другим AS с помощью
все того же протокола BGP. Т.о. пограничные маршрутизаторы каждой AS, потенциально могут узнать обо
всех имеющихся адресах сетей во всех остальных AS в Интернете (в мире) и могут сами выбирать
маршрут доставки пакетов в целевую сеть/AS. 
Наряду с анонсированием префиксов другим AS, каждая AS так же может фильтровать префиксы,
т.е. попросту игнорировать информацию о тех или иных адресах, приходящих от других AS.
Каждая AS (т.е. её владельцы) сама решает какие префиксы и какой трафик она будет 
принимать себе или через себя пропускать в другие AS.
Т.е. пограничные маршрутизаторы могут выполнять для всей AS роль неких 
"фаерволов маршрутизации" по отношению к окружающему миру.
</p>
<p>
Т.о. связность в Интернете зависит не только от состояния физических каналов связи, но и
например, от того какие адреса AS анонсирует другим, какие игнорирует от других, а так же
от договоренностей (человеческий фактор!) о прохождении трафика между AS.   
</p>
<p>
Для организации собственной AS помимо прочего, необходимо получить публичный номер AS - ASN.
Номер можно получить либо у регионального интернет-регистратора (RIR), либо у
локального интернет-регистратора (LIR).
RIR'ов всего в мире пять: AfriNIC, ARIN, APNIC, LACNIC, RIPE NCC.
LIR'ы - это те кто получает блоки ip-адресов от RIR'ов и распределяет их между своими клиентами.
Чаще всего LIR'ы - это провайдеры. 
Вот <a href="https://www.ripe.net/membership/indices/RU.html">https://www.ripe.net/membership/indices/RU.html</a>, например, LIR'ы для РФ. 
ASN полученный от RIR переносим между провайдерами, а ASN полученый от LIR 
привязан к данному провайдеру. 
</p>

<p>
Провайдеры к которым подключены конечные пользователи, поключены и маршрутизируют трафик 
провайдерам более высокого уровня (upstream provider),
которые в свою очередь маршрутизируют трафик провайдерам, которые являются 
провайдерами верхнего уровня. <a href="http://en.wikipedia.org/wiki/Tier_1_network">http://en.wikipedia.org/wiki/Tier_1_network</a>
Так же в Интернете существуют точки обмена трафиком, к которым подключены провайдеры
для прямого обмена трафиком, вместо транзитного, через AS. Наконец, провайдеры
организуют прямой обмен трафиком между собой (peering).
</p>

<p>
Т.о. подавляющее большинство "простых смертных" подключено к этой иерархии 
в самом низу - к провайдерам третьего уровня.
</p>

<br>
<h3 id="routing_conn">Варианты организации подключения к сети Интернет</h3>
<p>
Под <b>подключением</b> будем понимать организацию связи с ISP или IX на 
сетевом уровне (OSI Layer 3) с выделением клиентскому хосту IP-адреса(ов).  
</p>

<p>
Варианты огранизации подключения к провайдерам можно свести к следующим:
<ul>
<li><a href="#routing1woas">одно подключение к одному провайдеру</a>;</li>
<li>два или более подключения к одному или разным провайдерам (<b>multihoming network</b>): 
  <ul>
    <li><a href="#routing2woas">без организации AS</a>;</li> 
    <li><a href="#routing2wprivas">c организацией private AS</a>;</li> 
    <li><a href="#routing2wspubas">c организацией stub public AS</a>;</li> 
    <li><a href="#routing2wtpubas">c организацией transit public AS</a>;</li> 
  </ul>
  <li><a href="#routing2ix">подключение к <b>точке обмена трафиком</b> (<b>IX</b>)</a>.</li>
</ul>
</p>
<p>
<div class="excl">
В нашем варианте рассматривается один маршрутизатор (шлюз) через который наши сети
подключены к провайдеру(ам). Однако маршутизаторов может быть несколько как у клиента,
так и у провайдера(ов), так что подвариантов еще больше:
клиент может иметь несколько пограничных маршрутизаторов, каждый из которых 
может быть подключен к одному или нескольким провайдерским маршрутизаторам.
</p>
</div>

<div class="excl">
В каждом из указанных случаев маршрутизация <b>исходящего</b> и
<b>входящего</b> трафика и возможности управления этим процессом различны.
</div>

<p>
Так же, озвучим, какие задачи решает/как можно использовать несколько подключений: 
<ul>
  <li>отказоустойчивость связи с сетью Интернет, т.е. высокая доступность Интернета из сетей 
  клиента и доступность публичных ресурсов клиента из сети Интернет;</li> 
  <li>увеличение пропускной способности за счет одновременного использования нескольких каналов. 
      Различный трафик можно направить по разным подключениям;</li> 
  <li>оптимизация расходов за счет возможности управлять маршрутизацией трафика 
      через различные подключения. Данный пункт в основном имеет смысл 
      при эксплуатации собственной AS.</li> 
</ul>
</p>

<p>
При организации подключения необходимы IP-адреса, которые могут быть:
<ul>
  <li><b>провайдероагрегируемыми</b> (<b>Provider-aggregatable</b> address space, PA-adresses) или</li>
  <li><b>провайдеронезависимыми</b> (<b>Provider-independent</b> address space, PI-adresses).</li>
</ul>
</p>
<p>
<b>PA-адреса</b> выделяются RIR'ами LIR'ам (обычно крупными блоками, напр. размером /22).
Далее LIR'ы выделяют индивидуальные адреса и блоки адресов конечным пользователям.
Агрегируемые же они потому, что LIR может анонсировать другим АС не адреса каждого
своего клиента в отдельности, а агрегированные префиксы, т.е. фактически крупные блоки
выделенные RIR'ом. Это сокращает объем передаваемой информации о маршрутах между АС
и уменьшает объем таблиц маршрутизации.  
Т.к. адрес фактически "принадлежит" данному LIR'у (т.е. провайдеру), то данные адреса
<i>провайдерозависимы</i> и клиент не может использовать их на подключении к другому провайдеру.
</p>

<b>PI-адреса</b> выделяются RIR'ами напрямую конечным пользователям.
Поэтому, клиент имеющий в своем распоряжении PI-адреса может подключиться с этими
адресами к любому провайдеру, а провайдер проанонсирует PI-адреса клиента в сеть Интернет.
Т.к. PI-адреса не входят в адресное пространство, выделенное провайдеру, то он 
не сможет агрегировать PI-адреса со своими и будет анонсировать их как есть.
</p>

</p>
Т.о. образом <b>PA-адреса</b> клиент получает от каждого из провайдеров, к которому
он подключен. Кол-во адресов, которое сможет выделить провайдер клиенту, в первую очередь,
зависит от наличия свободных адресов у провайдера. 
</p>
<p>
Получением <b>PI-адресов</b> клиент (организация) должен озаботиться сам.
Стоит отметить, что у RIR'ов есть определенный регламент выдачи адресов, поэтому процедура
сложнее, чем при выделение адресов при подключении к провайдеру.
Например, в RIPE IPv4 PI-адреса уже все выделены и свободных нет, а минимальный выделяемый
блок - /24, т.е. 256 адресов (а LIR'ам - /22 = 1024 адреса).
<br>Например, см. здесь: <a href="http://www.ripe.net/ripe/docs/ripe-606">Pv4 Address Allocation and Assignment Policies for the RIPE NCC Service Region. Publication date: 06 Feb 2014 </a>   
</p>

<p>
В вариантах подключения с организацией AS необходимо получить и номер автономной системы (ASN).
ASN можно, аналогично IP-адресам, получить либо у RIR'а, либо через LIR'а, т.е. провайдера. 
</p>

<p>
<a href="http://tools.ietf.org/html/rfc4116">RFC 4116: IPv4 Multihoming Practices and Limitations</a>
</p>



<br>
<h3 id="routing_gen">Маршрутизация при различных вариантах подключения</h3>
<p>
Переходим к рассмотрению деталей для каждого из вариантов подключения.
</p>

<p>
Для общего понимания рекомендуется к прочтению серия статей <a href="http://linkmeup.ru/blog/129.html">Сети для самых маленьких</a>.
</p>

<p>
И сразу же основное и универсальное правило:
<div class="excl">
В любом из вариантов задача маршрутизация <b>исходящего</b> трафика (т.е. через какое из 
нескольких имеющихся подключений пойдет исходящий) <b>решается на стороне клиента</b> с использованием 
статической или динамической маршрутизации.  
</div>
</p>

<br>
<h4 id="routing1woas">Одно подключение к одному провайдеру <i>без организации AS</i></h4>
<p>
Этот вариант, видимо, самый распространенный. При таком подключении клиент получает
минимум один публичный PA IP-адрес или, для IPv4, glue-сеть размером /30: сеть, хост, шлюз, широковещательный.
В дополнение к этому можно арендовать у провайдера PA IP-подсеть и разместить хосты
у себя за маршутизатором (напр. организовать DMZ). Можно арендовать и несколько 
различных подсетей, хотя, видимо, это особого смысла не имеет.
</p>
<p>
С маршрутизацией все однозначно: статическая маршрутизация с маршрутом по умолчанию
(и для DMZ) через единственный шлюз с публичным адресом и со стороны клиента и со стороны провайдера.  
</p>
<p>
Подробно этот вариант подключения рассмотрен в разделе <a href="#routing_example">"Маршрутизация в рассматриваемом примере. Одно внешнее подключение"</a>.
</p>

<br>
<h4 id="routing2woas">Несколько подключений к одному или разным провайдерам <i>без организации AS</i></h4>
<p>
Данный вариант, очевидно, второй по распространнености, т.к. является "естественным продолжением"
предыдущего. Как и в варианте с одним подключением, на каждое новое подключение выделяется новый IP-адрес
или glue-сеть из PA-адресов того провайдера, которому принадлежит данное подключение. 
</p>
<p>
Маршрутизация: статическая, плюс, при необходимости, policy routing и/или ручное 
переключение маршрута по умолчанию. 
Теоретически, для нескольких подключений к одному провайдеру, возможно использование
протоколов внутренней динамической маршрутизации (IGP: RIP, OSPF, IS-IS).
Практикуется ли в реальности провайдерами использование IGP для клиентских подключений - неизвестно.
<div class="excl">
Судя по <a href="http://linkmeup.ru/blog/65.html">Сети для самых маленьких. Часть восьмая. BGP и IP SLA </a>:
"<b>...ситуация, когда клиент поднимает IGP с провайдером, крайне редкая...</b>" 
</div>
</p>

<h5>Исходящий трафик</h5>
<p>
При наличии нескольких подключений появляется практическая возможность направить
исходящий трафик через различных провайдеров. Возможные варианты управления данным
процессом рассмотрены в разделе
<a href="#routing2">"Маршрутизация в рассматриваемом примере. Два внешних подключения (два разных провайдера)"</a>;</li> 
</p>

<h5>Входящий трафик</h5>
<p>
Естественно, появляется желание организовать доступ
к арендуемой у одного из провайдеров подсети через несколько подключений (через других провайдеров).
Однако, эта задача не рашается так же просто, как в случае исходящего трафика потому что
<b>контроль над маршрутизацией входящего трафика находится на стороне провайдера,
а не клиента</b>: каждый провайдер анонсирует только свои, выделенные ему RIR'ом префиксы (адреса).
</p>
<div class="excl">  
Если клиент арендует у провайдера IP-подсеть, то каждая подсеть доступна
извне только через провайдера-владельца данных адресов.
В данном варианте (без собственной AS) невозможно сделать так, чтобы
входящий трафик на IP-адреса, принадлежащие <b>провайдеру X</b>, маршрутизировался 
через <b>провайдера Y</b> (через подключение к Y). 
</div>
<p>
Другими словами, невозможно сделать так, чтобы входящий трафик к одному  
арендуемому у провайдера публичному IP-адресу, "шел" через других провайдеров.  
Полноценно эта задача решается только с помощью организации собственной публичной AS.
Поэтому, если вы хотите организовать входящие через несколько подключений, 
то в данном варианте доступны следующие варианты-"костыли": 
<ul>
  <li>прием входящего трафика на каждую арендованную клиентом подсеть в отдельности:
  <ul>
    <li>одни сервера доступны по адресам провайдера X, а другие  - по адресам провайдера Y;</li> 
    <li>сервер имеет один или несколько интерфейсов с адресами, принадлежащими разным провайдерам;</li>
  </ul>
  <li>прием входящего трафика на одну подсеть: непосредственно на адреса провайдера-владельца подсети
  и с использованием port forwarding (DNAT плюс SNAT в обратке) на адреса подключений других провайдеров.
  Более подробно этот подключения рассмотрен в разделе 
  <a href="#routing2in">"Маршрутизация в рассматриваемом примере. Входящий трафик через двух провайдеров"</a>;</li> 
  <li><b>в дополнение к этим вариантам</b> можно использовать <b>round-robin DNS</b> или 
  <b>dynamic DNS</b> для доступа  <b>к разным IP-адресам по одному и тому же доменному имени</b>.</li>
</ul>
</p>

<br>
<h4 id="routing2wprivas">Несколько подключений к одному провайдеру <i>c организацией private AS</i></h4>
<p>
Отличие данного варианта от предыдущего - использование протокола BGP вместо 
статической маршрутизации. Возможно использование как PA и PI-адресов.
На каждое подключение к AS провайдера используется свой private ASN. 
Т.о. в рамках нескольких подключений возможно управлять маршрутизацией как исходящего 
так и входящего трафика средствами BGP.
http://isp-servis.ru/config_extended.html<br>
http://xgu.ru/wiki/BGP_2_ISP<br>
http://www.linux.org.ru/forum/admin/9318839<br>
uilding Reliable Networks with the Border Gateway Protocol Chapter 6 Traffic Engineering http://oreilly.com/catalog/bgp/chapter/ch06.html<br>
Sam Halabi Internet Routing Architectures, Second Edition<br> 
</p>
<h5>Исходящий трафик</h5>
TODO
<h5>Входящий трафик</h5>
TODO

<br>
<h4 id="routing2wspubas">Несколько подключений к одному или разным провайдерам  <i>c организацией stub public AS</i></h4>
<a href="http://www.opennet.ru/base/cisco/bgp_balancing.txt.html">http://www.opennet.ru/base/cisco/bgp_balancing.txt.html</a>
<p>
<b>stub AS</b> означает, что через вашу AS трафик далее (в другие AS) проходить не будет - входящий трафик
предназначен только для сетей вашей AS.
Организация такого подключения сводится к следующему:
<ul>
<li>необходимо получить публичный номер автономной системы (public ASN) у LIR'а или RIR'а;
<b>Что надо учитывать</b>: владельцами AS практикуется фильтрация маленьких сетей.</li>
<li>необходимо иметь собственный (PI) или арендованный у провайдера (PA) блок IP-адресов. 
В случае использования арендованного блока необходимо согласовать с арендодателем 
анонсирование этого блока через других провайдеров;</li>
<li>на клиентском пограничном маршрутизаторе (в нашем случае это интернет-шлюз) 
необходимо ПО, обеспечивающее работу протокола BGP. 
Это может быть как сервер с ОС и ПО, работающим с BGP, например Linux-дистрибутив с ПО Quagga или BIRD.
так и "аппаратный" маршрутизатор Cisco, Juniper и т.п.
<b>Маршрутизатор должен иметь кол-во ОЗУ, достаточное для размещения
нескольких экземпляров полных таблиц маршрутизации (по экземпляру на каждого подключенного провайдера)</b>;
</li>
<li>всё это надо настроить на стороне клиента (забота клиента) и 
на стороне провайдера (забота админов провайдера). Провайдер включит и настроит 
BGP на своей стороне, клиент настроит анонс своих сетей;</li>
<li>плюс энное кол-во тонкостей при эксплуатации, связанное с особенностями 
анонсирования и фильтрации маршрутов другими участниками сети Интернет.</li>
</ul>
</p>
<h5>Исходящий трафик</h5>
TODO
<h5>Входящий трафик</h5>
TODO

<br>
<h4 id="routing2wtpubas">Несколько подключений к одному или разным провайдерам  <i>c организацией transit public AS</i></h4>
<p>
Отличие данного варианта от <b>stub AS</b> в том что изначально подразумевается,
что через вашу AS может передаваться чужой трафик в какие-то другие AS. Т.е.
такая AS будет полноценной частью Интернета со всей вытекающей ответсвенностью 
и требованиями к надежности работы вашей AS. 
</p>

<h5>Исходящий трафик</h5>
TODO
<h5>Входящий трафик</h5>
TODO

<br>
<h4 id="routing2ix">Подключение к <b>точке обмена трафиком</b> (<b>IX</b>)</h4>
<p>
TODO
</p>

<h5>Исходящий трафик</h5>
TODO
<h5>Входящий трафик</h5>
TODO

<br>
<h3 id="routing2_hwredundancy">Отказоустойчивость подключения: резервирование подключений, каналов и шлюза</h3>

<p>
Введем следующие понятия.
<dl>
<dt><b>Резервированием подключения (Layer 3 Redundancy)</b></dt>
<dd>
будем называть организацию двух и более подключений
<i>к одному или разным</i> провайдерам на третьем, сетевом уровне модели OSI. 
Каждое такое подключение будет иметь свой сетевой адрес (публичный IP).
Для маршрутизации трафика на этом уровне используются протоколы внешней (BGP) или 
внутренней маршрутизации (RIP, OSPF, ISIS) или статическая маршрутизация.
</dd>

<dt><b>Резервированием канала (Layer 2 Redundancy)</b></dt> 
<dd>
будем называть организацию двух и более подключений
к одному провайдеру на втором, канальном уровне модели OSI, которое на 
сетевом, более высоком уровне, будет представлять собой <b>одно поключение</b>. 
Такое подключение будет иметь один сетевой адрес и несколько MAC-адресов (по кол-ву резервирующих линий).
На этом уровне для переключения каналов в сетях Ethernet 
(что чаще всего и используется в городах) применяется протокол STP. 
</dd>
</dl>
Т.о. в каждом подключении (L3) возможно резервирование канала (L2).
</p>

<p>
Проанализируем отказустойчивость рассмотренных выше вариантов подключения.
<h5>Одно подключение к одному провайдеру <i>без организации AS</i></h5>
<p>
Отказустойчивость отсутствует.
</p>
<br>
<h5>Несколько подключений к одному или разным провайдерам <i>без организации AS</i></h5>
<p>
Отказустойчивость ограничена. 
<br><b>Исходящиие соединения</b>. В случае отказа одного из маршрутов исходящие 
соединения (трафик) без особых последствий  довольно легко переключить на другое подключения
средствами маршрутизации шлюза.
<br><b>Входящие соединения</b>. 
При неработоспосбности подключения становятся недоступны как сервисы на самом 
публичном адресе подключения (если таковые есть, напр. OpenVPN на <i>eth5</i> в нашем примере),
так и клиентские подсети, маршрутизируемые через это подключения (напр. <b>DMZ</b> в нашем случае).
Т.к. здесь нет AS, то нет возмоджности оперативно менять маршрутизацию входящих.
Можно лишь получить у провадеров <b>IP-подсеть для каждого подключения</b> и 
назначенить на каждый публичный ресурс нескольких различных 
адресов из различных выделенных подсетей (multihomed server). Однако остается проблема того, 
что у публичного ресурса несколько адресов, и клиент должен выбирать тот или иной 
адрес для подключения в зависимости от его (не)доступности. 
Либо клиент делает это "вручную" либо можно применить round-robin DNS или dynamic DNS. 
В целом это "костыли".
</p>
<br>
<h5>Несколько подключений к одному провайдеру <i>c организацией private AS</i></h5>
<p>
</p>
<h5>Несколько подключений к одному или разным провайдерам  <i>c организацией stub public AS</i></h5>
Возможна полноценная отказустойчивость. 
<br><b>Исходящиие соединения</b>. В случае отказа одного из маршрутов исходящие 
соединения (трафик) без особых последствий  довольно легко переключить на другое подключения
средствами маршрутизации шлюза (статической или динамической).
<br><b>Входящие соединения</b>. 
Т.к. здесь имеется AS, а для маршрутизации используется BGP, то есть возможность
средствами BGP настроить отказусточивость анонсируя разным првайдерам одни и те же клиентские подсети,
но с разными параметрами (Announcing More Specific Routes, AS path prepending, ????).
http://oreilly.com/catalog/bgp/chapter/ch06.html#77087
http://oreilly.com/catalog/bgp/chapter/ch06.html#77080

<h5>Несколько подключений к одному или разным провайдерам  <i>c организацией transit public AS</i></h5>
</p>

<p>
Т.о <b>зарезервировать подключение</b> можно двумя способами: организовав более одного
подключения <b>к одному или разным провайдерам</b>. При этом, очевидно, 
В случае нескольких подключений к одному провайдеру необходимо учитывать как организованы <b>каналы</b>:
подключены ли они одному и тому же маршрутизатору или к разным маршрутизаторам провайдера.
</p>


<p>
Т.о., например, <b>резервирование канала</b> ("два физических провода" идущих разными путями к одному 
провайдеру) защитит только от отказа одного из них на участке от вас до провайдерского 
оборудования, где эти каналы заканчиваются.
Чтобы застраховаться от отказа провайдерского оборудования надо <b>резервировать подключение</b>:
подключиться ко второму провайдеру, имеющему свои канала связи к провайдеру более высокого уровня
(upstream provider).
А подключившись к двум разным провайдерам можно и каналы к каждому из них зарезервировать тоже.
</p>


<br>
<h3 id="routing_example">Маршрутизация в рассматриваемом примере</h3>
<h4 id="routing1">Одно внешнее подключение (один провайдер)</h4>
<p>
Маршрутизацию осуществляет ядро Linux.
Настраивается же маршрутизация командой <i>ip</i> из пакета <i>iproute</i> или
коммандой <i>route</i> из пакета <i>net-tools</i>. <i>route</i> не позволяет управлять всеми аспектами маршрутизации
в Linux и является, в принципе, устаревшей.
Поэтому рассматривается управление только через команду <i>ip</i>.
</p>

<p>
Стоит отметить, что в примере рассмотрена только <i>статическая маршрутизация (static routing)</i>,
при которой автоматически не отслеживаются изменения топологии (связей между хостами) сети,
и таблицы маршрутизации не изменяются автоматически в ответ на изменение топологии.
<i>Динамическая маршрутизация (dynamic routing)</i> в данном примере не используется
поэтому упоминается только теоретически.
</p>


<p>
В случае использования одного подключения процесс очевиден, в том смысле, что
путь через данное подключение для исходящего и входящего трафика является единственно возможным.
Поэтому для исходящего трафика ничего кроме статической маршрутизации "по умолчанию" 
на шлюзе использовать не надо. Со входящим трафиком все так же однозначно - 
провайдер настраивает в своей сети один маршрут до вашего маршрутизатора.  
</p>

<h5>Одно подключение - исходящий трафик</h5>
<p>
Рассмотрим как происходит маршрутизация исходящего трафика (напр. из сетей <b>LAN</b> и <b>DMZ</b>), 
без учета того факта, что
в Интернет можно "попасть" и через <b>ISP2</b> (<i>eth3</i>), т.е. как-будто у нас только одно подключение. 
<br>Рассмотрим таблицу маршрутизации на шлюзе <b>Gateway</b> для нашего примера: 
</p>

<pre class="cmd">
#ip route show
192.168.1.1 dev ppp1  proto kernel  scope link  src 192.168.3.1.
192.168.1.3 dev ppp2  proto kernel  scope link  src 192.168.3.1.
192.168.255.2 dev tun1  proto kernel  scope link  src 192.168.255.1.
198.51.100.0/30 dev eth1  proto kernel  scope link  src 198.51.100.2.
203.0.113.0/29 dev eth2  proto kernel  scope link  src 203.0.113.1.
192.168.5.0/24 dev eth3  proto kernel  scope link  src 192.168.5.1.
192.168.0.0/24 dev eth0  proto kernel  scope link  src 192.168.0.1.
192.168.255.0/24 via 192.168.255.2 dev tun1.
default via 198.51.100.1 dev eth1.
</pre>

<p>
Пройдемся по таблице. 
Тут все просто - адрес назначения пакета ищется  в этой таблице:
если адрес назначения в пакете совпал с адресом в таблице (в случае сети - если принадлежит данной сети),
то пакет направляется через интерфейс, указанный в подходящей строке.
Двух одинаковых адресов (хостов или сетей) в этой таблице быть не может, поэтому и двух
путей (через разные интерфейсы) к одному хосту (или в одну сеть) здесь задать нельзя.
</p>

<div class="excl">
При поиске маршрута используется <i>Longest prefix match</i>
(соответствие по самому длинному префиксу). 
Т.е. если в таблице будут строки:<br>
<i>192.168.0.0/16</i><br>
<i>192.168.5.0/24</i><br>
то адрес <i>192.168.5.1</i> соответствует обоим сетям, но выбран будет маршрут
для <i>192.168.5.0/24</i> так как здесь префикс (сети) длиннее. 
</div>

<p>
Итак, первые два маршрута в таблице говорят о том, что пакеты для хоста с адресом
<i>192.168.1.1</i> надо доставлять через интерфейс <i>ppp1</i>, а для хоста <i>192.168.1.3</i> - 
через <i>ppp2</i>.
<br>Это PPTP-клиенты подключившиеся из сети <b>LAN</b>.
</p>

<p>
3-ий маршрут говорит что пакеты для хоста с адресом
192.168.255.2 надо доставлять через через интерфейс tun1.
<br>Это виртуальный интерфейс сервера OpenVPN.
</p>

<p>
4-ый маршрут говорит что пакеты для сети с адресом
<i>198.51.100.0/30</i> надо доставлять через интерфейс <i>eth1</i>.
<br>Это glue-сеть между шлюзом и шлюзом провайдера с адресом <i>198.51.100.1</i>.
</p>

<p>
5-ый маршрут говорит что пакеты для сети с адресом
<i>198.51.100.0/30</i> надо доставлять через интерфейс <i>eth2</i>.
<br>Это сеть <b>DMZ</b>, <i>203.0.113.1</i> - адрес интерфеса <i>eth2</i> в этой сети.
</p>

<p>
6-ой маршрут говорит что пакеты для сети с адресом
<i>192.168.5.0/24</i> надо доставлять через интерфейс <i>eth3</i>.
<br>Это сеть <b>LAN2</b>, <i>192.168.5.1</i> - адрес интерфеса <i>eth3</i> в этой сети.
</p>

<p>
7-ой маршрут говорит что пакеты для сети с адресом
<i>192.168.0.0/24</i> надо доставлять через интерфейс <i>eth1</i>.
<br>Это сеть <b>LAN</b>, <i>192.168.0.1</i> - адрес интерфеса <i>eth1</i> в этой сети.
</p>

<p>
8-ой маршрут говорит что пакеты для сети с адресом
192.168.255.0/24 надо доставлять через шлюз (в сеть OpenVPN) 192.168.255.2 
через интерфейс tun1.
</p>

<p>
И наконец 9-ый маршрут говорит что пакеты для которых <b>не было найдено
подходящго маршрута</b> (т.е. хоста/сети назначения)
доставлять через через интерфейс eth1 на шлюз ("via" - "через") <i>198.51.100.1</i>.
<br>Это шлюз провайдера <b>ISP1</b>, через который пакеты далее попадут в Интернет.
<br>Это и есть маршрут <i>"по-умолчанию"</i> (<i>default</i>).
</p>

<div class="excl">
<b>По этому маршруту будет направлен любой пакет, для которого ядро не найдет подходящего маршрута!</b> 
</div>

<p>
Собственно так пакеты, "предназначенные для Интернета", и "попадают в Интернет".
Это просто пакеты, для которых ядро не знает, где расположена нужная сеть/нужный узел 
(потому что не все сети в мире подключены непосредственно к вашему шлюзу), 
и поэтому ему ничего не остается как направить их туда, "где находятся все остальные сети". 
</p>

<p>
Все эти маршруты были добавлены в таблицу маршрутизации автоматически 
при инициализации интерфейсов на основании конфигурационной информации.
В Debian это файл <i>/etc/network/interfaces</i>.
</p>
<p>
Поэтому, в случае одного провайдера вручную делать особо ничего и не надо, кроме
как правильно сконфигурировать интерфейсы.
При указанной выше конфигурации таблицы маршрутизации весь "Интернет-трафик" идет <b>только через провайдера ISP1</b>.
</p>
<h5>Одно подключение - входящий трафик</h5>
<p>
Вопрос, собственно, заключается в том, как трафик, предназначенный для вашего 
подключения или сети попадает к вам. До определенного момента это кажется само 
собой разумеющимся и очевидным, однако после появления у вас второго подключения
становится ясно, что не все так просто.
</p>
<p>
В нашем примере мы принимаем входящие соединения как на сам шлюз (198.51.100.2) 
так и так и на хосты в сеть <b>DMZ</b> (203.0.113.0/29). 
Ответный трафик исходящийх от нас соединений так же попадает на наш шлюз.
Каким образом это происходит?
</p>

<p>
Если коротко - провайдер "сообщает" (анонсирует) всем другим сетям в Интернете,
что адреса шлюза и сети находятся в его автономоной системе и 
соответственно трафик из Интернета, предназначенный для этих адресов поступает 
через пограничные маршрутизаторы AS провайдера в его сеть. Далее уже внутри 
сети провайдера трафик маршрутизируется на нужный маршрутизатор и далее,
возможно, в зависимости от варианта организации "последней мили",
через доп. обрудование (DSLAM, BRAS, модемы, медиаконвертеры и т.п.) к вам. 
</p>

<br>
<h4 id="routing2">Два внешних подключения (два разных провайдера)</h4>
<p>
В этом случае возникает несколько вариантов того какой именно трафик куда и когда направить:
<ul>
<li>направить все <b>исходящие</b> соединения через одного определённого провайдера;</li>
<li>направить <b>исходящие</b> соединения через двух провайдеров: часть соединений через одного
провайдера, оставшуюся часть - через другого, согласно неким критериям;</li>
<li>приём всех <b>входящих</b> соединений только через одного провайдера</li>
<li>приём <b>входящих</b> соединений через двух провайдеров одновременно, используя разные публичные адреса</li>
</ul>
Управление маршрутизацией исходящих и входящих соединений удобно рассматривать отдельно,
так как маршрутизацией исходящих мы можем управлять самостоятельно, без взаимодействия
с провайдерами, а вот маршрутизация входящих либо от нас не зависит, либо требует другой
организации подключений к провайдеру и плюс взаимодействие с маршрутизаторами провайдеров.<br>
</p>

<br>
<h5>Исходящий трафик</h5> 
<p>
Самый простой вариант - использовать провайдеров попеременно, переключаясь
между ними по необходимости. Для этого самое простое - менять в таблице маршрутизации
маршрут по умолчанию, например с
<pre class="cmd">
default via 198.51.100.1 dev eth5
</pre>
на
<pre class="cmd">
default via 192.168.5.10 dev eth6
</pre>
где <i>192.168.5.10</i> адрес модема <b>ADSL модем №2</b> в сети <b>LAN2</b>, который
выполняет роль шлюза в сеть провайдера <b>ISP2</b> и далее в Интернет.
</p>
<p>
Второе, что можно сделать, не прибегая к дополнительным возможностям - направить
исходящий трафик к определенному хосту или сети нужным маршрутом.
Для этого достаточно добавить нужный маршрут в стандартную таблицу <i>main</i>.  
</p>

<p>
<b>А что делать если мы хотим более гибко управлять маршрутизацией исходящего трафика?
Направлять пакеты через того или иного провайдера не только на основании 
адреса назначения, но и, например, исходящего адреса, порта, протокола?</b>
</p>
<p>
Использовать то, что называется <b><i>"policy routing"</i></b> (<i>марщрутизация по политикам</i>) совместно
<b>с несколькими таблицами маршрутизации</b> (<i>множественные таблицы маршрутизации</i>).
</p>

<br>
<h5 id="routing2policy">Policy routing</h5>
<p>
<b>Policy routing</b> - механизм позволяющий управлять маршрутизацией не только на основании
адреса-назначения в пакете (как только что было показано выше), но и на основе других критериев.
</p>

<p>
Взглянем на всю картину в целом.
В действительности, та таблица маршрутизации, которая была показана выше является
всего лишь одной из существующих таблиц маршрутизации и называется она <i>main</i>.
Есть еще две "встроенные" таблицы: <i>local</i> и <i>default</i>.
И, к тому же, можно создавать дополнительные таблицы маршрутизации - до 252 таблиц.
Итак можно создать несколько таблиц с разными наборами маршрутов.
А вот для того чтобы, управлять, тем по какой из этих таблиц будет определятся
маршрут для того или иного пакета и нужна <i>routing policy database (RPDB)</i>
(<i>база политик маршрутизации</i>).
Используя только команду <i>route</i> невозможно управлять всем этим механизмом - она позволяет
манипулировать только маршрутами в одной определенной таблице - <i>main</i>.
Для полноценного управления мрашрутизацией служит команда <i>ip</i> из пакета <i>iproute2</i>  
</p>

<p>
<i>RPDB</i> имеет несколько вариантов <i>селекторов</i> (<i>selector</i>) и <i>действий</i> (<i>action</i>),
которые позволяют нам указать ядру: <i>вот c такими пакетами</i> делай указанное действие.
Например: пакеты, идущие с такого-то адреса маршрутизируй (т.е. определяй куда его послать) 
с использование <i>вот этой таблицы</i> или пакеты помеченные так-то - блокируй и т.д.
Селекторы можно комбинировать.
Ниже приведен пример RPDB:
</p>

<pre class="cmd">
#ip rule show
0:      from all lookup local
32763:  from all fwmark 0x4 lookup isp2
32766:  from all lookup main
32767:  from all lookup default
</pre>

<p>
Номер до двоеточия - приоритет записи в базе , все что до слова "lookup" - 
селектор(ы) и последнее - имя таблицы маршрутизации.
Например запись 32763 значит: для пакетов с любым ("from all") адресом источника
и у которых отметка MARK равна 0x4 (просто число, которым мы помечаем определенные пакеты в NETFILTER),
маршрут определять по таблице с именем "isp2".
Т.е. в этой записи скомбинированы два селектора: "from" и "fwmark". 
</p>

<div class="excl">
Алгоритм поиска в <i>RPDB</i> следующий:
<ul>
<li>записи (правила) в <i>RPDB</i> <b>перебираются в порядке их приоритета</b> (первое число в записи <i>RPDB</i>)</li>
<li><b>если данный пакет соответствует селектору</b> правила, то <b>выполняется действие указанное в правиле</b></li>
<li>если указанное в правиле действие <b>окончательно определяет маршрут пакета</b>, то
правила, следующие за "сработавшим" <b>перебираться для данного пакета уже не будут</b>,
<b>иначе перебор правил продолжается</b></li>
</ul>
</div>

<p>
Например: если в правиле указана некая таблица маршрутизации, то производится поиск
подходящего маршрута в этой таблице: если маршрут найден, то правила <i>RPDB</i>,
следующие за "сработавшим" перебираться для данного пакета не будут (так как маршрут определен),
а вот если в таблице маршрутизации не будет найдено ни одного подходящего маршрута, то перебор
правил в <i>RPDB</i> будет продолжен.
</p>

Например если в базу, показанную выше, добавить такое правило
<pre class="cmd">
#ip rule del from all fwmark 0x4  priority 32700 prohibit
</pre>
то мы получим вот такую таблицу
<pre class="cmd">
#ip rule show
0:      from all lookup local
32700:  from all fwmark 0x4 prohibit
32763:  from all fwmark 0x4 lookup isp2
32766:  from all lookup main
32767:  from all lookup default
</pre>

<p>
В ней для пакетов с одной и той же отметкой <i>MARK</i> сначала идет правило с запретом
(<i>prohibit</i>), поэтому все пакеты, имеющие отметку <i>MARK=0x4</i> будут запрещены.
Для таких пакетов правило с приоритетом 32763 уже никогда срабатывать не будет, так
как в правиле 32700 маршрут пакета однозначно будет определен (<i>запрет</i>) в процессе маршрутизации!
</p>

Или другой пример. Например если в базу добавить такое правило с таблице маршрутизации:
<pre class="cmd">
#ip rule add from all fwmark 0x4 priority 32700 table newtable
</pre>
то мы получим вот такую таблицу
<pre class="cmd">
#ip rule show
0:      from all lookup local
32700:  from all fwmark 0x4 lookup newtable
32763:  from all fwmark 0x4 lookup isp2
32766:  from all lookup main
32767:  from all lookup default
</pre>

<p>
В этом случае, если в таблице маршрутизации newtable для пакета найдется подходящее правило, 
то правило с приоритетом 32763 тоже не сработает. А вот если в таблице newtable не найдется
ни одного подходящего для данного пакета маршрута, то пакет далее будет проверяться
на соответствие правилу с приоритетом 32763.
</p>

<p>
Видов селекторов не так уж много (смотрите документацию), но один из них <b>fwmark</b> нам и нужен.
</p>

<br>
<h5 id="routing2firewall">fwmark: NETFILTER+routing</h5>
<p>
Селектор <i>fwmark</i> позволяет сделать одну, но очень мощную вещь: <b>отбирать в правилах <i>RPDB</i> 
пакеты на основе меток, которые можно с помощью NETFILTER ставить в пакетах</b>.
</p>

<p>
С помощью iptables мы можем "помечать" пакеты в таблице mangle. Тут важно знать что
эти метки вставляются не внутрь пакета, а ассоциируются с пакетом, до тех
пор пока пакет не ушел в сеть. Т.е. эти метки привязаны к пакету, но хранятся вне
его - в памяти ОС и по сети не передаются.
Метки существуют пока пакет "крутится" внутри операционной системы данного хоста (компьютера).
</p>
<p>
Еще важно помнить про разные типы меток, которые могут быть привязан к пакету одновременно
и которые не надо путать между собой (<i>CONNMARK, CONNSECMARK, MARK, SECMARK</i>). 
Также важно не путать <b>установку отметки</b>
(это определённые виды <i>target</i>'ов в ключе '-j') c <b>проверкой на соответствие</b> (<i>match</i>)
между отметкой в пакете и заданной в правиле.
Сейчас речь пойдет о отметках, которые проставляются у пакетов target'ом <b>MARK</b>.
</p>

<br>
<h5 id="routing2policy">Настройка policy routing</h5>
<p>
Итак есть все элементы для решения задачи маршрутизации исходящего трафика через двух провайдеров:
<ul>
  <li>возможность создать несколько таблиц маршрутизации</li>  
  <li>база политик маршрутизации, позволяющая задать правила (в том числе и через отметки в пакетах):
	  какой трафик с помощью какой таблицы маршрутизировать</li>  
  <li>возможность NETFILTER помечать нужные нам пакеты</li>  
</ul>
</p>

<div class="excl">
Настройка будет организована следующим образом.
<ul>
<li>Маршрутизация будет осуществляться с помощью основной (<i>main</i>) и дополнительных таблиц.</li>
<li>Для каждого подключения к провайдеру будет отдельная таблица, содержащая только 
маршрут по умолчанию для данного подключения.</li> 
<li>В RPDB для каждой дополнительной таблицы будет правило, маршрутизирующее пакет с пометкой
по соответствующей таблице.</li>
<li>Пакеты без пометок маршрутизируются по default-маршруту в таблице <i>main</i>.</li>  
<li><b>Выбор основного подключения, через которые пойдут все исходящие соединенения по умолчанию, 
будет осуществляться сменой default-маршрута в таблице <i>main</i></b>.</li>  
<li><b>Выборочное направление исходящих соединений через то или иное подключение (провайдера)
будет осуществляться маркировкой пакетов (fwmark) с помощью iptables</b>.</li>  
</ul>
</div>

<p>
Теперь уточним, чего именно хотим добиться:
</p>
<ul>
  <li>общий принцип: ответный трафик для входящих соединений должен уходить через 
    те же интерфейсы, что ;</li>
  <li>чтобы все исходящие соединения из <b>LAN</b> и <b>DMZ</b> в Интернет шли через провайдера <b>ISP3</b></li>
	  за исключением:
    <ul>
      <li>rdp трафик (<i>dst tcp 3389</i>) должен идти через <b>ISP1</b>;</li>
      <li>VPN трафик между мобильными клиентами и сетью <b>LAN</b> должен идти через <b>ISP1</b>.
			  Клиенты будут подключаться только через <b>ISP1</b> на адрес 198.51.100.2, поэтому и уходить трафик
				должен через <b>ISP1</b>;</li>
      <li>трафик пользователя "user" (<i>192.168.1.3</i>) шёл через <b>ISP1</b>;</li>  
      <li>трафик к хосту XXX.XXX.XXX.XXX шёл через <b>ISP1</b>;</li>  
      <li>трафик к хосту YYY.YYY.YYY.YYY шёл через <b>ISP2</b>;</li>  
    </ul>
  <li>чтобы все исходящие соединения от шлюза <b>Gateway</b> в сеть Интернет шли через провайдера <b>ISP3</b></li>
	  за исключением:
    <ul>
      <li>SMTP-сессии (<i>tcp 25</i>) должны идти через <b>ISP1</b>; это связано с тем, что
			 сейчас многие SMTP-сервера проверяют не является ли адрес динамическим,
			 проверяют по обратной зоне DNS, проверяют SPF и т.д. Поэтому, чтобы не иметь проблем
			 с уходом почты с вашего SMTP-сервера, лучше чтобы трафик шёл со статического адреса,
			 с правильно прописанной RR в обратной зоне;</li>
      <li>DNS-трафик (<i>udp, tcp 53</i>) должен идти через <b>ISP1</b> - трафик критичный
			(для того же SMTP) и если "отвалится" <b>ADLS модем №2</b>, почта может не уходить из-за
			недоступности DNS. Поэтому DNS пускаем тем же путем, что и SMTP - через более надёжный канал через <b>ISP1</b>.
    </ul>
</ul>

<p>
После старта ОС у нас уже есть автоматически сконфигурированная основная таблица маршрутизации
<i>"main"</i>. Она используется как основная для маршрутизации во все сети, подключенные к шлюзу. 
</p>
<pre class="cmd">
#ip route show table main
192.168.1.1 dev ppp1  proto kernel  scope link  src 192.168.3.1.
192.168.1.3 dev ppp2  proto kernel  scope link  src 192.168.3.1.
192.168.255.2 dev tun1  proto kernel  scope link  src 192.168.255.1.
198.51.100.0/30 dev eth1  proto kernel  scope link  src 198.51.100.2.
203.0.113.0/29 dev eth2  proto kernel  scope link  src 203.0.113.1.
192.168.5.0/24 dev eth3  proto kernel  scope link  src 192.168.5.1.
192.168.0.0/24 dev eth0  proto kernel  scope link  src 192.168.0.1.
192.168.255.0/24 via 192.168.255.2 dev tun1.
default via 198.51.100.1 dev eth1.
</pre>

<p>
Теперь подготовим <b>альтернативные таблицы маршрутизации</b>.
У этих таблиц только одна роль - определить другой <i>"маршрут по умолчанию"</i>, отличный
от того, который указан в таблице маршрутизации <i>"main"</i>. 
</p>
В файле /etc/iproute2/rt_tables описываем новые таблицы

<pre class="cmd">
#
# reserved values
#
255     local
254     main
253     default
0       unspec
#
# local
#
#1     inr.ruhep
1      isp1 # таблица для первого подключения
2      isp2 # таблица для первого подключения
3      isp3 # таблица для первого подключения
</pre>

Добавляем в каждую таблицу свой маршрут по умолчанию:
<pre class="cmd">
ip route flush table isp1
ip route flush table isp2
ip route flush table isp3
ip route add default via 198.51.100.1 dev eth5 table isp1
ip route add default via 192.168.5.10 dev eth6 table isp2
ip route add default via 192.0.2.1 dev eth7 table isp3
</pre>

Проверяем, что получилось
<pre class="cmd">
#ip route show table isp1
default via 198.51.100.1 dev eth5

#ip route show table isp2
default via 192.168.5.10 dev eth6

#ip route show table isp3
default via 192.0.2.1 dev eth7
</pre>

Добавим  в <i>RPDB</i> <b>новые политики</b> для маршрутизации по этим таблицам: 
<pre class="cmd">
#ip rule add from all fwmark 1 table inet1 pref 32010
#ip rule add from all fwmark 2 table inet2 pref 32020
#ip rule add from all fwmark 3 table inet3 pref 32030
</pre>

Проверяем, что получилось:
<pre class="cmd">
#ip rule show
0:      from all lookup local
32010:  from all fwmark 0x1 lookup isp1
32020:  from all fwmark 0x2 lookup isp2
32030:  from all fwmark 0x3 lookup isp3
32766:  from all lookup main
32767:  from all lookup default
</pre>

<p>
Осталось сделать один шаг - пометить те пакеты, которые будут маршрутизироваться по
таблицам <i>isp1</i>, <i>isp2</i>, <i>isp3</i>. 
Для этого надо разобраться с еще одним важным вопросом - где и каким образом помечать нужные пакеты.
</p>

<br>
<h5 id="routing2steps">Этапы прохождения пакета через NETFILTER</h5>
<p>
Открываем <a style="font-size:20px" href="iptables_mangle.html" target="_blank">листинг команды 'iptables -t mangle -nvL'</a>, чтобы было удобнее следить за дальнейшими пояснениями.
</p>
<p>
  Ниже приведена схема этапов (таблицы и цепочки) которые проходит пакет.
	Информация о этапах взята отсюда:
	<a target="_blank" href="http://www.frozentux.net/iptables-tutorial/iptables-tutorial.html#TRAVERSINGOFTABLES">http://www.frozentux.net/iptables-tutorial/iptables-tutorial.html#TRAVERSINGOFTABLES</a>
</p>
<p>
На схеме показаны таблицы и цепочки, которые проходят пакеты в ядре Linux.
Сплошной линиией показан путь пакетов, маршрутизируемых по default-маршруту 
таблицы <i>"main"</i>.
Показан только фрагмент таблицы <i>"main"</i>, а именно маршрут по умолчанию через <i>eth7</i>.
Пунктиром показан путь пакетов, маршрутизируемых по таблицам <i>"isp1"</i>, <i>"isp2"</i> и <i>"isp3"</i>
с использованием policy routing.
Показаны точки, в которых осуществляется пометка пакетов (<b>MARK</b>) и преобразование адресов (<b>SNAT</b>).
</p>
<p>
Начнем с рассмотрения прохождения пакетов исходящих соединений.
</p>

<center><img src="img/nf-flows2.png"/></center>
<br>
<center>Рис. 8 Исходящие соединения из шлюза и сети LAN в Internet</center>

<br>
<h5>Исходящие соединения. <br>Потоки GW->INET</h5>
</p>
<div class="excl">
Помечать пакеты с помощью target <b>MARK</b> можно только в таблице <i>mangle</i>.
Делать преобразование адресов <b>SNAT</b> можно только в таблице <i>nat</i>  в цепочке
<i>POSTROUTING</i>.								 
</div>
</p>

<p>
Исходя из сказанного, помечаем нужные нам пакеты  новых соединений <b>Gateway<->Internet</b>
в <i>mangle OUTPUT</i>. Почему, например, не в <i>mangle POSTROUTING</i>? Отметку надо проставить
еще <b>до процесса маршрутизации</b> (<i>routing decision</i>), чтобы на момент
осуществления маршрутизации нужные пакеты уже имели нужную метку.
Пометка в <i>mangle POSTROUTING</i> просто не будет иметь никакого эффекта.
</p>

<p>
<div class="excl">
<b>В нашем случае правильно <i>всегда</i> помечать не просто пакеты, а соединения</b>!.
Т.е., например, в OUTPUT надо помечать новое исходящее соединение (ставить ctmark с помощью -j CONNMARK),
а затем соответственно метке соединения маркировать пакеты (ставить nfmark с помощью -j MARK). 							 
Это необходимо, чтобы отличать пакеты исходящих со шлюза соединений, от ответных пакетов
на входящие на шлюз соединения. 
<br><br>
Пример. Пусть мы со шлюза пингуем хост Y через <i>eth5</i>. Для этого исходящие icmp-пакеты маркируются
в mangle OUTPUT правилом типа "-p icmp -j MARK --set-mark 1" 
и наш первый ping-пакет уходит через <i>eth5</i> к хосту Y.
<br>В это же время хост Y начинает пинг нашего шлюза на <i>eth7</i>.
В ответ на ping от Y наш шлюз шлет ответный пакет (pong) хосту Y <b>который должен уйти через</b> <i>eth7</i>,
но согласно метке проставлямой правилом "-p icmp -j MARK --set-mark 1" в mangle OUTPUT он уйдет через <i>eth5</i>!.
Т.е. <i>pong</i> в ответ на вошедший через <i>eth7</i> <i>ping</i> уйдет не через <i>eth7</i>:
хост Y не увидит ответов нашего шлюза!  
</div>
</p>


<div class="excl">
<b>При этом пакеты трафика <i>Gateway</i><-><i>Internet</i> будут иметь адрес источника
<i>192.0.2.2</i>, присвоенный еще на первом этапе маршрутизации, сразу после <i>Local process</i>,
аж до самого <i>SNAT to:198.51.100.2</i> и <i>SNAT to:192.168.5.1</i></b>.
Обратите внимание: в <i>filter OUTPUT</i> выходным интерфейсом для всех пакетов всё еще будет
<i>eth7</i>. Для маркированных пакетов он сменится на <i>eth5</i> и <i>eth6</i> 
только после второго <i>routing decision</i>.
</div>

<p>
С учетом этого, посмотрим на процесс прохождения пакетов от локальных процессов (т.е. от работающих на шлюзе ОС и ПО).
</p>

<p>
Первым делом осуществляется маршрутизация пакета (Routing Decision #2). Из этого следует
что у пакета <b>выходной интерфейс и адрес источника всегда определяются по маршруту по умолчанию</b>
и в нашем случае в <i>raw:OUTPUT</i> мы увидим у пакетов <i>src=192.0.2.2, out=eth7</i>.
Исключение - если userspace-процесс сам явно устанваливает nfmark (см. например ping с ключем '-m').
</p>

<p>
Далее в <i>mangle:OUTPUT</i> мы помечаем нужные нам <b>исходящие соединения</b>. 
Исходя из вышесказанного осуществляется это так: сначала помечаем <b>новые соединения</b>, которые мы 
хотим направить не по default-маршруту неким числом (-j CONNMARK --set-mark),
а вторым этапом уже маркируем <b>пакеты соединения</b> (-j MARK --set-mark) числом,
которое будет задано в соответсвующем селекторе RPDB (в нашем случае эти числа совпадают). 
Условимся пакеты, которые должны уходить через <i>eth5</i> помечать числом '1',
через <i>eth6</i> - числом '2', через <i>eth7</i> - числом '3'.
Тогда соответствующие правила могут выглядеть так:
<pre class='cmd'>
-A OUTPUT -o eth5 -m connmark --mark 1 -j MARK --set-mark 1
-A OUTPUT -o eth6 -m connmark --mark 2 -j MARK --set-mark 2
-A OUTPUT -o eth7 -m connmark --mark 3 -j MARK --set-mark 3
</pre> 
Или, вместо этого, можно одной командой просто скопировать число, 
которым было помечено соединение (ctmark) в nfmark : 
<pre class='cmd'>
-A OUTPUT -j CONNMARK --restore-mark
</pre> 
Но лучше все же сделать это только для определенных интерфейсов:
<pre class='cmd'>
-A OUTPUT -o eth5 -j CONNMARK --restore-mark
-A OUTPUT -o eth6 -j CONNMARK --restore-mark
-A OUTPUT -o eth7 -j CONNMARK --restore-mark
</pre> 
И, хотя кажется, что предварительная пометка соединения здесь является излишней, 
на примере выше уже было показано, что это необходимо для корректной маркировки 
пакетов уже установленных исходящий и входящих соединений.
</p>

<p>
Итак после <i>mangle:OUTPUT</i> у пакетов соединений, которые мы хотим направить не по default-маршруту
будут установлены нужные нужные отметки <b>ctmark</b> и <b>nfmark</b>.
</p>

<p>
Далее следует последний этап маршрутизации (Routing Decision #3) и мы анализируем
правила RPDB уже с учетом проставленных у отдельных пакетов <b>ctmark</b> и <b>nfmark</b>.
<p>

<p>
Пакеты без отметок <b>nfmark</b> не попадут ни под один селектор <b>fwmark</b>
и смаршутизируются согласно default-маршруту таблицы <i>"main"</i>.
В nat:POSTROUTING произойдет замена адреса источника (<i>SNAT to:192.0.0.2</i>) и
пакеты уйдут через <i>eth7</i> к провайдеру <b>ISP3</b>.
SNAT здесь кажется излишним и в некотором смысле это так, но как далее будет видно
за счет использования policy routing на интерфейсе <i>eth7</i> возможны исходящие
пакеты и с адресами, привязанными к интерфейсам <i>eth5</i>, <i>eth6</i>.
</p>

<p>
Пакеты соединений c отметкой <b>0x1</b> согласно правилу <i>from all fwmark 0x1 lookup isp1</i> в <i>RPDB</i>,
и таблице "isp1" смаршутизируются через <i>eth5</i>. 
По пути будет сделано преобразование адресов (<i>SNAT to:195.51.100.2</i>)
и после этого трафик через <i>eth5</i> отправится к провайдеру <b>ISP1</b>.
</p>

<p>
Пакеты соединений c отметкой c отметкой <b>0x2</b> согласно правилу <i>from all fwmark 0x2 lookup isp2</i> в <i>RPDB</i>,
и таблице "isp2" смаршутизируются через <i>eth6</i>. 
По пути будет сделано преобразование адресов (<i>SNAT to:192.168.5.1</i>)
и после этого трафик через <i>eth6</i> уйдет в сеть <b>LAN2</b> на шлюз <i>192.168.5.10</i>.  
При этом пакет будет иметь: адрес источника <i>192.168.5.1</i>, адрес назначения -
<i>адрес в Интернете</i> и MAC-адрес назначения - <i>MAC-адрес внутреннего порта</i> <b>ADSL модема №2</b>
Таким образом, пакет будет доставлен в <b>ADSL модем №2</b>, откуда он, согласно
<i>маршрута по умолчанию</i> <b>модема ADSL модем №2</b>, отправится через провайдера <b>ISP2</b>.
</p>

<p>
Посмотрим на соответствующие правила в листинге таблицы <i>mangle</i> (помечены красным).
<br>В <b>OUTPUT</b> для <i>eth1</i> есть переход в цепочку <b>04_GW_INET_MARK</b>.
Там первые три правила, исключают из маркировки исходящий SMTP и DNS трафик.
Это сделано для того чтобы трафик почтового сервера всегда уходил со статичного адреса,
для которого прописана адрес в обратной зоне (.in-addr.arpa), так как очень часто 
принимающий SMTP-сервер использует обратную зону для проверки истинности адреса 
принимающего SMTP-сервера (Forward-confirmed reverse DNS).
DNS тоже не помечается, чтобы идти через более надежного провайдера <b>ISP1</b>. 
</p>

<p>
Предпоследнее правило в цепочке помечает все новые исходящие от шлюза соеденения 
(<b>CONN</b>MARK) неким произвольным числом (0x500). Ну и последнее правило уже все
пакеты соединений, помеченных этим числом (match 0x500), помечает числом 4 (<b>MARK xset 0x4</b>).   
</p>

<p>
<b>Потоки LAN->INET</b><br>
С пакетами соединений <b>LAN<->INET</b> поступаем также как и c пакетами <b>LAN<->INET</b>,
только в этом случае существует возможность пометить трафик раньше - 
еще до первого решения о маршрутизации - в <i>mangle PREROUTING</i>. 
Т.о. уже в цепочке <i>FORWARD</i> у помеченных пакетов будет
"правильный" выходной интерфейс источника <i>eth5</i> или <i>eth6</i>, а не <i>eth7</i>.
</p>

<p>
Смотрим листинг (правила помечены синим): там тоже есть исключения.
<br>Первое исключение для подсети <i>198.51.100.0/30</i> - эта сеть подключена напрямую к <i>eth1</i>
и трафик в нее всегда должен идти через <i>eth1</i>.
<br>Второе исключение для пользвателя в сети <b>LAN</b> с адресом <i>192.168.1.3</i> - ему надо "ходить"
через ISP1.
<br>Третье исключение для сети <b>DMZ</b> - ответный трафик из <b>DMZ</b> должен всегда
уходить через <i>eth1</i>, так как входящие соединения приходят на <i>eth1</i> (публичный статический адрес).

<div class="excl">
Хотя сеть <b>DMZ</b> никак не относится к потоку LAN->INET, но данное исключение необходимо просто потому, 
что согласно правилам перехода из <i>PREROUTING</i> в цепочку <b>05_LAN_INET_MARK</b>,
сеть <i>DMZ</i> тоже относится к "Интернету" (просто подпадает под условие !192.168.0.0/16)
и для обхода этого приходится вставлять данное исключение.
</div>
Если бы можно было (без ipset) задать условие типа "!192.168.0.0/16 and !203.0.113.0/29" то
все было бы проще... 
<br>Четвертое исключение для подсети <i>192.168.255.0/24</i>, выделенной для клиентов OpenVPN - 
ответный трафик также должен всегда уходить через <i>eth1</i>, 
так как openvpn принимает входящие соединения на <i>eth1</i>.
<br>Пятое исключение для RDP (tcp 3389) трафика.
<br>Далее, как и в случае 04_GW_INET_MARK помечаем новые исходящие соединения и 
уже только помеченные соединенения метим числом для дальнейшей маршрутизации по таблице <i>isp2</i>.
</p>

<p>
Далее смотрим как идет трафик в ответ на наши исходящие соединения.
</p>

<p>
<b  id="routing_eth3">Потоки INET->GW, INET->LAN (цепочки 03_INET_GW, 06_INET_LAN в mangle)</b><br>
</p>

<center><img src="img/nf-flows3.png"/></center>
<br>
<center>Рис. 9  Прохождение пакетов в обратном направлении: из сети Internet в LAN</center>

<p>
Пожалуй самое примечательное здесь это то, что в <i>filter INPUT</i> могут появляться
пакеты вот с такими адресами назначения:  
</p>
<pre class="cmd">
host [361.745891] IN=eth3 OUT= SRC=213.180.204.190 DST=198.51.100.2 LEN=52 TOS=0x08 PREC=0xC0 TTL=55 ID=6076 DF PROTO=TCP SPT=80 DPT=44909 WINDOW=13387 RES=0x00 ACK FIN URGP=0 
host [361.917213] IN=eth3 OUT= SRC=217.14.203.229 DST=192.168.5.1 LEN=40 TOS=0x08 PREC=0xC0 TTL=55 ID=38988 DF PROTO=TCP SPT=80 DPT=54684 WINDOW=6456 RES=0x00 ACK FIN URGP=0  
</pre>

<div class="excl">
Т.е. в <i>filter INPUT eth3</i> приходят пакеты с адресом назначения <i>198.51.100.2</i>, который
вообще-то привязан к <i>eth1</i>!
Происходит следующее (рис. 8): при отправке пакетов от локальных процессов (Local Process)
сразу идет маршрутизация и согласно таблице <i>"main"</i> пакетам назначается
выходной интерфейс <i>eth1</i> и адрес источника <i>198.51.100.2</i>.
После этого, в <i>mangle OUTPUT</i>, идет пометка части пакетов и повторная маршрутизация
с учетом отметки. И вот тут помеченным пакетам переназначается выходной интерфейс с 
<i>eth1</i> на <i>eth3</i>, а адрес источника (почему-то) остается исходным -  <i>198.51.100.2</i>.  
Т.е. уже после второй маршрутизации и до <i>SNAT to: 192.168.5.1</i> пакеты с отметкой 
<i>MARK=0x4</i> имеют <i>out=eth3 и src=198.51.100.2</i>.
</div>

<p>
Ответные пакеты (рис. 9), приходящие на <i>eth3</i> подвергаются обратной трансляции
адресов и уже после <i>nat PREROUTING</i> и до самых <i>Local Process</i> пакеты будут иметь <i>in=eth3 dst=198.51.100.2</i>.  
</p>
<div class="excl">
Вот почему в цепочке INPUT есть правило с <i>in=eth3 и dst=198.51.100.2</i>:
на <i>eth3</i> сюда могут приходить пакеты как с <i>destination</i> 198.51.100.2</i> так и <i>192.168.5.1</i>!
</div>

<p>
В процессе маршрутизации <b>исходящего трафика</b> из "общего потока" "вычленяется" трафик,
который мы хотим направить через <i>eth3</i>: поэтому "дополнительный поток"
(пунктир) появляется только в процессе маршрутизации. 
</p>
<p>
<b>Входящий</b> же через <i>eth1</i> и <i>eth3</i> трафик "не сливается" после маршрутизации,
так как входные интерфейсы у пакетов остаются разными на всем пути до интерфейса <i>eth0</i>. 
</p>

<p>
С трафиком <b>Internet<->DMZ</b> (на рис. 8 и рис. 9 не показан) происходит то же, что и 
с трафиком <b>LAN<->Internet</b>, за исключением отсутствия преобразования адресов.
</p>
<div class="excl">
Так как в сети DMZ у нас публичные ("реальные, белые") Интернет-адреса, то не
надо делать <i>SNAT</i> для трафика <b>Internet<->DMZ</b>.  
</div>

<br>
<h5 id="routing2in">Входящий трафик через двух провайдеров</h5>
<p>
Рассмотрим для нашей схемы случай, в котором мы хотим принимать входящие соединения на веб-сервер
в сети <b>DMZ</b> через два подключения <b>ISP1</b> и <b>ISP2</b> <b>одновременно</b>
(не путайте этот случай со случаем использования двух каналов (основной+резервный) <b>к одному провайдеру</b>). 
Это возможно в случае, если клиенты будут подключаться на два разных IP-адреса, 
принадлежащих разным провайдерам. Пакеты от клиентов, подключающихся на адрес <i>203.0.113.2</i>,
будут проходить через провайдера <b>ISP1</b> на <i>eth1</i> и далее через наш шлюз в сеть <b>DMZ</b>.
Пакеты от клиентов, подключающихся на внешний адрес модема <b>ADLS №2</b>,будут проходить через провайдера <b>ISP2</b>.
</p>
<p>
Так как на модеме <b>ADLS №2</b> включен NAT и публичный адрес присваивается интерфейсу самого модема,
то клиенты, подключающиеся на этот адрес, будут соединяться с модемом, а не с веб-сервером в сети <b>DMZ</b>. 
Для того чтобы "пробросить" входящие соединение через модем до веб-сервера 
в сети <b>DMZ</b>, надо <b>на модеме</b> включить и настроить "проброс портов" (port forwarding).
С этой функцией входящие соединение на определённый tcp или udp порт <b>модема</b>, будут 
перенаправляться далее на IP-адрес, в указанный настройке port forwarding'а.
Дополнительно в таблице маршрутизации модема надо прописать путь до сети DMZ через <i>192.168.5.1</i>,
иначе "проброшенный" трафик модем будет направлять обратно в Интернет, так как у него
этот трафик будет маршрутизироваться согласно маршруту по умолчанию, т.е. обратно через <b>ISP2</b>.  
</p>
<p>
Однако одного этого недостаточно для чтобы сервер в сети DMZ мог работать с клиентами,
подключившимися через разных провайдеров.   
Дело в том, что сервер, получив пакет от клиента, будет посылать ответные пакеты
только через того провайдера, чей шлюз прописан в маршруте по умолчанию.
В зависимости от используемых протоколов и особенностей написания клиента, 
это может вызвать проблемы при обработке ответных пакетов у клиентов,
к которым ответный трафик будет посылаться через другой интерфейс, а не через тот
на который клиент отправлял пакеты серверу. 
</p>
<p>
Это обусловлено тем, что клиент будет посылать пакеты серверу на один IP-адрес 
(в данном случае на внешний адрес ADLS-модема №2), а получать ответные пакеты с совершенно 
другого IP-адреса (в данном случае с адреса <i>eth1</i>). Большинство же реальных программ
не рассчитаны на это, так как написаны с использованием такой абстракции 
транспортного уровня как <b>internet socket</b>.
</p>
<p>
В этой абстракции рассматривается передача трафика между двумя 
конечными точками соединения - сокетами, каждый из которых характеризуется 
комбинацией IP-адреса, порта и протокола (напр. TCP или UDP), 
где порт фактически идентифицирует приложение, создавшее сокет.
Абстракция сокетов реализована в большинстве операционных систем и является
стандартом де факто для написания приложений работающих с сетью Интернет.
Т.о. именно ОС "занимается" учётом и идентификацией сокетов и соединений.
Для сокетов с поддержкой соединения (TCP/SCTP), соединение однозначно 
идентифицируется парой сокетов: серверным и клиентским.
Для сокетов без соединений (UDP), соединение однозначно 
идентифицируется локальным сокетом.
</p>
<p>
Из этого следует, что при использовании протоколов с поддержкой соединений,
операционная система будет всегда ожидать прихода ответных пакетов с того сокета
(т.е. IP-адреса:порта), с которым соединение было установлено изначально.
Т.о. ответные пакеты, проходящие к клиенту с другого IP-адреса, 
ОС клиента не сможет отнести ни к одному из уже существующих сокетов и установленных 
соединений и просто их проигнорирует. Это справедливо для любых протоколов, 
работающих поверх TCP.
</p>
<p>
Для голого IP-протокола или для UDP такого ограничения нет - в принципе ничто 
не запрещает написать такой клиент и сервер, которые смогут получать и обрабатывать
пакеты, приходящие с адресов, отличных от того, куда был отправлен или откуда 
был получен ответный первый пакет.
</p>

<p>
Т.к. протокол HTTP реализован поверх TCP, HTTP-клиент не сможет работать с веб-сервером,
который принимает входящие пакеты на один IP-адрес, а ответные пакеты отправляет 
этому же клиенту с друого IP. 
</p>
<div class="excl">
Т.о., для рассматриваемого случая, надо сделать так, чтобы пакеты уходили 
тем же маршрутом (т.е. через тот же интерфейс шлюза), которым они попали в шлюз (и далее в <b>DMZ</b>).  
</div>
<p>
Пока я вижу только один универсальный способ добиться этого -  использовать модуль
conntrack. Надо пометить все входящие в <b>DMZ</b> соединения с помощью CONNMARK:
пакетам, входящим в разные интерфейсы, надо присваивать разные отметки.    
</p>
<p>
Т.о. и ответные пакеты от серверов <b>DMZ</b> также будут иметь отметки, соответствующие
тем отметкам, что были проставлены для входящих пакетов.
Благодаря этому, становится возможным по этим отметкам с помощью policy routing 
маршрутизировать ответный трафик входящего соединения через тот интерфейс, через 
это соединение было установлено.  
</p>

<p>
<b>Так же можно почитать</b>:<br>
<a href="http://www.potaroo.net/ispcol/2015-06/mptcp.html">Multipath TCP</a>
<br>
<a href="http://tools.ietf.org/html/rfc3704#page-5">http://tools.ietf.org/html/rfc3704#page-5</a>
</p>

<br>
<h2 id="router_hardware">Немного о железе для шлюза</h3>
<p>
В рассматриваемом в тексте варианте (рис. 3) до 2014 года использовался 
обычный десктопный компьютер с CPU Celeron 2.4 GHz, RAM 1Gb и четырьмя 100Mbit 
сетевыми PCI-картами который работал с 2006 года. Так как описывается использование
маршрутизатора на основе сервера с ОС GNU/Linux, то в первую очередь рассмотрим именно этот вариант. 
</p>
<h5>Сервер на платформе x86</h5>
<p>
Более приемлемым (чем старый десктоп) вариантом является полноценный сервер, особенно в случае организации своей AS.
Акцент необходимо сделать на производительном процессоре, достаточном объеме памяти
(для таблиц BGP, squid) и кол-ве сетевых портов. Дисковая система на маршрутизаторе вторична и нужна простая - 
достаточно SATA-контроллера без RAID и небольшое кол-во SATA-дисков. 
Если захотите аппаратный RAID - берите модель с BBU (батареей) или суперконденсатором, чтобы
не мучаться с рассинхронизацией массива при отключении питания.
</p>
<p>
Требования к памяти для полной таблицы BGP ~500000 записей (на 2014г.) на одного провайдера.
Например, Cisco рекомендует для своих маршрутизаторов от 512MB памяти на одну полную таблицу: 
"To store a complete global BGP routing table from one BGP peer, it is best to 
have a minimum of 512 MB or 1 GB of RAM in the router. If 256 MB of RAM is used, 
it is recommended that you use more route filters."
(<a href="http://www.cisco.com/c/en/us/support/docs/ip/border-gateway-protocol-bgp/12512-41.html#memissues">
http://www.cisco.com/c/en/us/support/docs/ip/border-gateway-protocol-bgp/12512-41.html#memissues</a>) 
</p>
<p>
Пример конфигурации на базе x86-сервера, собранная на замену старому шлюзу и 
эксплуатируемая с середины 2014г. (66 тыс.руб./ок.$2000 в ценах 1кв. 2014):
<ul>
<li>1 x Серверная платформа (мат. плата+корпус) SuperMicro SYS-5018D-MTLN4F</li> 
<li>1 x Процессор Intel Xeon X4 E3-1270v3 3.5Ghz Socket-1150</li> 
<li>2 x Память SuperMicro 8Gb DDR3 1600MHz ECC (обычная небуферизированная DDR DIMM, но с ECC)</li>
<li>4 x Жесткий диск Hitachi SATA-II 500Gb HUA722050CLA330</li> 
<li>1 x Сетевой адаптер Intel Ethernet Server Adapter Gigabit ET2 Quad Port E1G44ET2BLK</li>
</ul>
В данной конфигурации: на материнской плате 4 гигабитных Ethernet-порта и
на сетевом адаптере еще 4 гигабитных Ethernet-порта + порт управления (IPMI).
Итого 8 рабочих портов - достаточно.  
</p>
<p>
Примеры других вариантов серверов можно посмотреть здесь:
http://plonetest.univ-pau.fr/doc/folder.2006-12-12.6464510577/folder.2006-12-16.8029221739/Vyatta_Cisco_Replacement_Guide.pdf
http://mirrors.winkstreaming.com/vyatta/documents/hardware_guides/Vyatta_Cisco_Replacement_Guide.pdf
</p>

<p>
Меньше 4GB RAM нет смысла и брать: память относительно дешева и никогда не помешает (контейнеры, виртуализация, BGP, squid).
Модулей памяти лучше брать кратно количеству каналов в CPU.
Контроллер памяти в этой модели процессора (E3-1270v3)
двухканальный - два модуля будут быстрее работать, чем один модуль любой емкости. 
</p>
<p>
Диски можно распределить так:
<ul>
<li>1-ый и 2-ой в программный RAID-1 массив (mdraid) + LVM собственно под установку ОС;</li> 
<li>3-ий: горячая замена (spare disk) в программный RAID-массив (на случай отказа 1-го или 2-го);</li> 
<li>4-ый вспомогательный: бэкапы, что-то сохранить, переписать и т.п.</li>
</ul>
Дисков можно взять три или два и меньшей емкости (160Гб хватит за глаза).
</p>

<p>
У маршрутизатором на базе сервера можеть быть следующее узкое место:
<ul>
<li>из-за большого кол-ва аппаратных прерываний нескольких сетевых карт проседает производительность маршрутизации;</li> 
<li>относительно небольшое кол-во физических портов на сервере.</li> 
</ul>
На каком-то форуме я увидел следующий рецепт для обхода этих двух проблем - 
использование управляемого L2-коммутатора с поддержкой VLAN в дополнение к 
маршрутизатору на базе сервера.
Нужное кол-во имеющихся портов маршрутизатора подключаем к портам коммутатора, 
сконфигурированным в транк-режиме: через них будет передоваться
тэгированный трафик (IEEE 802.1Q). 
<br>За счет использования VLAN на маршрутизаторе и коммутаторе : 
<ul>
<li>можно уменьшить до минимума кол-во сетевых карт (портов) на сервере и получить много портов для подключения на коммутаторе;</li>
<li>получить много отдельных L2-сегментов.</li>
</ul>
При этом коммутация (L2 switching) внутри одной VLAN будет происходить в коммутаторе,
не загружая сервер, а на сервере будет осуществляться маршрутизация между VLAN
(L3 routing).  
Остальные порты коммутатора можно включить в ту или иную VLAN и использвать для подключения к ним оборудования.
</p>
<p>
Вместо одного коммутатора можно использовать несколько отдельных, без поддержки VLAN, но
в таком варианте есть недостатки:
<ul>
<li>больше свитчей которые занимают место;</li>
<li>кол-во L2-сегментов будет равно кол-ву портов на сервере, тогда как 
в варианте с VLAN сегментов можно получить гораздо больше кол-ва физических портов на сервере.</li> 
</ul>
</p>
<h5>Cisco</h5>
<p>
Следующий вариант - использование "аппаратного" маршрутизатора от Cisco.
Для рассматриваемого случая будет достаточно Cisco серии 2900, например Cisco ISR 2951.
Однако в этом варианте есть следующие особенности: 
<ul>
<li>цена базовой комплектации с 512MB RAM и вариантом IOS "IP Base" около $4000;</li> 
<li>скорее всего необходмио будет докупать еще 512MB RAM;</li> 
<li>не так уж много Ethernet-портов - 3 штуки. Можно докупить модуль с двумя портами
HWIC-2FE 2-port 10/100 Routed-Port;</li> 
<li>недостаточно ясная схема лицензирования возможностей OS - со временем варианты 
меняются (обычно с вводом нового модельного ряда).
Похоже что на 1 кв. 2014 для пограничного маршрутизатора достаточно идущего в комплекте 
варианта "IP Base", однако, например, фаервола там нет - это отдельный пакет за отдельные деньги;</li>
<li>скорее всего необходимо докупать лицензию на пакет <b>SECK9</b> для 
использования возможностей фаервола либо отдельное устройство серии ASA5000;</li> 
<li><a href="http://tools.cisco.com/ITDIT/CFN/jsp/by-feature-technology.jsp">Cisco Feature Navigator</a> помогает.
Выбираем "Search by Software", последние релизы и "IP Base" (или нужное сочетание пакетов) и получаем список чего там поддерживается.
Firewall - stateful failover, HSRP, Firewall feature set, RADIUS, IPv6 Firewall
</li>
</ul>
Судя по <a href="http://www.cisco.com/c/en/us/products/collateral/cloud-systems-management/software-activation-on-integrated-services-routers-isr/white_paper_c11_556985.html">
http://www.cisco.com/c/en/us/products/collateral/cloud-systems-management/software-activation-on-integrated-services-routers-isr/white_paper_c11_556985.html</a>
сейчас есть:
<ul>
<li>базовая версия <b>IPBaseK9</b> (фаервола здесь нет);</li> 
<li>пакет безопасности <b>SECK9</b>: лицензируется кол-во VPN-сессий; подписки для 
системы обнаружения вторжений (IPS) и фильтрации контента;</li> 
<li>пакет унифицированных коммуникаций <b>UC</b>;</li> 
<li>пакет <b>DATA</b> (в частности MPLS и STUN).</li> 
</ul>
</p>

<h5>Mikrotik</h5>
<p>
Собственно про производителя и особенности продукции <a href="http://ru.wikipedia.org/wiki/MikroTik">http://ru.wikipedia.org/wiki/MikroTik</a>.
</p>

<p>
Для пограничного маршрутизатора можно рассмотреть что-то с достаточным объемом памяти под full BGP.
Из старой серии RB1100AHx2 (2GB RAM).
Из нового - что-то из серии CCR1016 (CPU Tilera c 16 ядер) или CCR1036 (CPU Tilera c 36 ядер). 
Однако RouterOS на данной серии до сих пор работает нестабильно.
Народ плачется про перезагрузки, зависания, неравномерную загрузку ядер: http://forum.mikrotik.com/viewtopic.php?f=1&t=67195&start=1250 
Если и когда эти проблемы будут решены, то получится вполне доступный маршрутизатор с ценой до $1000.
</p>

<p>
У нас в эксплуатации SOHO роутеры RB750 и RB951Ui-2HnD - год без нареканий, приятные штучки. 
</p>

<p>
http://mum.mikrotik.com/presentations/RU14/megis.pdf
</p>

<h5>Другие варианты</h5>
<p>
Перечисленными выше вариантами выбор, естественно, не ограничивается, однако про 
что-то другое сказать что-то определенное затруднительно, кроме названий. Juniper.
</p>


<br>
<br>
<h2 id="nat_share">Доступ из внутренних сетей к сети Интернет через один публичный адрес (NAT - трансляция адресов)</h2>
<p>
При прохождении трафика через маршрутизатор из одной сети в другую могут иметь место два случая:
<ul>
  <li>при прохождении пакетов через маршрутизатор никаких преобразований адресов не 
    осуществляется</li>  
  <li>при прохождении пакетов через маршрутизатор происходит преобразование (трансляция) 
  адресов (NAT) согласно какому-то принципу</li>  
</ul>
</p>
<p>
Например (см. рис. 3) пакеты из сети <b>LAN</b> в сеть <b>DMZ</b> проходят шлюз без преобразования
адресов: пакет от хоста <i>192.168.0.1</i> доходит до хоста <i>203.0.113.2</i> с неизменённым
адресом отправителя. Т.е. узел <i>203.0.113.2</i> "видит", что пакет пришел от хоста адреса
<i>192.168.0.1</i>, который, естественно, находится в другой IP-сети. Поэтому, когда
узел <i>203.0.113.2</i> отошлет (ответный) пакет, то в качестве адреса получателя
будет указан адрес <i>192.168.0.1</i> - пакет будет смаршрутизирован на нужный интерфейс
(<i>eth0</i>) и попадет на узел <i>192.168.0.1</i>.
</p>

<p>
В случае, когда по какой-либо причине такая "естественная" маршрутизация невозможна 
(нежелательна), применяется трансляция адресов. Например, это невозможно в случае,
когда нужен доступ из сети с приватными адресами в сеть Интернет - так устроена (организована) адресация.
<br>
Пакеты с приватными адресами глобально (в рамках всей сети Интернет) не маршрутизируются.
Приватные адреса специально выделены для этих целей (локальных сетей).
Таких сетей великое множество и какая из них имеется ввиду в очевидно невозможно 
определить в принципе.
Поэтому, даже если в сторону провайдера от вашего шлюза пойдут пакеты с приватными
адресами источника, то дойдут они не далее чем до первого маршрутизатора - там они и "умрут".
И по этой же причине пакет, посланый неким сервером клиенту с приватным
адресом, просто не дойдет до клиента. 
</p>

<div class="excl">
Т.о. для того чтобы хосты из сети с приватными адресами могли обмениваться трафиком
с хостами сети Интернет нужны: хотя бы один публичный ("белый","реальный") адрес 
"от имени" которого вся приватная сеть сможет обмениваться трафиком с Интернетом и
механизм трасляции адресов. Также подразумевается что на шлюзе включен <i>ip forwarding</i>.
</div>

<p>
В нашем случае, для трафика между сетями <b>LAN</b> и <b>Internet</b> необходимо
применить NAT, а точнее Source NAT: адрес-источник во всех пакетах 
(например <i>src 192.168.0.1</i>), перед "выходом" 
через <i>eth1</i> должнен быть заменен на адрес <i>198.51.100.2</i> 
(публичный адрес выданный нам провайдером <b>ISP1</b>). При этом все пакеты, отправляемые
из сети <b>LAN</b> "будут выглядеть для Интернета" как отправленные одним хостом с
публичным адресом <i>198.51.100.2</i>.
</p>

<p>
Когда некий сервер, в ответ на приход такого пакета, ответит своим пакетом, то в 
адресе назначения будет указан <i>198.51.100.2</i> (а не, например, <i>192.168.0.1</i>).
Как же он, в конце концов, дойдет до хоста <i>192.168.0.1</i>, ведь в ответ на пакеты
с разными адресами (<i>192.168.0.1</i>,<i>192.168.0.138</i> и т.д.) сервер будет отвечать
на один и тот же адрес <i>198.51.100.2</i>? 
</p>
<p>
Для этого NETFILTER, помимо смены адреса источника, также меняет некоторые
параметры отправляемых пакетов и ведет таблицы соответствия параметров исходных и модифицированных
пакетов. Поэтому любой приходящий на <i>198.51.100.2</i> пакет NETFILTER может проверить
на соответствие в этой таблице и "выяснить" относится ли пришедший пакет к соединению,
установленному с хоста сети <b>LAN</b> (здесь также задействован модуль conntrack).
Для TCP/UDP пакетов NETFILTER кроме адреса источника дополнительно заменяет порт источника
на определённое значение, что и позволяет транслировать номер порта назначения в ответном пакете в адрес сети <b>LAN</b>. 
В случае протоколов, не имеющих портов, NETFILTER применяет (какие-то) другие 
(отличные от замены номеров портов) методики сопоставления адресов.
</p>

<p>
Также, есть ещё один путь прохождения трафика между сетями <b>LAN</b> и <b>Internet</b> - 
 через <i>eth3</i>. Здесь преобразование адресов не является обязательным, так как
 оно все равно обязательно произойдет позже - в <b>ADSL модеме №2</b>.
 Хотя наличие NAT для этого трафика имеет свои преимущества: например не надо будет
 в таблицах маршрутизации хостов сети <b>LAN2</b> (того же <b>ADSL модема №2</b> или 
 например принтеров) прописывать маршруты в сети <i>192.168.0.1</i> и <i>192.168.1.1</i>
 что облегчает адинистрирование и позволяет пользоваться устройствами сети <b>LAN2</b>, не поддерживающими
 настройку маршрутов, из сети <b>LAN</b>. 
</p>

<p>
Теперь посмотрим как это сделано в нашей схеме.
<br>
Открываем <a style="font-size:20px" href="iptables_nat.html" target="_blank">листинг команды 'iptables -t nat -nvL'</a>.
Как видно из <a href="#routing2steps">рис.6</a> <i>SNAT</i> удобнее всего сделать в <i>nat POSTROUTING</i>.
В листинге это первое правило в цепочке <i>POSTROUTING</i> и оно означает: у всех пакетов,
выходящие через <i>eth1</i>, кроме идущих из сети <i>203.0.113.0/29</i> (т.е. из <b>DMZ</b>)
заменить адрес источника на адрес <i>198.51.100.2</i>, что и требуется.
</p>

<div class="excl">
<b>Обратное преобразование</b> для приходящих пакетов происходит 
<b>автоматически</b>, т.е. никаких правил для обратного преобразования в NEТFILTER задавать не надо! 
</div>

<p>
Второе правило в цепочке <i>POSTROUTING</i> задает трансляцию адресов для трафика,
идущего через <i>eth3</i>: здесь мы заменяем адрес источника на адрес интерфейса
<i>eth3</i> только у тех пакетов, которые должны были идти на <i>eth1</i>, а мы 
"завернули" их при помощи <i>policy routing</i> на <i>eth3</i>.
Как минимум часть таких пакетов (например, от локальных процессов) будут иметь
адрес <i>198.51.100.2</i>, а не нужный нам <i>192.168.5.1</i>. 
Хотя, повторюсь, можно сделать преобразование и у абсолютно всех исходящих пакетов
или только у пакетов у которых адрес источника не равен <i>192.168.5.1</i> - для
решения задачи "расшаривания Интернета" эти варианты эквивалентны.
</p>

<p>
Однако, при необходимости надо учитывать влияние преобразования на связь между другими 
сетями: выборочная или поголовная трансляция пакетов перед выходом через <i>eth3</i>, 
будет влиять на доступность хостов
сетей <b>LAN</b> и <b>LAN2</b> между собой. Например, если будут транслироваться
все пакеты, то связь из сети <b>LAN</b> с устройствами сети <b>LAN2</b> будет работать
независимо от наличия маршрутов в сети <i>192.168.0.1</i> и <i>192.168.1.1</i> в 
таблицах маршрутизации устройств сети <b>LAN2</b>. При этом, в обратном направлении
(из <b>LAN2</b> в <b>LAN</b>), без специальных ухищрений (напр. port forwarding ),
невозможно будет "достучаться". Если же NAT будет выборочный (напр. как в нашем случае),
то с устройствами в <b>LAN2</b>, у которых не заданы маршруты в сети <i>192.168.0.1</i> и 
<i>192.168.1.1</i> из сети <b>LAN</b> работать будет невозможно.  
</p>

<p>
Вообщем с помощью этих двух правил мы и добъемся нужного эффекта: хосты из сети <b>LAN</b>
смогут "лазить в Интернет через один белый адрес". 
</p>

<br><br>
<h2 id="tc">Управление трафиком (traffic control: shaping, policing и т.д.)</h2>
<p>
Раcсмотрим основные концепции управления трафиком (traffic control) в Linux.
</p>

<p>
Traffic control в Linux включает в себя три главных компонента: 
<ul>
<li><b>qdisc</b> - дисциплины очереди, т.е. доступные в ядре типы планировщков для очередей сетевых пакетов;</li> 
<li><b>class</b> - классы позволяют задавать "внутри" дисциплины сложные, иерархические настройки по ограничению пропускной способности;</li> 
<li><b>filter</b> - позволяют классифицировать пакет, т.е. сопоставить пакет и класс, согласно правилам которого он будет обрабатываться в очереди;</li> 
</ul>

</p>

<h3 id="tc_egress">Управление исходящим трафиком: shaping, scheduling</h3>

<p>
Начнём с исходящего трафика. В случае шлюза, исходящий трафик может быть сгенерирован
как локальными процессами так и в результате прохождения трафика (forwarding) через
шлюз, от одного сетевого интерфейса к другому. С каждым сетевым интерфейсом связана
<i>очередь</i> (<i>queue</i>), в которую пакеты ставятся перед отправкой: пакет, который должен быть отправлен, 
отправляется в сеть не сразу, а ставится в очередь ("привязанную" к исходящему интерфейсу) 
и затем извлекается из очереди планировщиком для отправки в сеть согласно некому принципу (алгоритму). 
</p>

<p>
<div class="excl">
Алгоритм, согласно которому пакеты извлекаются из очереди и отправляются в сеть и есть <b>queuing discipline (qdisc)</b>.
При этом (внутренне) используются два механизма: 
<ul>
<li><i>shaping</i> - задержка отправки (определённых) пакетов из очереди в сеть для 
ограничения скорости передачи или сглаживания неравномерности передачи </li>
и 
<li><i>scheduling</i> - реорганизация порядка (приоретизиация) пакетов в очереди, т.е. управление
порядком обработки (отправки в сеть) пакетов из очереди.</li>
</ul>
</div>
В опеределённом смысле, любая <i>queuing discipline</i> является scheduler'ом (планировщиком).
</p>

<p>
В traffic control для обозначения исходящяго трафика или исходящей <b>qdisc</b> используется термин <b>egress</b> (например <i>egress qdisc</i>).
В команде же <i>tc</i> для задания исходящей дисциплины используется ключевое слово <b>root</b>, 
а не <b>egress</b>; например:   
<pre class="cmd">
tc <b>qdisc</b> add dev eth0 <b>root</b> handle 6: htb r2q 5   
</pre>
</p>

<p>
Например, пусть в нашей схеме, два хоста сети <b>LAN</b> получают пакеты из сети <b>Internet</b> (что-то качают).
Для определенности пусть пакеты проходят через <i>eth1</i> далее они форвардятся
на <i>eth0</i> - естественно сначала они попадают в очередь этого интерфейса.
В этой очереди единовременно могут находиться пакеты предназначенные для обоих качающих хостов. 
<div class="excl">
Так вот управление тем, в какой последовательности и с какой частотой (через какие промежутки времени) 
эти пакеты будут отправлены в сеть тому или иному хосту и дает нам возможность управлять
трафиком.
</div>
</p>

<p>
Естественно, чаще всего, интересуют формулировки вида:
<ul>
<li>"<i>чтобы Вася мог качать со скоростью не меньше XXX Кбайт/с, но не больше YYY Кбайт/с</i>" и одновременно с этим</li>
<li>"<i>чтобы Петя мог качать со скоростью не меньше ZZZ Кбайт/с, и если ему надо, автоматически,
он мог повышать скорость скачивания до максимума, отбирая пропускную способность у других таких 'вась'</i> "</li> 
</ul>
а не какие-то там "очереди" и прочие сложности.
</p>
<p>
Эти вопросы (и все другие вопросы связанные с управлением трафиком) решаются описанными
принципами управления очередей и <b>планированием</b>: желаемые пропускные способности
для каждого из хостов, их изменения в реальном времени в зависимости от неких приоритетов,
а также пропускные способности задействованных каналов - все это взаимосвязано и 
<b>для построения системы работающей ожидаемым и предсказуемым образом, надо в первую очередь,
знать какого именно поведения вы хотите добиться от системы в целом</b>.
</p>
<p>
Например, на рассматриваемой схеме (пока отбросим провайдера <b>ISP2</b>) пропускная способность 
подключения к <b>ISP1</b> (пусть 1 Мбит/с=125 кбайт/с) распределяется между всеми хостами
сетей <b>LAN</b> и <b>DMZ</b>, а также самим шлюзом. Допустим пользователей в сети 
<b>LAN</b> хотя бы 10 - на каждого придется по 12,5 кбайт/с чего явно мало даже для
комфортного серфинга по Инету. А ведь есть еще сервера в сети <b>DMZ</b> и сам шлюз, которым
тоже надо что-то выделить из 1 Мбит/с. Если все это распределить и установить
вычисленные жёсткие лимиты, то каждому хосту достанется мизерные пару кбайт/с, которые
ни один узел не сможет привысить, даже если остальные хосты не качают (и как следствие - канал свободен).
Поэтому, возникает желание установить некие более высокие лимиты, но в то же время они
не должны давать возможность какому-либо хосту единолично загрузить канал и не давать качать 
остальным. И вот тут, прежде чем что-то делать, надо уже придумать схему (политику), согласно
которой различные хосты (группы хостов) смогут делить пропускную способность общего канала, а так же,  
как и в каком приоритете одни хосты будут "отбирать" пропускную способность у других хостов.
Например, пользователи с выскоим приоритетом хотят чтобы, когда они качают "нечто важное", им отдавалась
большая часть пропускной способности канала - естественно за счет "ущемления" других потребителей трафика.
И даже между ними (пользователями с высоким приоритетом) желательно как-то явно поделить
канал (поровну или опять же - по приоритету).
Вот эту задачу (придумать политику использования канала) и надо решить перед тем как что-то делать.   
И это мы еще не учли влияние входящяго трафика!
</p>
<p>
После такого вводного отступления вернёмся к traffic control.
</p>
<p>
Вот для того, что бы получить нужное поведение трафика, например схожее с вышеописанным, и 
используют различные дисциплины очередей (qdisc) и их комбинации.
</p>
<p>
<b>Дисциплины</b> (<b>qdisc</b>) могут быть двух видов:
</p>
<ul>
<li><b>безклассовые</b> (<i>clasless</i>)</li> и  
<li><b>с поддержкой классов</b> (<i>classful</i>, для краткости - "классовые")</li> 
</ul>

<p>
<b><i>Classful qdisc</i></b> позволяют определить классы. К каждому из классов можно привязать:
<ul>
<li>Другие <i><b>классы</b></i>, тем самым построить иерархию (дерево) классов. В каждом классе 
задается собственная дисциплина, совпадающая с дисциплиной родительского класса или отличная от неё. Т.о. иерархия
классов фактически задаёт иерархию очередей с потенциально различными свойствами.
</li>
<li><i><b>Фильтры</b></i>, которые будут классифицировать (фактически помечать) тот или иной вид трафика в тот или иной класс</li>
<li><b><i>Безклассовую дисциплину очереди</i></b> (<i>clasless qdisc</i>) - чтобы управлять трафиком в данном классе 
определенным образом</li>  
<li><b><i>Классовую дисциплину очереди</i></b> (<i>classful qdisc</i>) - начав, таким образом, новую иерархию,
базирующуюся на дисциплине, отличной от "корневой"!</li>
</ul>
</p>

<p>
Ниже приведен абстрактный пример со всеми четырьмя случаями:
</p>
<pre class="cmd">
(1) tc qdisc del dev eth0 root
(2) tc qdisc add dev eth0 root handle 6: htb r2q 5
(3)     tc class add dev eth0  parent 6: classid 6:1 htb rate 48Kbit ceil 48Kbit
(4)         tc qdisc add dev eth0 parent 6:1 handle 7: prio
(5)             tc qdisc add dev eth0 parent 7:1 handle 10: sfq
(6)             tc qdisc add dev eth0 parent 7:2 handle 20: tbf rate 20kbit buffer 1600 limit 3000
(7)             tc qdisc add dev eth0 parent 7:3 handle 30: pfifo
(8)     tc class add dev eth0 parent  6: classid 6:2 htb rate 48Kbit ceil 48Kbit
(9)         tc class add dev eth0 parent 6:2 classid 6:3 htb rate 24Kbit ceil 48Kbit
(10)        tc class add dev eth0 parent 6:2 classid 6:4 htb rate 24Kbit ceil 48Kbit
</pre>

<p>
В данном примере к <i>egress</i> (<i>root</i>) интерфейса присоединена <i>classful</i> <b>HTB</b> дисциплина (команда 2).
К дисциплине присоединены два класса: 6:1 и 6:2 с гарантированной полосой 48 кбит (команды 3 и 8). 
Обмена токенами (свободной полосой пропускания) между этими классами не будет, 
так как у них нет общего родительского класса.
Далее, к классу 6:1 присоединена дисциплина <i>classful</i> <b>PRIO</b> в которой (автоматически созданы) три класса:
7:1, 7:2, 7:3, к каждому из которых, в свою очередь подключены clasless дисциплины 
<i>sfq, tbf, pfifo</i>.
К классу 6:2 подключены два дочерних класса 6:3, 6:4. Каждому из них гарантированы
24 кбита из 48 кбит родительского класса 6:2. И каждый из 6:3 и 6:4 может позаимствовать 
у другого до 48 кбит, если в каком-то из них есть свободные токены (свободная пропускная способность).    
</p>

<p>
Описанные команды задают только иерархию классов, 
но никак не определяют каким пакетам какой класс назначить 
(что влияет на время и порядок отправки пакетов из очереди в физ. интерфейс).
Чтобы указать какому трафику какой класс назначить надо воспользоваться фильтрами (<i>tc <b>filter</b> ...</i>)   
<b>Фильтры</b> (<b>filter</b>) позволяют <b>классифицировать</b> трафик: каким 
пакетам какой из классов классовой дисциплины назначить. 
</p>

<div class="excl">
<p>
Фильтры могут существовать <b>только в классовых (<i>classful</i>)</b> дисциплинах 
<b>и всегда привязываются к определенному классу</b> (без опции <b><i>parent</i></b> фильтр
добавляется в корневой класс дисциплины, а если дисциплина безклассовая то фильтр 
просто не добавится - будет выдана ошибка...).
Это значит что <b>фильтры не являются глобальными сущностями</b> - фильтр, привязанный к данному классу,
используется только для классификации того трафика, которому назначен этот класс ("который попал в этот класс").
</p>
</div>

<div class="excl">
Таким образом, <i>дисциплины</i> позволяют задать "схему" управления трафиком на данном интерфейсе,
а <i>фильтры</i> дают возможность "привязать" определённый вид трафика к определённым "точкам" в этой "схеме",
задавая логику управления конкретным видом трафика в рамках описанной для интерфейса "схемы".  
</div>
<!-- <big><big><big><big>ВСТАВИТЬ СХЕМУ взаимоотношений qdisc-class-filter</big></big></big></big> -->
</p>

<h3 id="tc_ingress">Входящий трафик и почему им невозможно управлять. Policing</h3>
<p>
Всё вышесказанное относилось <b>только к исходящему трафику</b>. С входящим трафиком 
несколько иная ситуация. Строго говоря, управлять тем, как откуда-то нам посылают пакеты 
(и как следствие тем, что приходит на "наш" интерфейс) принципиально невозможно. Рассмотрим почему.
</p>

<p>
Рассмотрим один единственный интерфейс, который обменивается трафиком с неким другим, удаленным интерфейсом.
<div class="excl">
<b>Конечной</b> целью управления трафиком является управление состоянием (загрузка, задержки) 
среды передачи. Поэтому наиболее наглядным будет показатель того, можем ли мы управлять тем, <b>какие пакеты будут направлены в среду передачи</b>
(т.е. <i>передачей каких пакетов занята среда передачи в каждый конкретный момент</i>).
В случае исходящего трафика мы можем контролировать уйдет ли тот или иной пакет в среду 
передачи и когда, а в случае входящего трафика пакет <b>уже находится в среде передачи</b> и 
<b>уже</b> "стучится" в наш интерфейс. Он уже был "испущен" неким другим устройством,
и мы не можем влиять явно на процесс передачи пакетов в среду передачи этим устройством.
<br>
<span id="tc_ingress_what">
<b>Мы можем только игнорировать уже пришедшие пакеты, либо обрабатывать их дальше - 
мы не можем кого-то <i>заставить</i> слать "нам" пакеты с определенной скоростью</b>.
</span>
</div>
</p>
<p>
В случае некоторого типа трафика возможно <b>косвенное</b> управление входящим трафиком, но 
это ничего не гарантирует. Например протокол TCP имеет механизм "управление заторами" 
(congestion control), который позволяют не допускать превышения пропускной способности
всей цепи (сети, маршрутизаторы) между клиентом и сервером. Для этого, в частности, 
используется анализ прихода <i>подтверждающих пакетов</i> от противоположной стороны (пакеты с установленным флагом <i>ACK</i>).
И, например, клиент, манипулируя отправкой этих пакетов (или игнорируя часть входящих), может 
косвенно вынудить сервер уменьшить скорость передачи - в надежде, что сервер подстроится под эти условия. 
Казалось бы - вот оно! Но это работает, только если сервер будет "подчиняться правилам 
хорошего тона" и только в случае TCP (или каких-то других протоколов с поддержкой congestion control).  
</p>
<p>
Например в "голом" IP ничего такого нет, и в UDP тоже.
Т.е. на исходящем интерфейсе мы можем <b>принудить</b> трафик к определённому поведению,
а вот другую, удалённую сторону соединения мы не можем в принципе <b>принудить</b> - только в 
некторых случаях "вежливо попросить". Т.о. мы можем явно влиять на загрузку канала в 
направлении "от нас" и не можем в направлении "к нам".   
</p>
<p>
Кто-то может возразить, что если та система, которая шлёт пакеты "в нашу строну" 
доступна нам для настройки, то мы можем управлять трафиком  на ней. Но ведь для той системы
это и будет исходящий трафик. Речь же идет о влиянии "принимающей" системы на "отправляющую".
</p>

<p>
<div class="excl">
Поэтому для входящего (<b>ingress</b>) трафика (в Linux) применяется другой подход - <b>policing</b>.
На практике, разница заключается в том, что для входящего трафика нельзя использовать дисциплины
(команда <i>tc</i> просто не даст прикрепить дисциплину к <i>ingress</i>). 
Для управления (<a href="#tc_ingress_what">в смысле вышесказанного</a>) входящим трафиком 
надо применять, так называемый <b>policer</b>. Его использование возможно <b>только</b>
в составе фильтра (<i>tc filter...</i>).  
</div>
</p>
<p>
Обратите внимание, что собственно <i>policing</i> может применяться <b>как для входящего 
так и для исходящего трафика</b>. Просто входящим можно управлять только с помощью <i>policing'а</i>,
а исходящим - всеми доступными средствами: и с помощью <i>qdisc</i> и с помощью <i>policer</i>.   
И это может приводить к некоторым двухсмысленным ситуациям (см. Вопросы без ответов).
</p>

<p>
На примере последовательности команд рассмотрим в общих чертах, возможности <i>policer</i> и представим, 
что можно сделать с входящим трафиком. 
</p>

<pre class="cmd">
(1) tc qdisc add dev eth1 handle ffff:0 ingress
(2) tc filter add dev eth1 parent ffff:0 protocol ip prio 1 u32 match ip dport 110 ffff \
      police rate 256kbit burst 10k continue flowid 44:1
(3) tc filter add dev eth1 parent ffff:0 protocol ip prio 2 u32 match ip dport 110 ffff \
      police rate 128kbit burst 5k drop flowid 44:2
</pre>

<p>
В первой (1) команде мы добавляем дисциплину для входящего трафика (ingress) для интерфейса eth1.
Указан <i>handle ffff:0</i> так как именно <i>ffff:0</i> зарезервирован для ingress.
<div class="excl">
Тип дисциплины для входящего трафика указать нельзя, так как фактически есть только одна, 
специальная дисциплина, которую допустимо подключить к <i>ingress</i>, которая
(возможно) называется <i>ingress</i>. По крайней мере в тексте дисциплина для входящего
трафика будет называться <i>ingress</i> (qdisc).
Дисциплина <i>ingress</i> является <b>классовой</b>, несмотря на то, что в ней нельзя создавать
классы. Зато к ней можно прикреплять фильтры, а фильтры прикрепляются только к classful
дисциплинам.
</div>
</p>
<p>
Во второй (2) команде к ingress дисциплине прикрепляется фильтр <b>с приоритетом равным 1</b> и содержащий policer.
В третьей (3) команде также подключается фильтр <b>с приоритетом равным 2</b> и содержащий policer.
Приоритет будет определять порядок перебора фильтров, о чём ниже.
</p>

<p>
Команда (2) добавляет в ingress <i>eth1</i> фильтр, ограничивающий входящий на tcp порт 110 трафик
величиной 256 kbit. Трафик, укладывающийся в 256 kbit, будет классифицироваться как 44:1.
Так как в policer'е 2-ой команды указано continue, то трафик <b>превышающий</b> 256 kbit 
(те пакеты, при прохождении которых уже будет превышен лимит в 256 kbit) 
будет подпадать под действие следующего по порядку фильтра, заданным в команде (3).
Т.о. 2-ая команда фактически ничего не ограничивает, а только вычленяет из трафика
"полосу" в 256 кбит и назначает ей определенный класс.   
Третья команда добавляет фильтр, который "подхватывает" трафик, превысивший лимит,
заданный в преыдущем фильтре и вычленяет из него полосу в 128 кбит и назначает ему класс 44:2,
а трафик превысивший 128 кбит он просто игнорирует (т.к. указан drop).  
</p>
<p>
Т.о. оба фильтра, суммарно, ограничивают входящий трафик величиной 256+128=384 кбита.
Если же, например первый же фильтр имел бы drop в policer'е, то ограничение накладывал бы
только он (величиной в 256 кбит), и второй бы фильтр вообще не имел бы смысла: трафик,
превышающий 256 кбит, был бы "сдропан" первым фильтром и до второго бы никогда не дошел.
</p>
<p>
Осталось разобраться что даёт возможность присваивать <b>классы в фильтрах для ingress</b>.
</p>

<p>
В документации tc и во всех примерах упоминается что в команде '<i>tc policer...</i>'
параметр <i>flowid</i> является обязательным. Если при использовании policer 
в <b>egress</b> виден смысл и назначение параметра <i>flowid</i>, каковы его 
функции при использовании policer в ingress (ведь это обязательный параметр) - 
пока загадка! Ведь в ingress не может быть классов!
<a href="http://www.opalsoft.net/qos/DS-211.htm">Например здесь</a> в командах 
и описании к ним есть параметр flowid, указывающий какой класс назначить трафику,
но вот что за классы подразумеваются!? 
</p>

<h2 id="tc_netfilter">Управление трафиком в рассматриваемом примере</h2>
<p>
Рассмотрим управление трафиком в нашем примере.
Имеются два подключения (не важно, к разным или к одному и тому же провайдеру).
Каждое подключение это <b>полнодуплексных (full-duplex)</b> канал до провайдера
и его пропускная способность в нисходящем (downstream) направлении (от провайдера)
не зависит от загрузки канала в восходящем (upstream) направлении (к провайдеру). 
</p>

<p>
Опишем, что мы хотим получить от управления трафиком. 
Сконфигурированная система должна позволять: 
<ol>
<li>Разделять пропускную способность каждого канала в отдельности между сетями 
<b>DMZ</b>, <b>LAN</b> в определенных пропорциях.</li>
<li>Задавать приоритеты для различного типа трафика (SSH, RDP, DNS, HTTP, p2p и т.д.).</li>
<li>Равномерно разделять пропускную способность между хостами сетей <b>DMZ</b>, <b>LAN</b>,
не позволяя какому-либо из хостов этих сетей занимать канал.</li>
<li>Использовать свободную пропускную способность хостами сетей <b>DMZ</b>, <b>LAN</b>  
в случае, если одна из этих сетей не использует выделенную ей полосу полностью.</li>
<li>Задавать максимальную скорость скачивания для отдельных хостов.</li>
</ol>
</p>

<p>
Из рис. 3 видно, что для того чтобы полностью контролировать канал (например от ISP1)
в нисходящем направлении, надо <b>одновременно</b> управлять трафиком следующих потоков: 
<ul>
<li><b>INET</b>><i>eth1</i>-<i>eth0</i>><b>LAN</b> и <b>INET</b>><i>eth1</i>-<i>pppX</i>><b>LAN</b>,</li> 
<li><b>INET</b>><i>eth1</i>-<i>eth2</i>><b>DMZ</b>,</li>
<li><b>INET</b>><i>eth1</i>-<b>GW</b></li> 
</ul>
так как все три потока одновременно "конкурируют" за пропускную способность downstream-канала. Без некоего 
"объединения" дисциплин интерфейсов <i>eth0, eth1, eth2, pppX</i> этого сделать не получится.
Поэтому далее, на примере ppp-интерфейсов рассмотрим варианты управления 
трафиком нескольких интерфейсов в рамках одной дисциплины.
</p>

<div class="excl">
Т.о. <b>управление трафиком в нисходящем канале</b> (от провайдеров  к нам) 
будет осуществляться только косвенно - <b>управлением отдачей трафика 
на исходящих интерфейсах</b> <i>eth0</i> и <i>eth2</i>, т.к. это единственный
реалистичный вариант.
</div>

<h3 id="tc_ifb">Объединённое управление трафиком на нескольких интерфейсах (IFB, IMQ - одна дисциплина на несколько интерфейсов)</h3>
<p>
Так как одну (и ту же) дисциплину нельзя привязать к нескольким 
интерфейсам, то невозможно согласованно управлять трафиком на нескольких интерфейсах.
Ниже показаны как раз такие ситуации для рассматриваемой нами схемы. На первой схеме 
показана ситуация, когда весь трафик во внутренние сети (<b>LAN</b> и <b>DMZ</b>) поступает через провайдера <i>ISP1</i>.
Второй рисунок иллюстрирует ситуацию, когда трафик в <b>LAN</b> поступает через <i>ISP2</i>, а
трафик в <b>DMZ</b> - через <i>ISP1</i>.
</p>

<br>
<p>
<center><img src="img/ifb_isp1_down.png"/></center>
<br>
<center>Рис. 10 Весь входящий трафик идет через ISP1.</center>
</p>

<br>
<p>
<center><img src="img/ifb_isp2_down.png"/></center>
<br>
<center>Рис. 11 Входящий трафик идет через двух провайдеров.</center>
</p>

<br>
<p>
В первом случае видно, что загрузка канала к провайдеру <b>ISP1</b> зависит от суммарного
потребления трафика клиентами в сети <b>LAN</b>, pptp-клиентами в сети <b>LAN</b> и 
серверами сети <b>DMZ</b>. Таким образом, если, например, pptp-клиент подключенный,
через (виртуальный) интерфейс ppp1 начнет качать, со скоростью равной пропускной способности
канала до <i>ISP1</i>, то другим хостам "ничего не достанется".
Можно, например, задать <b>каждому</b> ppp-интерфейсу свою индивидуальную дисциплину
с нужными ограничениями, однако это не решит проблему эффективного распределения
свободной пропускной способности канала. Сказанное относится и к интерфейсам 
eth0, eth1, eth2 через которые внутренние сети получают трафик.
</p>

<div class="excl">
Вообще-то эту задачу можно решить установкой еще одного шлюза перед описываемым.
Через этот шлюз надо пропустить весь трафик и на его исходящем интерфейсе создать одну дисциплину и 
уже без всяких ухищрений управлять всем проходящим трафиком. Ранее для этого требовался отдельный
компьютер, но сейчас можно обойтись контейнером типа OpenVZ, LXC или виртуальной машиной.
Фактически это эквивалентно "рассечению" описываемого шлюза на две независимые части, 
содединеные между собой сетью.
При этом интерфейсы внутренних сетей (eth0, eth2, pppX) остаются на одной машине,
а интерфейсы внешних сетей (eth1 и eth3) "перемещаются" на другую машину. 
Правда для этого требуется настройка и адимнистрирование еще одной физической машины или
контейнера.
Мы же рассматриваем решение в рамках одного экземпляра ОС.
</div>

<p>
В рамках одного экземпляра ОС задача решается <b>использованием некоего специального виртуального устройства</b> 
(интерфейса), у которого есть очередь пакетов и к которому можно прикрепить дисциплину.
Также нужен механизм, перенаправляющий (внутри ядра) трафик с желаемых интерфейсов 
на этот виртуальный интерфейс. Таким образом достигается некая "агрегация" желаемого трафика
на одном интерфейсе, где им бы можно было управлять в рамках одной дисциплины очереди.
<b>Добиться подобного мрашрутизацией</b> (видимо?) <b>нельзя</b>, 
так как перенаправление дожно происходить уже после (для исходящего трафика) или до (для входящего)
этапов маршрутизации.
</p>

<p>
На данный момент для ядра Linux cуществуют два таких уcтройства: 
<ul>
		<li><b>IMQ (Intermediate Queueing Device</b>)</li> и 
		<li><b>IFB (Intermediate Functional Block)</b>.</li>
</ul>
Для меня важно, то что IMQ не включено (и очевидно никогда не будет) в ванильное ядро,
а IFB входит в ядро и соответственно не надо компилировать ядро (или отдельные модули)
при каждом обновлении и держать для этого нф шлюзе всё необходимое для компиляции. 
<b>Поэтому в рассматриваемой схеме применяется IFB, а IMQ не рассматривается вообще 
(независимо от их преимуществ и недостатков)</b>.
</p>

<p>
IFB-интерфейс - узкоспециализированное устройство и оно не является полноценным сетевым 
интерфейсом, как например виртуальные ppp или tun. Интерфейсы IFB не могут 
иметь IP-адрес, они не "видны" как устройства в iptables, но на них можно 
перехватывать трафик с помощью tcpdump. Исходящим интерфейсом всегда будет тот,
с которого трафик перенаправлен на IFB, а не сам IFB-интерфейс. 
</p>

<p>
Итак, на схемах показан IFB-интерфейс <i>ifb0</i>, условно "объединяющий"
трафик с разных интерфейсов, что и даёт возможность управлять им как единым целым в рамках
одной дисциплины, прикрепленной к <i>ifb0</i>.
</p>

<p>
После этого отступления переходим непосредственно к рассмотрению конфигурирования
управления трафиком в нисходящем канале.
</p>


<h3 id="tc_downstream">Управление трафиком в downstream-канале</h3>
<p>
С учетом вышеизложенного конфигурирование будет состоять из следующих шагов: 
<ul>
<li>обьединение трафика на виртуальном IFB-интерфейсе для создания единой дисциплины управления трафиком
потоков, загружающих downstream-канал;</li>
<li>настройка дисциплины IFB-интерфейса (в т.ч. создание иерархии классов)</li>
<li>классификация трафика при помощи NETFILTER (iptables target CLASSIFY)</li>
</ul>
</p>

<h4>Обьединение трафика на виртуальном IFB-интерфейсе</h4>
<p>
Рассмотрим команды, необходимые для конфигурирования IFB-интерфейса и перенаправления
на него трафика.
</p>
<p>
Сначала надо загрузить модуль ядра и если необходимо более двух IFB-устройств, указать
в параметрах модуля необходимое их кол-во, например четыре.
<pre class="cmd">
modprobe ifb numifbs=4
</pre>
</p>

<p>
Затем активируем интерфейсы, как и обычные:
<pre class="cmd">
ifconfig ifb0 up
ifconfig ifb1 up
ifconfig ifb2 up
ifconfig ifb3 up
</pre>
</p>

<p>
А далее надо перенаправить трафик с нужных нам интерфейсов на один из IFB-интерфейсов.
В нашем примере для управления нисходящим трафиком будем использовать <i>ifb0</i>.
</p>

<p>
Перенаправляем трафик c <i>eth0</i> (<b>LAN</b>) на <i>ifb0</i>.
<pre class="cmd">
(1) tc qdisc del dev eth0 root
(2) tc qdisc del dev eth0 ingress
(3) tc qdisc add dev eth0 root handle 1: prio
(4) tc filter add dev eth0 parent 1: protocol ip u32 match u32 0 0 action mirred egress redirect dev ifb0
</pre>
Первые две команды удаляют прикрепленные (если они были) дисциплины у egress (1) и 
ingress (2). Далее, к egress прикрепляем какую-нибудь, <b>обязательно классовую</b> 
дисциплину (3), например самую "простую" - <b>PRIO</b>.
</p>

<div class="excl">
К egress надо прикреплять именно <i>classful</i> дисциплину, <b>так как только к классовой 
дисциплине можно прикрепить</b> любой, в данном случае "перенаправляющий", <b>фильтр</b>!
</div>

<p>
И командой (4) производится собственно перенаправление - используются так называемые
<i>actions</i>.
</p>

<p>
Перенаправляем трафик c <i>eth2</i> (<b>DMZ</b>) и на <i>ifb0</i>.
<pre class="cmd">
(1) tc qdisc del dev eth2 root
(2) tc qdisc del dev eth2 ingress
(3) tc qdisc add dev eth2 root handle 1: prio
(4) tc filter add dev eth2 parent 1: protocol ip u32 match u32 0 0 action mirred egress redirect dev ifb0
</pre>
</p>

<p>
Для ethernet-интерфейсов эти команды можно выполнить один раз, в момент инициализации интерфейса.
</p>

<p>
В случае ppp-интерфейсов ситуация осложняется тем, что они создаются и уничтожаются 
динамически. Поэтому схожую последовательность команд, надо выполнять при создании
каждого ppp-интерфейса, в скрипте, выполняющемся при активации ppp-интерфейса. 
В Debian это будет некий скрипт, расположенный в специальном каталоге <i>/etc/ppp/ip-up.d/</i>.
Пусть в нашем случае этот скрипт называется <i>tc-ppp</i>.
</p>

</p>
<pre class="cmd">
<i>/etc/ppp/ip-up.d/tc-ppp:</i>

tc qdisc del dev $PPP_IFACE root
tc qdisc del dev $PPP_IFACE ingress
tc qdisc add dev $PPP_IFACE root handle 1: prio
tc filter add dev $PPP_IFACE parent 1: protocol ip u32 match u32 0 0 action mirred egress redirect dev ifb0
</pre>
где в переменной PPP_IFACE будет имя активированного интерфейса. 
Значение переменной PPP_IFACE будет передано демоном pppd через скрипт <i>/etc/ppp/ip-up</i>. 
</p>

<p>
Таким образом, при подключении каждого pptp-клиента, автоматически будет выполняться 
этот скрипт и исходящий трафик с ppp-интерфейса этого клиента будет перенаправлен на <i>ifb0</i>. 
</p>
<p>
Теперь весь трафик (<b>кроме входящего на eth1, который пока опускаем для простоты!</b>) 
"собран" на ifb0 и всё должно работать как и ранее, до перенаправления.
Далее рассмотрим схему классов для HTB-дисциплины, которую мы будем использовать для
управления трафиком на <i>ifb0</i>.
</p>

<h4>Настройка дисциплины IFB-интерфейса и создание иерархии классов</h4>
<p>
Сначала рассмотрим схему классов для управления трафиком только в одном из каналов - 
к <strong>ISP2</strong>.
</p>

<p>
<center><img src="img/tc_ifb0_one_isp.png"/></center>
<br>
<center>Рис. 12 Иерархия классов для управления загрузкой канала к ISP2.</center>
</p>

<p>
К ifb0 прикрепляем HTB дисциплину и создаем "корневой" класс, чтобы все подчиненные
классы могли занимать (borrow) друг у друга свободную пропускную способность (п.4 списка).
У данного класса rate и ceil равны пропускной способности канала в нисходящем направлении - <b>10 Мбит/с (10240 Кбит/с)</b>.
</p>
<p>
К "корневому" классу прикрепляем два подчиненных класса, обозначенные на схеме как <b>LAN</b> и <b>DMZ</b>.
Эти два класса служат для задания пропорции, в которой сети <b>LAN</b> и <b>DMZ</b> будут делить  
пропускную способность канала  (п.1 списка). В моменты, когда одна из этих двух сетей не использует
канал, другая сеть может загрузить канал полностью. В моменты же, когда хосты обеих сетей
одновременно стремятся максимально загрузить канала, каждой сети будет выделяться
только часть пропускной способности, согласно заданной пропорции. Т.о. и в моменты
максимальной загрузки ни одна из сетей не сможет полностью захватить канал, ничего
не оставляя другой сети. 
</p>

<p>
Для обеспечения приоритетов  (п.2 списка) <b>различных типов трафика</b>, у классов <b>LAN</b> и <b>DMZ</b> создаем
по три подчиненных класса (на схеме - <b>LANp2</b>, <b>LANp3</b>, <b>LANp4</b>, <b>DMZp2</b>, <b>DMZp3</b>, <b>DMZp4</b>) 
с различным значениями
<b>приоритета</b> и <b>rate/ceil</b>.
Приоритезация трафика полезна вот для чего.
например, если в сети LAN хосты начнут загружать канал второстепенным трафиком,
не столь важным для рабочих целей, то прохождение более важного трафика замедлится,
начнут расти задержки и падать его скорость.
Классический пример трафика, который часто "мешает" в офисной сети - p2p-трафик 
(<b>LANp2</b>), а к самым приоритетным типам трафика 
можно отнести SSH, RDP, DNS, ACK-пакеты, VoIP (<b>LANp2</b>). 
HTTP и FTP трафик можно в этом случае отнести к классу <b>LANp3</b>. 
Кроме приоритета, каждому из трех классов назначается свои значения rate/ceil,
аналогично тому, как это сделано для классов <b>LAN</b> и <b>DMZ</b>. Это позволит задать
некий гарантированный для каждого типа трафика минимум (rate) и максимум (ceil) 
скорости.
</p>
<p>
Т.о. благодаря приоритезации, даже если в сети будут хосты, стремящиеся занять 
весь канал p2p-трафиком, более важный трафик (SSH, RDP  и т.д.) будет пропускаться
первым, а менее важный трафик в этот момент не привысит заданную минимальную скорость 
и не будет мешать более важному, так как часть второстепенного трафика, 
привысившая заданный порог уже будет вынуждена ожидать, пока канал не будет освобожден более приоритетным трафиком.
</p>

<div class="excl">
HTB-класс позволяет задать <b>только 8 значений приоритетов</b>: от 0 до 7. 
</div>

<p>
Ну и, наконец, так как в каждый из классов у нас будет классифицироваться трафик
от нескольких хостов, а не от одного будет полезно как-то их "уровнять в правах"
между собой, в рамках каждого из типов трафика  (п.3 списка). Для этого к каждому классов с приоритетом
подключеам дисциплину SFQ, которая более-менее равномерно и справедливо "раздает"
в каждом из классов пропусную способность хостам. 
</p>

<p>
Т.о. данной иерархией классов мы добились решения всех задач, поставленных в начале, кроме последней - 
"задавать максимальную скорость скачивания для части хостов сетей LAN и DMZ". Переходим к решению данной задачи.
</p>

<p>
Для задания индивидуальных скоростных ограничений необходимо для каждого ограничиваемого хоста 
добавить группу классов с минимально возможным значением rate, и ceil, равным величине
требуемого ограничения. На схеме ниже показана схема классов, реализующая ограничение
скорости скачивания для двух хостов из сети <b>LAN</b>: <b>host1</b> и <b>host2</b>. 
</p>

<p>
<center><img src="img/tc_ifb0_one_isp_hosts.png"/></center>
<br>
<center>Рис. 13 Иерархия классов для управления загрузкой канала к ISP2 с индивидуальными параметрами для двух хостов.</center>
</p>

<p>
Собственно классы <b>host1</b> и <b>host2</b> необходимы для возможности
задать индивидуальное ограничение для хостов (<b>ceil</b> 200Kbit и <b>ceil</b> 500Kbit сооответственно). 
Так как в HTB-классе параметр <b>rate</b> является 
необходимым (т.е. <b>rate</b> не может быть равен 0), то каждому хосту, которому мы 
захотим задать индивидуальные настройки, придется дать некий гарантированный минимум,
который бессмысленно делать меньше MTU (1500 байт для стандартного Ethernet). 
В нашей схеме это 4500bps (три класса по 1500bps).
Заметьте, что пока узел не был выделен в отдельный класс,
ему (и другим) по сути никакого минимума не гарантировалось. Выделяя же узел в 
отдельный класс приходится дать гарантированый минимум, отбирая тем самым
часть <b>rate</b> у подчиненных классов класса <b>LAN</b>:
у <b>LANp2</b> отобрали 32Kbit (4500 байт/с * 8) и у <b>LANp3</b> тоже 32Kbit.
Т.е. на каждый узел, которому мы захотим дать индивидуальные настройки, придется
отдать <strong>как минимум</strong> гарантированные 32Kbit (3*8*1500 байт/с).   
</p>

<p>
Посмотрим зачем нужны подчиненные классы приоритетов для классов <b>host1</b> и <b>host2</b>.
Если бы подчиненных классов не было и приоритеты были бы указаны прямо у 
классов <b>host1</b> и <b>host2</b>, то тем самым была бы нарушена
логика работы данной иерархии классов. Т.е. весь трафик для такого хоста 
подпадал бы под один заданный класс и приоретизровался не так, как трафик других хостов.
Например, если бы классу <b>host1</b> дали <i>prio 2</i>, то его трафик стал бы 
приоритетнее трафика всех остальных хостов, а если бы <i>prio 4</i> - то наборот.
Это как раз то что треубется, если наша задача - дать преимущество какому-нибудь хосту.
Мы же хотим всего лишь задать верхнюю границу скорости скачивания для выбранного хоста
и чтобы при этом правила приоритезации оставались едиными и неизменными для всех хостов сети LAN.
</p>

<p>
Поэтому необходимо к классу <b>host1</b> (и к каждому классу, играющему роль ограничителя)
прикрепить иерархию классов с приоритетами, аналогичную той, что используется и 
для всех остальных, но естественно со своими собственными значениями <b>rate</b> и <b>ceil</b>. 
Так как к классам <b>host1p2/3/4</b> будет относится трафик только одного хоста, а не нескольких
как у классов <b>LANp2</b>, <b>LANp3</b>, <b>LANp4</b>, <strong>то нет необходимости присоединять к ним дсициплину SFQ</strong>.
Естественно, всё вышесказанное справедливо и для сети <strong>DMZ</strong>, 
для хостов которой, по тому же принципу, можно задвать индивидуальные настройки.
</p>

<p>
Приведенная схема классов позволяет реализовать все указанные выше требования,
а также в общем задавать индивидульные настройки для отдельных хостов.
Например, устанавливая определенную величину rate можно выделить гарантированную полосу пропускания,
а устанавливая меньшие/большие приоритеты, можно давать преимущество на скачивание 
одним хостам по отношению ко всем остальным. Например, если необхоидмо дать преимущество
хосту <strong>host1</strong>, по отношению ко всем остальным хостам, то достаточно установить у класса
<strong>host1</strong> <i>prio 2</i>, а подчиненные классы не создавать вовсе, 
так как они в этом случае становятся излишними, и трафик к хосту <strong>host1</strong>
будет обслуживаться наравне с трафиком класса <strong>LANp2</strong>.
Если, например необходимо дать классу <strong>host1</strong> абсолютное преимущество,
то можно установить величину его <strong>prio</strong> меньше, чем у любого из классов, например <i>prio 1</i>.
</p>

<p>
Недостаток у данной схемы следующий: с ростом кол-ва хостов, которым необходимо задать 
индивидуальные настройки быстро растёт и общее количество классов и иерархия классов становится очень громоздкой.
Например, в показаном на схеме варианте с делением трафика на три типа (с тремя приоритетами),
каждый узел с индивидуальными настройками требует создания четырех классов (класс для хоста + три подчиненных).
Если мы захотим более детально делить трафик на типы, например на пять типов, 
то это уже даст по шесть классов на каждый узел с индивидуальными настройками.  
Мне неизвестно какова реальная масштабируемость подсистемы traffic control по кол-ву классов
и как большое кол-во классов сказывается на производительности. 
Кроме того каждому такому хосту необходимо выделить некий минимум пропускной способности,
кратный MTU помноженному на количество подчиненных классов для типов трафика, что тоже 
ограничивает разумное кол-во таких хостов. Из конечной ширины канала можно получить
конечное число полос с гарантированной полосой. В нашем примере для сети LAN (8192 Кбит/с)
можно "нарезать" 256 полос по 32 Кбит, а ведь еще надо что-то оставить для остальных.  
</p>

<div class="excl">
На данный момент не видно способа построить эквивалентную по возможностям, 
но менее громоздкую схему классов или какую-то другую комбинацию из дисциплин очереди,
доступных в ядре Linux. Однако можно рассмотреть следующие возможности.
</div>

<p>
В случае, если в рассмотренном выше примере, необходима лишь возможность ограничивать
скорость скачивания, а возможности приоритезации и гарантированной
полосы пропускания для отдельных хостов не нужны, то можно сконфигурировать <strong>две независимые друг от друга 
HTB-дисциплины на двух разных интерфейсах</strong>, как схематично показано ниже.
</p>


<p>
<center><img src="img/tc_ifb0_2qdisc.png"/></center>
<br>
<center>Рис. 14 Вариант управления с использование промежуточного интерфейса.</center>
</p>

<p>
С помощью HTB-дисцплины на промежуточном интерфейсе происходит ограничение максимальной
скорости скачивания для выбранных хостов, а на следующем интерфейсе, куда трафик попадает
с промежуточного, уже независимо происходит приоритезация и разделение пропускной способности
по сетям и типам трафика. Т.е. получаем некую двухступенчатую схему управления
трафиком в нисходящем канале. <strong>Но для этого нужны два исходящих интерфейса через которые
проходил бы трафик.</strong>
</p>

<p>
К сожалению, выстроить цепочку из IFB-интерфейсов не получится. Т.е. перенаправить
трафик с ethX или pppX на IFB можно, а вот далее с этого IFB на другой IFB-интерфейс - нельзя.
<br><b>Из файла doc\actions\mirred-usage в исходниках iproute2: 
"Do not redirect from one IFB device to another... 
Redirecting from ifbX->ifbY will actually not crash your machine but your 
packets will all be dropped"</b>
Проверено - всё именно так.   
</p>

<p>
Поэтому нужнен "нормальный" промежуточный интерфейс, а не IFB.
И решить эту задачу в одном экземпляре ОС с помощью, например TUN тоже не получится,
ибо нам необходимо, чтобы в конце концов трафик "уходил" со своего "родного" интерфейса:
в случае (eth0,eth2) -mirred redirect-> ifb0 это работает, так как на самом деле IFB просто
помещает пакет в то же место сетевого стека, откуда он был "подобран".
Очевидно в цепочке (eth0,eth2) -mirred redirect-> tunX -mirred redirect-> ifb0 
это работать не будет.
<br>
Возможность использования TUN как некоего абстрактного интерфеса, на который можно
перенаправить (mirred action) траффик с eth0, eth2 и eth1, потом с этого tun 
перенаправить трафик на ifb0 не изучен на практике - проверить. (Проверил - не работает). 
</p>

<p>
Т.о. образом приходим к выводу, что для реализации этого подхода надо использовать
либо две физические/виртуальные машины, либо контейнеры. И всё это чтобы сократить 
общее кол-во классов в дисциплинах очереди. Т.е. пока первоначальный вариант остается
самым универсальным. 
</p>

<h4 id="tc_ifb0_2isp">Иерархия классов для двух каналов/провайдеров</h4>
<p>
Описанная выше схема классов подходит для случая, когда весь трафик поступает
через одно подключение - именно загрузкой этого канала мы и управляем.
При наличии двух каналов с разными публичными IP (к одному или разным провайдерам)
расмотренной выше схемы управления трафиком становится недостаточно в силу того, 
что она задает правила только для одного из каналов, в то время как второй остается без управления.
В случае одновременного использования двух подключений, трафик даже к одному и тому же
хосту в сети, например <strong>LAN</strong>, может одновременно поступать через два канала за 
счет применения policy routing. И даже если два подключения не используются одновременно,
то при переключении на другой канал (предположим именно он не сконфигурирован),
возникнет ситуация неуправляемой загрузки этого канала, особенно если его
пропускная способность меньше основного. Люди же привыкают к "толстому" каналу,
загружают его "по полной" и при переключении менее "толстый" канал "ложится".  
Поэтому, чтобы управлять загрузкой двух каналов в нисходящем направлении 
необходимо на интерфейсе <i>ifb0</i> сконфигурировать две иерархии - каждая
со своими характеристиками, подходящими для пропускной способности "своего" канала.
Естественно это повлияет и на классификацию трафика. В случае использования двух каналов
уже будет нельзя использовать один набор правил классификации для всего трафика из Интернета,
как это делалось в случае одного канала. Трафик из сети Интернет в сеть <strong>LAN</strong>
(а равно и <strong>DMZ</strong>) необходимо будет классифицировать по разному, 
в зависимости от того через какой канал (интерфейс <i>eth1</i> или <i>eth3</i>) он пришёл. 
Для такой классификации в таблице mangle для трафика <strong>Internet</strong> -> <strong>LAN/DMZ</strong>
будет использоватья две цепочки, а не одна.
</p>

<p>
Посмотрим на схему классов для двух каналов с пропускной способностью в downstream 10 Мбит/с и 512 Кбит/с.
</p>

<p>
<center><img src="img/tc_ifb0_two_isp.png"/></center>
<br>
<center>Рис. 15 Иерархия классов для управления трафиком в двух каналах - ISP1 и ISP2.</center>
</p>

<p>
К описанной ранее иерархии классов (правая часть диаграммы) в корень HTB-дисциплины 
добавлен второй класс <strong>ISP1</strong> с rate и ceil равными пропускной
способности другого канала. Классы <strong>ISP1</strong> и <strong>ISP2</strong> 
не имеют общего родительского класса и прикреплены напрямую к HTB-дисциплине, поэтому не
могут обмениваться между собой свободной пропускной способностью. Именно это и требуется,
так как у нас два независимых канала, пропусная способность которых
просто физически не может быть "передаваться" от одного другому. 
</p>

<p>
К классу <strong>ISP1</strong> прикреплена иерархия классов аналогичная иерархии 
классов в <strong>ISP2</strong>, только с величинами rate/ceil уменьшенными пропорционально
меньшей пропускной способности второго канала. 
Т.о. иерархия класса <strong>ISP1</strong> будет регламентировать загрузку канала
до провайдера <strong>ISP1</strong>, а класса <strong>ISP2</strong>  - до провайдера <strong>ISP2</strong>.
При этом в иерархии классов <strong>ISP1</strong>  индивидуальные настройки 
тоже реализуются созданием отдельных подклассов для отдельных хостов, аналогично тому,
как это было показано для класса <strong>ISP2</strong>.   
</p>



<h4>Классификация трафика при помощи NETFILTER</h4>
<p>
Для классификации трафика можно воспользоваться возможностями <i>tc filter classifier</i>, 
а можно использовать возможностями NETFILTER.
Для классификакции трафика в NETFILTER есть target CLASSIFY, 
с помощью которой можно проставлять пакету класс, заданный в подсистеме traffic control (классифицировать пакет). 
Классификация возможна только в таблице mangle NETFILTER.
</p>
<p>
В рассматриваемой конфигурации используется классификация с помощью NETFILTER,
так как для меня проще отбирать нужные пакеты с помощью NETFILTER, чем возиться
с классификаторами tc filter. В таблице mangle у меня уже формируются цепочки
по потокам - остается только вставить нужное правило классификации в нужную цепочку.
Возможно классификация при помощи tc filter быстрее, особенно для большого 
числа правил (см. tc hashing filters). Пока не ясно как в классификаторе tc filter 
отбирать указывать условия эквивалентные возможностям ipset. 
</p>

<div class="excl">
<b>Трафик будем классифицировать с помощью NETFILTER, а не с помощью tс-фильтров.</b>
</div>
<p>
Классификация возможна только в таблице <i>mangle</i>. Правила в таблице <i>mangle</i> организуем по тому же
принципу, что и в таблице <i>filter</i>: каждому потоку свою цепочку с правилами плюс
переходы на них со встроенных цепочек согласно адресам сетей.
Рассмотрим <a style="font-size:20px" href="iptables_mangle.html" target="_blank">листинг команды <i>iptables -nvL -t mangle</i></a>:
нас интересуют цепочки <b>06_INET_LAN</b> и <b>07_INET_DMZ</b>.
</p>

<p>
В цепочке <b>06_INET_LAN</b> мы классифицируем трафик для всех четырех хостов (boss, admin, user, server) в сети LAN.
Правила (выделены синим) для перехода в цепочку <b>06_INET_LAN</b> находятся во встроенной цепочке <b>FORWARD</b>.  
</p>

<p>
Правила для классификации трафика для хостов в сети <b>DMZ</b> расположены в цепочке <b>07_INET_DMZ</b>.
Правила (выделены желтым) для переходов в цепочки <b>07_INET_DMZ</b> расположены во встроенной цепочке <b>FORWARD</b>.  
</p>

<h3 id="tc_squid">Управление трафиком и SQUID</h3>
<p>
Как видно из рис. 10 и 11 хосты сетей LAN и DMZ могут использовать канал до провайдера
не только напрямую, но и опосредованно - через прокси-сервера, если таковые работают
на шлюзе. В рассматриваемом примере это SQUID - http/ftp прокси.
</p>
<p>
Исходящий трафик от хоста сети LAN к какому-либо http-серверу может быть явно направлен 
через прокси, либо принудительно перенаправлен на локальный прокси средствами NETFILTER (transparent proxy, "прозрачный прокси").
В результате узел сети LAN уже не устанавливает прямое соединение с целевым сервером:
сначала трафик идет на прокси-сервер, а уже прокси-сервер создает новое соединение
с целевым сервером и шлёт пакеты "от своего имени". 
В обратном направлении происходит тоже самое:
целевой сервер отвечает прокси-серверу, а прокси-сервер передает данные клиенту.
Дополнительная сложность возникает из-за того, что прокси-сервер может
отдать данные клиенту взяв их из кэша и при этом канал до провайдера не будет занят передачей этих данных.
</p>

<p>
Это осложняет задачу управления трафиком, так как трафик, проходящий через прокси не 
является "непрерывным": он разбит на два этапа. <b>При этом потребителем трафика</b> 
http-сервер -> прокси, как видно из рис. 10 и 11, <b>является сам шлюз, а не конечный клиент</b>.  
</p>

<p>
Поэтому, строго говоря, для такого трафика исчезает возможность управлять им на исходящих
(<i>eth0, pppN, ifb0</i>) интерфейсах, а управление входящим трафиком (до SQUID'а, на eth1)
малореалистично: надо анализировать пакеты на уровнях выше транспортного, так как
"где чей пакет" уже невозможно понять на транспортном уровне (по IP источника - 
источник один - локальный процесс squid'a) а также "увязывать" входящую дисциплину для
<i>eth1</i> и исходящих (<i>eth0, pppN, ifb0</i>) интерфесов. 
</p>

<p>
Есть два пути решения задачи управления трафиком при наличии прокси на шлюзе:
<ul>
<li>использовать встроенные (если они есть, конечно) в прокси возможности управления трафиком</li>
<li>попытаться косвенно повлиять на использование канала прокси-сервером, управляя трафиком
на том интерфейсе, через который прокси отдает трафик внутренним хостам</li>
</ul>
Оба решения имеют свои плюсы и минусы. Рассмотрим их.
</p>

<h4 id="tc_squid_v1">Вариант №1: использовать возможности SQUID (версии 2.x) по управлению трафиком </h4>
<p>
Первый вариант - воспользоваться возможностями самого SQUID'а по управлению трафиком - <b>delay pools</b> 
("пулы задержек").

<p>
<b>Минусы</b>:
</p>
<ul>
<li>Единое управление загрузкой канала, как таковое невозможно: клиенты могут
получать одновременно трафик как напрямую так и через squid: первым мы управляем
через подсистему ядра ОС, другим - через настройки squid и <b>они никак не связаны 
между собой</b> и нет возможности из увязать.
Например, если хосту выделена полоса с помощью <i>tc</i>, то это никак не влияет на squid
и трафик получаемый хостом через squid никак не будет ограничен.
Если в этом случае задать некие ограничения в настройке самого squid'а, то в лучшем случае 
мы ограничим узел только раздельно по этим двум путям получения трафика: невозможно
будет задать единое суммарное ограничение, которое бы перераспределялось бы между 
трафиком через squid и другим трафиком.
К тому же возможности squid в этом плане не обладают гибкостью traffic control ядра.
Фактически можно лишь задать максимальную скорость для каждого хоста: невозможно 
из выделенной каждому пропускной способности, свободную перераспределять между другими,
как это возможно в tc ядра.
Например мы хоти ограничит download некоего хоста общей величиной 100 кбит/с.
Для этого придется задать некую величину для tc и некую для squid.
Зададим в squid - 50 кбит/c (для http трафика), и c помощью tc - 50 кбит/с (для 
всего остального трафика).
Получим следующую проблему: пользователь этого хоста сможет получить максимальную 
скорость скачивания по http (например в браузере) 50 кбит/c, а не 100 кбит/c как 
все хотели бы.
Т.е. выделенные хосту через tc другие 50 кбит/c могут быть использованы только трафиком, 
идущим не через squid!
Та же проблема вознивает и при динамическом перераспределении пропускной способности
между разными хостами (пользователями). Если канал свободен (пользователи "мало качают"), 
и есть некто, кому надо использовать канал для быстрого скачиваниия, то он не сможет
"занять" у других неиспользуемую ими пропускную способность, как это можно сделать
в tc. Это сильно ограничивает гибкость и возможности всей системы в целом.
Если же мы и в squid выделим - 100 кбит/c, и c помощью tc - 100 кбит/с, то пользователь
"получит свои 100 кбит/c", однако на самом деле он сможет загрузить канал на все 200 кбит/с,
качая одновроеменно через squid и по другим протоколам, что уже не устраивает нас, 
так как нам надо "уложить" всех потребителей в пропускную способность канала.  
</li>
<li>
Усложняется администрирование: задавать ограничения надо в двух различных системах,
что на практике приводит к различным несоответствиям (в tc задал, в squid забыл; 
в tc изменил, в squid забыл и т.п.). Ухудшается восприятие всей системы управления 
трафиком в целом. 
</li>
<li>
Возможности управления трафиком в SQUID беднее в сравнении с возможностями tc и 
исопльзовать их (в случае индивидуальных настроек) неудобно.
В SQUID (2.x) есть три класса (а проще говоря типа) delay pools (пулов задержки или проще буферов).
</p>
<b>Класс 1</b> позволяет задать индивидульное ограничение каждому <i>ACL</i> SQUID'a. Этот тип можно 
использовать так: создать кол-во пулов, равное количеству ограничиваемых по скорости 
клиентов и затем присвоить каждому клиенту свой <i>ACL</i>, а каждому <i>ACL</i>'у - свой пул 
с индивидуальным ограничением скорости. Т.е. в настройке SQUID надо писать кучу <i>ACL</i>'ов,
кучу пулов, задать общее количество пулов и следить за их использованием при 
удалении/добавлении новых пользователей. <b>Естественно, в этом варианте, нуждающиеся клиенты не смогут 
занять у других, неиспользуемые ими ресурсы полосы пропускания</b>.
</p>
 
<p>
<b>Класс 2</b> позволяет задать одно, общее ограничение и задать в рамках этого ограничения
ограничение для каждого из хостов, но не произвольно, а только по фиксированным битам ipv4-адреса (сеть класса C).
Т.е. есть хосты будут выделяться по битам с 25 по 32, а задать индивидуальное 
ограничение можно только одинаковое для всех! <i>В этом варианте, нуждающиеся клиенты тоже не смогут 
занять у других, неиспользуемые ими ресурсы полосы пропускания, так как индивидульное ограничение
жёсткое и единое для всех.</i>
Пример: 
<pre class="cmd">
delay_pools 1                       # общее кол-во пулов - один

acl server1 src 192.168.1.1         # ACL для компьютера с адресом 192.168.1.1 
acl server1 src 192.168.1.2         # ACL для компьютера с адресом 192.168.1.2 
acl client1 src 192.168.0.1         # ACL для компьютера с адресом 192.168.0.1
acl client1 src 192.168.0.2         # ACL для компьютера с адресом 192.168.0.2

delay_class 1 2                     # пул №1 (а он у нас всего один) имеет класс 2
delay_parameters 1 40960/20480 10240/10240 # параметры пула №1 
delay_access 1 allow server1        # этот у нас будет использовать пул №1
delay_access 1 allow server2        # и этот у нас будет использовать пул №1
delay_access 1 allow client1        # и этот у нас будет использовать пул №1
delay_access 1 allow client2        # и этот у нас будет использовать пул №1
</pre>
Это значит, что четыре хоста (server1, server2, client1, client2) 
могут потреблять суммарно не более 40 кбайт\с,
а каждый из них - не более 10 кбайт\с. Цифры <i>40960/40960</i> - по сути означают:
первая цифра - максимальная скорость, с какой будет идти передача, после того как 
будет скачано кол-во байт указанное во второй цифре. Т.е. можно "сжечь" (получить 
без ограничения скорости) 20 кбайт данных, а после этого вступит в силу ограничение 
скорости в 40 кбайт\с. 
</p>
<p>
<b>Класс 3</b> похож на 2-ой, но добавляет еще одно суммарное ограничение - по подсетям.
</p>

<p>
В SQUID 3.x появились пулы <b>классов 4 и 5</b>, но они также ничего нового в суть 
управления трафиком не вносят.
</p>
</ul>

<p>
<b>Плюсы</b>:
</p>
<ul>
<li>
Плюс всего один: пользователи получат трафик из кэша SQUID'а на максимальной скорости. Так и должно быть,
так как то, что squid взял из кэша уже не качается и не занимает канал. 
</li>
</ul>

<p>
Итак основными недостатками управления тарфиком в SQUID <b>с помощью delay_pools</b> являются: <b>отсутствие интеграции
с подсистемой управления трафиком ядра (traffic control) и как следствие - невозможность 
гибкого распределения неиспользуемой пропускной способности между нуждающимися в ней хостами, 
невозможность управления трафиком рамках единой системы и связанная с этим громоздкость конфигурирования 
и администрирования. 
</b>
</p>

<h4 id="tc_squid_v2">Вариант №2: косвенное влияние на использование канала SQUID'ом плюс частичная "интеграция" с traffic control</h4>
<p>
В этом варианте с помощью tc ограничивается скорость, <b>с которой SQUID отдает трафик 
хостам внутренних сетей</b> (зелёные стрелки на рис. 10 и 11). Это косвенно влияет на то, с 
какой скоростью squid загружает себе данные для каждого клиента.
</p>
<p>
Для этого трафик от SQUID в сЕти <b>LAN</b> и <b>PPTP</b> (цепочка <b>02_GW_LAN</b>) 
надо проклассифицировать с помощью iptables (<i>-j CLASSIFY</i>) аналогично цепочке <b>06_INET_LAN</b> (таблица mangle). 
И, естественно, этот трафик должен быть пернаправлен на интерфейс <i>ifb0</i>, что мы уже рассмотрели ранее. 



Т.о. трафик от SQUID будет проклассифицирован так же как и трафик, идущий к 
пользователям напрямую, "мимо" SQUID'а, и управление этим трафиком будет 
происходить согласно правилам для "прямого" трафика. 
Т.е. для целей управления трафиком, трафик идущий от SQUID во внутренние сети, 
так же, как интерпретируется как прямой трафик из Интернета. 
</p>
<p>
Чтобы проклассифицировать трафик, идущий от SQUID во внутренние сети, надо вычленить этот трафик,
из общего трафика между шлюзом и внутренними сетями.
</p>

<p>
Трафик от SQUID во внутренние сети пометим (CONNMARK) в цепочке 01_LAN_GW_ALLOW таблицы filter:
<pre class="cmd">
CONNMARK   tcp  --  *      *       0.0.0.0/0            0.0.0.0/0           tcp dpt:3128 state NEW CONNMARK xset 0x3128/0xffffffff 
CONNMARK   tcp  --  *      *       0.0.0.0/0            0.0.0.0/0           tcp dpt:3129 state NEW CONNMARK xset 0x3128/0xffffffff 
</pre>
</p>

<p>
Далее, помеченный трафик из цепочки OUTPUT таблицы mangle отправим 
на классификацию в цепочку 06_INET_LAN таблицы же mangle:
<pre class="cmd">
06_INET_LAN  all  --  *      eth0    0.0.0.0/0            192.168.0.0/24      connmark match 0x3128 
06_INET_LAN  all  --  *      ppp+    0.0.0.0/0            192.168.1.0/24      connmark match 0x3128
</pre>
Как результат: пакеты от SQUID к внутренним хостам будут проклассифицированы также,
как и пакеты приходящим им напрямую из Интернета. 
</p>

<p>
Таким образом мы получаем возможность (косвенно) управлять трафиком, который идет 
пользователям через SQUID, средствами traffic control.
</p>
<p>
Однако, при этом, возникает следующий минус: <b>контент, который SQUID возьмет из 
своего кэша, будет отдаваться не на максимально возможной скорости</b>, а с ограничением, 
установленным для данного клиентского хоста. Это происходит 
потому, что подсистема traffic control, естественно, не может "понять" откуда SQUID 
получил передаваемый контент - из своего кэша или из Интернета.
</p>

<p>
От этого недостатка можно избавиться следующим образом. Начиная с версии SQUID 2.7 появилась возможность
пометить трафик, который squid берет из кэша. Для "отметки" можно использовать 
<b>поле TOS заголовка IPv4 пакета</b>.
<div class="excl">
Т.о. можно настроить SQUID так, что контент взятый из кэша,
будет иметь одно значение поля TOS, а контент которого нет в кэше SQUID'а 
(и который поэтому загружен SQUID'ом из Интернета) будет иметь другое значение поля TOS. 
</div>

</p>
<p>
Ниже показаны две дерективы, которые надо использовать в конфигурационном файле SQUID 2.7:			 
<pre class="cmd">
zph_mode tos
zph_local 0x88
</pre>
Первая (<i>zph_mode tos</i>) включает пометку пакетов в зависимости от попадания/промаха (HIT/MISS) 
в кэше SQUID. 
<br>Вторая (<i>zph_local 0x88</i>) - задает (произвольное) значение поля TOS IP-пакета 
для контента взятого из локального кэша SQUID'а.
</p>

<p>
<b>С этими директивами, IP - пакеты контента, взятого из кэша (HIT), будут иметь 
поле TOS равное 0x88, а пакеты контента взятого не из кэша (MISS) - 0x00.</b>
</p>

<p>
На данный момент предыдущие варианты использования поля TOS аннулированы 
(<a href="http://tools.ietf.org/html/rfc3168#section-22" target="_blank">RFC 3168, section22</a>) и
сейчас биты 0-5 это DSCP, а 6-ой и 7-ой - ECN 
(<a href="http://tools.ietf.org/html/rfc3168#page-8" target="_blank">RFC 3168, page 8</a>), 
то число, которое вы выбираете
для <i>zph_local</i>, в двоичной форме должно иметь нули в 6-ом и 7-ом битах.
Например: 0x88 - 100010<b>00</b> (136 дес.), 0x58 - 010110<b>00</b> (88 дес.), 0x20 - 001000<b>00</b> (32 дес.).  
</p>

http://bytesolutions.com/Support/Knowledgebase/KB_Viewer/ArticleId/34/DSCP-TOS-CoS-Presidence-conversion-chart.aspx

<p>
В SQUID 3.1 принцип тот же, но директива называется по другому (<b>qos_flows</b>) 
и имеет другой синтаксис:
(<a href="http://wiki.squid-cache.org/Features/QualityOfService" target="_blank">http://wiki.squid-cache.org/Features/QualityOfService</a>)
</p>

</p>

</p>
<b>Как результат - исчезают все минусы, указанные в Варианте №1</b>: появляется возможность 
целостного управления входящим трафиком из сети Интернет во внутренние сети, упрощается 
админстрирование, так как не надо управлять скоростями в двух разных системах (ядре и SQUID).
</p>

<br>
<h2 id="rw">Roadwarriors: мобильные клиенты. Доступ к внутренним сетям и VPN.</h2>
<p>
Со временем у сотрудников появляется необходимость получить доступ к ресурсам
внутренних сетей извне. Например, сотруднику надо из Интернета (дом, командировка)
подключиться удаленным рабочим столом к своему компьютер, стоящему в офисе и что-то
на нём посмотреть или поработать с него с какими-либо проложениями. 
Фактически, это позволяет находясь вне офиса, работать за своим 
офисным компьютером почти также как если бы сотрудник лично сидел за ним в офисе:
получать тот же доступ к внутренним ресурсам и т.п.
Таким же образом можно организовать прямой доступ к серверам и приложениям,
работающим на хостах внутри офисной сети, не выставляя их в публичный доступ. 
Или, например, маршрутизировать трафик от удаленного работника через офисный шлюз,
создавая для ресурсов Интернета иллюзию того, что трафик идет непосредственно из офиса.
Это полезно, если нужные сотруднику ресурсы разрешают доступ только 
с адресов офисного шлюза. 
</p>
<p>
"Напрямую" такое сделать невозможно: и из-за фаервола и из-за частных адресов, 
используемых во внутренних сетях. Непосредственно можно подключиться только хостам,
имеющим публичные IP-адреса (например сам шлюз и сеть <b>DMZ</b>) и только 
если к ним разрешён доступ в фаерволе.  
</p>

<p>
Для того, чтобы организовать такой "прямой" и что важно защищённый доступ 
к хостам внутренней сети извне можно использовать VPN.
По сути VPN создаёт туннель, виртуальное соединение, между компьютером мобильного 
клиента и шлюзом, инкапсулируя полезный трафик "внутри" "несущего" трафика между ними. 
Кроме того, обычно, VPN можно настроить так, что для создания этого туннеля 
(т.е. для подключения к VPN серверу) мобильный клиент должен пройти процедуру 
аутентификации. Т.о. только лица, знающие аутентификационные данные смогут
подключиться к VPN серверу и получить доступ к внутренним сетям. 
</p>

<p>
Естественно, реализация такой возможности, <b>потенциально</b> открывает 
для посторонних ещё одну возможность получить доступ к внутренней сети. 
В случае утечки аутентификационных данных (ключей, паролей) легитимных 
пользователей, посторонний сможет подключиться к VPN-серверу и получить доступ
ко всему, к чему имеет доступ реальный пользователь. 
Из-за неправильного конфигурирования VPN-сервера или наличия уязвимостей в 
сервере/клиенте/протоколе VPN, возможны неавторизованные подключение, 
взлом сервера VPN или шлюза или всей сети в целом.
</p>

<p>
В любом случае, рекомендуется очень чётко ограничить, к каким внутренний ресурсам
получает доступ пользователь через VPN. Однако, например, если сотруднику разрешено 
подключиться через VPN удалённым рабочим столом (получить шелл) к компьютеру внутри сети, 
то уже его мало что сможет ограничить в его действиях. Поэтому желательно 
сегментировать внутреннюю сеть, изолируя различные её части друг от друга, тем самым
ограничивая часть сети, на которую сможе воздействовать злоумышленник, получивший 
такой доступ. 
</p>

<h3 id="rw_vpn">Функционирование  VPN с точки зрения фаервола</h3>
<p>
Как уже говорилось выше, VPN всего лишь создает туннель между VPN-сервером и 
мобильным клиентом.  
Поэтому всех подключенных клиентов VPN (или их часть) можно рассматривать 
как просто еще одну IP-сеть, подключенную к шлюзу через (виртуальный) интерфейс.
С точки зрения маршрутизации и фаервола это такая же сеть, как и например <b>LAN</b>, <b>DMZ</b>.
Опять, же таких VPN сетей можно организовать несколько, выделив каждой группе 
пользователей по отдельной IP-сети.
</p>

<p>
В зависимости от конкретной реализации, сервер VPN либо может создавать
виртуальный интерфейс, являющиеся серверным концом этого туннеля, либо работать 
без создания виртуальных интерфейсов. 
В случае, если используемый сервер VPN создает для своей работы виртуальные интерфейсы
(напр. OpenVPN, PPTP), эти интерфейсы будут входящими/исходящими в iptables  
для туннелируемых пакетов (payload protocol).
</p>

<p>
Если используемый сервер VPN не создает виртуальные интерфейсы в своей работе (напр. IPSec/NETKEY),
то в цепочках iptables будут "видны" уже распакованные из туннеля
и заново инжектированные в сетевой стек пакеты VPN-протокола (payload protocol), 
естественно уже с частными адресами VPN-клиентов.
При этом входящим/исходящими интерфейсом в iptables для этих пакетов  будет тот
физический интерфейс, на который подключились VPN-клиенты.
Поэтому на этом интерфейсе будут одновременно "видны" как пакеты c частными
адреcами VPN-клиентов, так и пакеты других интернет-соединений с публичными адресами.
При этом пакетов самого туннеля (delivery protocol) в iptables не "видно". 
(см. <a href="#filter_note1">Примечание</a>)
</p>

<p>
Исходя из того что VPN-клиентов можно рассматривать как еще одну сеть,
следует что управлять доступом и трафиком VPN-клиентов к другим сетям можно 
абсолютно также как и в случае "обычных" физических сетей.    
</p>

<p>
<center><img src="img/vpn_flows.png"/></center>
<br>
<center>Рис. 16 Потоки VPN-LAN инкапсулированные в VPN-туннель.</center>
</p>

<p>
Как показано на рис. трафик между VPN-клиентом <i>192.168.255.5</i> 
и хостами сети <b>LAN</b> (payload protocol) трактуется не как Интернет-трафик, 
хотя VPN-клиент для нас "находится в Интернете", а как отдельный трафик между
сетями <b>VPN</b> (192.168.255.0/24) и <b>LAN</b>.
В свою очередь этот трафик инкапсулирован в VPN-туннель (delivery protocol) и 
трактуется с точки зрения фаервола как трафик Интернет-шлюз и относится к 
потокам INET>eth1-GW и GW>eth1-INET (цепочки потоков 18_VPN_LAN и 19_LAN_VPN). 
</p>

<p>
В рассматриваемом примере VPN-клиентам выделен диапазон адресов <i>192.168.255.0/24</i>. 
В цепочке FORWARD имеем следующие цепочки для трафика  между VPN-клиентом и
сетью LAN и между VPN-клиентом и Интернетом.
<pre>
18_VPN_LAN  all  --  tun1   eth0    192.168.255.0/24     192.168.0.0/24
19_LAN_VPN  all  --  eth0   tun1    192.168.0.0/24       192.168.255.0/24
20_VPN_INET  all  --  tun1   eth1    192.168.255.0/24    !192.168.0.0/16
21_INET_VPN  all  --  eth1   tun1   !192.168.0.0/16       192.168.255.0/24
</pre>
Так как у нас нет необходимости в обмене трафиком между VPN-клиентами 
и внутренними PPTP-клиентами, то этот трафик и не разрешён.
Цепочки же 20_VPN_INET, 21_INET_VPN как раз и нужны для случаев когда VPN-клиенту 
необходимо свой трафик "пропустить" не напрямую через своё подключение к Интернету,
а "в обход" - через офисный шлюз, чтобы исходящие пакеты в результате NAT имели адрес шлюза. 
Это надо, например, если на фаерволе клиентов разрешёно подключение по RDP-протоколу
только с офисного IP-адреса, а сотруднику надо подключиться к клиенту из дома.
Он подключается к офисному шлюзу посредством VPN и на своём компьютере добавляет маршрут,
указывающий, что к такому-то IP отправлять пакеты не через шлюз по умолчанию, а через туннель и пакеты
к этому адресу начинают уходить в туннель и далее через офисный шлюз к адресу назначения.    
</p>

<p>
Вкратце рассмотрим правила в цепочках <b>18_VPN_LAN</b> и <b>19_LAN_VPN</b>.<br>
Видно что в цепочке <b>18_VPN_LAN_ALLOW</b> у нас правила для двух VPN-клиентов:
с адресом <i>192.168.255.5</i> и с адресом <i>192.168.255.9</i>.
Клиенту <i>192.168.255.5</i> разрешён доступ к своему офисному компьютеру по 
RDP-протоколу (tcp dpt:3389) и напрямую к серверу по любому протоколу.<br> 
Клиенту <i>192.168.255.9</i> также разрешён доступ к своему офисному компьютеру по 
RDP-протоколу и он может пинговать свой	компьютер (icmp). 
В цепочке же <b>19_LAN_VPN</b> есть только <i>ACCEPT...state RELATED,ESTABLISHED</i>.
Это позволяет VPN-клиентам инициировать новые соединения на разрешённые адреса и порты,
а вот к ним из сети <b>LAN</b> (и других) никто подключится не сможет -  
хосты сетей <b>LAN</b>, <b>PPTP</b>, <b>DMZ</b> не будут их "видеть". Это полезно 
для того, чтобы пользователи внутри офиса не могли случайно или умышленно негативно 
воздействовать на подключённых VPN-клиентов (например, взломать или заразить их компьютеры).
</p>

<h3 id="rw_route">VPN и маршрутизация</h3>
<p>
Маршрутизация с VPN работает также как и с "обычными" сетями. Дополнительно
можно заострить внимание на VPN-клиенте. 
</p>
<p>
К VPN-серверу мобильный клиент подключается с помощью специального ПО - VPN-клиента, 
который либо уже входит в состав ОС, либо устанавливается отдельно.
Само мобильное устройство, естественно, должно быть подключено к сети Интернет или 
иметь выход в Интернет через другие сети. Обычно выход в Интернет описывается 
маршрутом по умолчанию, что означает, что <b>все</b> пакеты для неизвестных 
(не описанных в таблице маршрутизиации) сетей будут уходить туда - в Интернет.
В случае же использования VPN, мобильному клиенту надо выборочно направлять трафик:
пакеты к определённым хостам надо направлять в VPN-туннель, а не напрямую в Интернет.
Достигается это добавлением соответствующих
маршрутов на компьютере мобильного клиента. При этом возможны ситуации, когда
подключение к VPN-серверу прошло нормально, "всё должно работать, но не работает".
Это может быть связано с вопросами маршрутизации. Рассмотрим пример. 
</p>

<p>
Пусть наш сотрудник, находится в некоей организации (или домашней сети за роутером)
со своим ноутбуком.
Диапазон адресов сети этой организации совпадает с диапазоном адресов офисной сети -
и там и там <i>192.168.0.0/24</i>. Сотрудник подключает свой ноутбук к сети организации
и ноутбуку присваивается адрес, допустим, <i>192.168.0.17</i>. Сотрудник подключается к офисному
VPN-серверу и пытается удалённо через VPN подключиться к своему компьютеру внутри офиса с адресом
<i>192.168.0.10</i>. Естественно ничего не получается, потому что в таблице маршрутизации
его компьютера при подключении к сети, автоматически был прописан маршрут, говорящий
что сеть <i>192.168.0.0/24</i> подключена непосредственно к его ноутбуку.
Другими словами попытка подключения происходит к компьютеру с адресом <i>192.168.0.10</i> <b>внутри
той сети, где находится сотрудник</b>, а не к компьютеру находящемуся 
в нашей офисной сети (пакеты маршрутизируются в прямо "сетевуху", а не в туннель).
Т.е. надо прописать дополнительный маршрут в таблицу маршрутизации компьютера сотрудника.  
</p>

<p>
Однако, если указать маршрут ко всей сети <i>192.168.0.0/24</i> через туннель, то 
работать ничего не будет, потому что физический шлюз организации, через который 
сотрудник "выходит" в Интернет и далее к VPN-серверу перестанет быть доступен.
Перестанет быть доступным вообще всё: и сеть организации в которой находится 
сотрудник и Интернет. 
Ведь компьютер начнёт "пытаться пихать" в туннель <b>любые</b> пакеты из-за того,
что шлюз для выхода в Интернет (тот который в маршруте по умолчанию) тоже 
находится в местной сети <i>192.168.0.0/24</i>!
</p>

<p>
Правильным в этом случае является указание дополнительного маршрута не ко всей сети,
а только до нужного сотруднику хоста в офисной сети. В этом случае, только трафик к этому 
адресу будет направляться в туннель. Естественно, этот адрес не должен совпадать
ни с адресом шлюза, ни с адресом ноутбука сотрудника в местной сети.
В случае, если адрес местного шлюза совпадает с адресом того хоста в офисной сети,
к которому сотруднику надо получить доступ, то никакими средствами не получится
получить такой доступ.
</p>

<p>
Следовательно, диапазон адресов для VPN-клиентов надо выбирать так, чтобы он с 
наименьшей вероятностью мог совпасть с адресами тех сетей, где предстоит 
находиться мобильному клиенту. В случае, если мобильный клиент подключается
к Интернету напрямую, получая публичный адрес, описанной проблемы не возникает.
</p>

<h3 id="rw_choice">Выбор VPN</h3>
<p>
Выбирать будем из следующих вариантов: PPTP, L2TP, SSTP, IPSec, OpenVPN
</p>

<p>
Необходимы: 
</p>
<ul>
<li>простота в настройке;</li> 
<li>надежность в плане безопасности (архитектурные ошибки и уязвимости в реализации);</li>
<li>100% интероперабельность сервера с Windows-клиентами;</li>
<li>возможность автоматического и ручного присвоения VPN-клиентам виртуальных IP-адресов из пула;</li>
<li>работоспособность через NAT и различное сетевое оборудование;</li>
<li>аутентификация по паролю и/или x.509-сертификатам;</li>
</ul>

<h4>PPTP</h4>
<p>
Слабая аутентификация. Потенциальные проблемы с прохождением протокола GRE
через "кривые" маршрутизаторы (напр. Eltex NTE-RG-1402G-W с прошивками до 04.02.2011).
Демон не расчитан на прослушивание разных интерфейсов с разными настройками 
(в т.ч. разными базами пользователей).
Используя xinetd в качестве костыля, можно получить два pptp-демона с разными настройками
- один слушает в LAN другой в Интернет.
Однако при перезапусках такой конфигурации бывали сбои - зависело от того, к какому из них 
подключится первый клиент. Если в настройках pptp-демона, слушающего на 
Интернет-интерфейсе включить обязательное шифрование, то у pptp-клиентов в сети LAN,
которые используют PPTP без шифрования, случайным образом становились недоступны различные серверы в Интернете.
Причина сразу была неясна - разбираться не стал, выяснил только, что 
если все клиенты обоих демонов работают с одинаковыми настройками шифрования, то
работает без сюрпризов. Возможно проблема связана с MTU. 
Макс. скорость передачи PPTP-клиента (Windows 7) по 100 Мбит ethernet-каналу 
чуть меньше (8-9 Мбайт/с), чем у IPSec, но с бОльшей загрузкой CPU (10-15%).
Задержки (RTT) в ping больше в 3-4 раза, чем в IPSec и OpenVPN и "плавают".   
</p>

<h4>L2TP</h4>
<p>
<b>L2TP</b> - протокол туннелирования. Не обеспечивает шифрование.
Обычно в L2TP-туннель инкапсулируется PPP-трафик.
В свою очередь, для защиты  самого L2TP туннеля его туннелируют
через IPSec. Т.е. для организации VPN на основе L2TP надо конфигурировать
в три раза больше сущностей чем в IPSec: L2TP, PPP и IPSec.
Уж проще тогда использовать "голый" IPSec или OpenVPN.  
</p>

<h4>SSTP</h4>
<p>
Протокол от Microsoft для туннелирования PPP или L2TP через SSL:
IP(TCP(SSL(SSTP(PPP-payload)))). Во время аутентификации payload-протокол
еще и в HTTP "заворачивается". SSTP-сервера для Linux не существует.
Т.о. пользоваться этим "чудом" реально только на продукции Microsoft. 
</p>

<h4>IPSec</h4>
<p>
KLIPS - первый стек IPSec проекта FreeS/WAN для Linux версий 2.4 и 2.6.<br>
NETKEY - "родной" IPSec стек ядра Linux версий 2.6 и 3.<br>
Для userspace (IKE-демоны, библиотеки и утилиты) существуют:
<ul>
<li>"родные" утилиты ipsec-tools (порт KAME project);</li> 
<li>FreeS/WAN - уже закрытый проект из которого были форкнуты OpenSWAN и strongSWAN;</li> 
<li>strongSWAN;</li>
<li>OpenSWAN;</li>
</ul>
</p>

<p>
Для подключения к IPSec-серверу нужен клиент под Windows (ясно что под Linux всё родное).
Мне известен только один бесплатный IPSec клиент - Shrew Soft VPN Client.
В текущей версии 2.x поддерживается только IKEv1. Разработка версии с поддержкой
IKEv2 не предвидится. 
Shrew Soft VPN Client хорошо взаимодействует с ipsec-tools и strongSWAN.  
</p>

<p>
В Windows 7 есть встроенная поддержка IKEv2, но установить туннель
из Windows 7 с ipsec-tools или strongSWAN так и не удалось предположительно из-за сертификатов. 
Правда с этими же сертификатами Shrew Soft VPN Client успешно подключался к 
ipsec-tools (IKEv1-демон racoon) и к strongSWAN (IKEv1-демон pluto), но, видимо, 
у Microsоft свои недокументированные особенности. 
</p>

<p>
Вот что показала проверка реальной работы. 
</p>

<p>
У <b>ipsec-tools</b> обнаружился недостаток - IKE-демон racoon не позволяет задать определённому
клиенту определённый виртуальный IP (адрес клиентской части туннеля). Из-за этого
невозможно указать правила фаервола для конкретных клиентов (сотрудников).
Выснилось, что этот функционал можно реализовать с использованием RADIUS-сервера,
однако, как случайно выяснилось в переписке, в Debian IKE-демон racoon скомпилирован
без поддержки RADIUS-сервера, и изменений не предвидится (см. 
<a href="http://lists.shrew.net/pipermail/vpn-help/2010-May/002435.html">здесь</a> и 
<a href="http://lists.shrew.net/pipermail/vpn-help/2010-May/002438.html">здесь</a>)
Работа через NAT не проверялась.
</p>

<p>
Сколько нибудь внятную документацию <b>OpenSWAN</b> и информацию о совместимости
с другими реализациями на сайте www.openswan.org найти не удалось.
</p>

<p>
<b>strongSWAN</b> лучше документирован авторами, имеет более зрелую реализацию IKEv2 и 
активнее развивается. И из рассматриваемых реализаций IPSec, только strongSWAN 
формально отвечает всем требованиям, перечисленным в начале раздела. Однако от
strongSWAN (и от IPSec) пришлось отказаться, так как в результате тестов с 
подключением клиента из-за NAT (Linux box и DLink DIR-300) выяснилось, что 
туннель создаётся, но через пару секунд "отваливается". При "прямом" (без NAT) 
подключении к strongSWAN всё отлично работало. Eстественно NAT-Traversal и всё что надо для его работы
(открытые порты) при этом было включено и настроено. Короткая переписка с авторами
strongSWAN и Shrew Soft VPN Client (см. ниже) ничего так и не прояснила.
Позже появилась непроверенная догадка, что проблема как-то связана с MTU.
</p>

<p>
В ходе настройки, выяснилось, что настройка IKEv1 демона для IPSec довольно муторное дело,
так как без опыта, по логам трудно понять что не так и в чём именно ошибка настройки.
У strongSWAN (pluto) довольно информативные логи в сравнении с net-tools (racoon).
Основные проблемы возникали из-за ошибок при генерации сертификатов.
</p>

<p>
Производительность IPSec оказалась самой высокой: около 10 Мбайт/с на 100 Мбит ethernet-канале
при самой низкой (2-3%) загрузке CPU на клиенте. 
</p>

<p>
<a href="http://tiebing.blogspot.com/2011/06/fairly-new-comparison-of-openswan-and.html">http://tiebing.blogspot.com/2011/06/fairly-new-comparison-of-openswan-and.html</a><br> 
<a href="http://www.installationwiki.org/Openswan#Choosing_the_Kernel_IPsec_Stack">http://www.installationwiki.org/Openswan#Choosing_the_Kernel_IPsec_Stack</a><br>
<a href="http://habrahabr.ru/post/250859/">http://habrahabr.ru/post/250859/</a><br>
</p>

<h4>OpenVPN</h4>
<p>
Так как мне не удалось заставить IPSec работать в случае нахождения мобильного клиента за NAT'ом,
то остался последний вариант - OpenVPN, который собственно и используется.   
</p>
<p>
Всех вышеперечисленных недостатков у OpenVPN нет, но есть свои собственные.
В ОС Windows подключиться из под пользователя без административных прав, 
из идущего в комплекте GUI-клиента не получится - OpenVPN не сможет создать 
TAP-устройство и прописать новые маршруты.
Для подключения из под пользователя без административных прав предлагается
запускать сервис (службу OpenVPN Service). Однако при этом теряетcя возможность 
управления openvpn через штатный GUI-клиент - ни отключиться, ни подключиться,
ни посмотреть статус. Кроме того, при старте службы opevpn автоматически
подключается ко всем сконфигурированным серверам, что не всегда удобно.  
Неудобная схема присвоения адресов концам туннеля, используемая для 
совместимости с TAP-Win32 драйвером.
Производительность ниже чем у PPTP и IPSec: около 5-6 Мбайт/c при 20% загрузке CPU клиента (Celeron 2.4).
Наблюдалось полное блокирование (непрохождение) трафика через туннель при 100% загрузке CPU (Celeron 2.4) 
другими процессами, чего не наблюдалось с PPTP (у PPTP просто раза в 4 увеличивался "пинг"). 
</p>

<br>
<br>
<h2 id="ta">Учет (подсчет) трафика (traffic accounting)</h2>
<p>
Был выбран вариант, использующий счетчики NETFILTER. В нужные цепочки 
вносятся правила для счетчиков. С помощью cron, каждую минуту счетчики в этих цепочках
сохраняются в файлы перенаправлением вывода команды <i>iptables -nvxL цепочка</i> в файл
в имени которого содержится имя потока, дата и время. Таким образом за сутки создается 1440 файлов с 
поминутной историей значений счетчиков. Файлы сгруппированы в каталоги по годам, месяцам и числам.
</p>

<div class='excl'>
Вместо хранения значений счетчиков в текстовых файлах можно использовать RRDtool или другие БД.
</div>

<p>
По прошествии отчетного периода, неактульные данные упаковываются в архив, а 
оригинальные файлы удаляются.
</p>

<p>
Т.о., если не было перезагрузок, то подсчет трафика сводится к следующему:
<ul>
<li>находим значение счетчика в строке с нужным IP в файле, <b>соответствующем концу интересующего периода</b></li>
<li>находим значение счетчика в строке с нужным IP в файле, <b>соответствующем началу интересующего периода</b></li>
<li>вычитаем из более позднего значения счетчика более раннее - получаем расход за период.</li>
</ul>
</p>

<p>
С учётом перезапуска фаервола или перезагрузок шлюза алгоритм несколько усложняется - необходимо 
учитывать сброс счётчиков.
Возможны два варианта: 
либо как-то явно помечать факт перезагрузки, 
либо в процессе подсчета трафика анализировать сохраненные показания счетчиков, "ловя" 
момент обнуления счетчиков по сохранённым показаниям. 
</p>

<p>
Несмотря на то, что есть возможность сохранить правила фаервола вместе с 
показаниями счетчиков в файл, а затем восстановить эти правила, также, вместе со значениями счетчика.
Однако этим крайне нетривиально воспользоваться в случае изменения набора правил.
См. скрипт <i>conf.d/main/ta/print_inet_lan_counters</i> в исходниках.
</p>
ДОПИСАТЬ

<h3 id="ta_conntrackd">Подсчет трафика с помощью conntrackd</h3>
<p>
На момент начала созадния данного текста conntrack-tools не были доступны в 
стабильной версии в Debian 5 Lenny.
Изучить данный вопрос.
<a href="http://conntrack-tools.netfilter.org/manual.html#what">http://conntrack-tools.netfilter.org/manual.html#what</a>.
</p>

<h3 id="ta_ulogd">Подсчет трафика с помощью ulogd</h3>
<p>
Когда ulogd 2-ой версии будет в Debian.
</p>


<h3 id="ta_squid">Учет трафика и SQUID</h3>
<p>
Ситуация с учетом трафика при использовании прокси-сервера на шлюзе схожа 
с ситуацией управлением трафиком: клиенты внутренних сети получают трафик не
напрямую, а опосредованно - через squid, что затрудняет подсчет трафика средствами NETFILTER. 
</p>

<p>
Без веб-прокси трафик между сетями мы подсчитываем в цепочке FORWARD, так как он 
проходит через шлюз от одного интерфейса к другому.
В случае же использования веб-прокси, уже squid будет получать трафик из Интернета
(цепочка INPUT) и затем отдавать его клиентам (цепочка OTUPUT).
</p>

<p>
Поэтому, на уровне NETFILTER, возникает сложности с определением
адресов источников и получаетелей трафика.
В случае использования squid пакеты, приходящие на шлюз из Интернета, будут иметь адрес
назначения равный адресу самого шлюза, a пакеты, отправляемые squid'ом клиентам, будут 
иметь адрес источника равный адресу самого шлюза.
Из-за этого, средствами NETFILTER, крайне затруднительно определить реальные 
адреса назначения.
</p>

<p>
В случае, если хотим поставить счётчики NETFILTER в INPUT <i>eth1, eth3</i>,
то встает проблема определения адреса клиента, для которого squid скачал пакет.
Так как пакеты приходят на адрес шлюза, а не напрямую на адрес клиента, 
то определить адрес истинного получателя трафика можно только анализируя 
HTTP-заголовки, что затруднительно на практике и негативно влияет на производительность.  
</p>

<p>
Существует Application Layer Packet Classifier for Linux (L7-filter), позволяющий
с помощью шаблонов из регулярных выражений с довольно высокой степенью вероятности
определить к какому прикладному протоколу (уровень 7 модели OSI) относится трафик.
Однако этого недостаточно, ведь надо определить не просто протокол, а выделить
из каждого HTTP-заголовка IP-адрес клиента.
Лобовым решением было бы создание индивидуального шаблона, включающего сам IP-адрес
для каждого клиента, но очевидно, что это крайне неэффективный вариант. 
Опять же использование данного фильтра требует патчить и компилировать ядро и iptables.
Т.о. использование L7-filter не подходит для решения данной задачи.
Остаются два рабочих варианта подсчета трафика: подсчет трафика отдаваемого squid'ом
и подсчет трафика пол логам squid'а.
</p>

<p>
Для подсчета трафика, который squid отдаёт клиентам внутренних сетей,
счетчики можно разместить OUTPUT <i>eth0, ppp+</i>.
В этом случае необходимо отличать трафик взятый squid'ом из своего кэша, от трафика
полученного из Интернета.
Сделать это можно с помощью поля TOS IP заголовка, это описано в разделе 
<a href="#tc_squid_v2">"Вариант №2: косвенное влияние на использование канала SQUID'ом плюс частичная "интеграция" с traffic control"</a>.
Кроме того, подсчёт в этом варианте будет давать слегка завышенные результаты, из-за
того что squid будет добавлять в HTTP-заголовки передаваемых клиентам пакетов 
дополнительны данные.
</p>

<p>
Вариант с подсчетом трафика по логам squid'а сводится к парсингу access-логов с информацией 
о каждом HTTP-ответе (адрес клиента и размер). 
Далее полученная таким образом информация о количестве трафика для каждого 
клиентского IP суммируется с данными, полученными с помощью счётчиков iptables.
В результате получается суммарный (через squid + весь остальной) трафик для каждого IP.
</p>
<p>
В данном примереи спользуется вариант с подсчетом трафика по логам squid'а.<br>
См. скрипт <i>conf.d/main/ta/print_inet_lan_counters</i> в исходниках.
</p>

<br>
<br>
<h2 id="ipset">Использование <b>ipset</b>. Оптимизация набора правил</h2>
<p>
Модуль ipset позволяет в одном правиле iptables эффективно выполнить проверку не по 
одному адресу и/ или порту (как в -s/-d и --dport/--sport), а по (большому) списку адресов и/или портов.
Для этого создается нужное кол-во таблиц ipset, которые заполняются проверяемыми IP адресами и/или портами.
Далее созданные таблицы используются в командах iptables для проверки соответствия 
с помощью '-m  set --match-set setname', где setname - имя таблицы. 
В рассматриваемом примере с помощью ipset можно сделать следующие оптимизации.  
</p>
<p>
В INPUT заменить два правила
<pre class="cmd">
03_INET_GW_ISP2  all  --  eth3   *      !192.168.0.0/16       192.168.5.1
03_INET_GW_ISP2  all  --  eth3   *      !192.168.0.0/16       198.51.100.2
</pre>
на одно, предварительно создав таблицу и заполнив её dst-адресами 192.168.5.1 и 198.51.100.2. 
</p>

<p>
В рассматриваемом примере используется такое понятие как "адреса сети Интернет" которыми
в данном случае удобно считать все остальные адреса кроме, адресов локальных сетей.
Т.о. "Интернетом считается" всё что НЕ 192.168.0.0/16. 
Однако, если в локальных сетях одновременно будут использоваться адреса из разных диапазонов
(10.0.0.0/8, 172.16.0.0/12, 192.168.0.0/16), то такой простой вариант указания того,
что такое "Интернет" в правилах iptables не пройдет.
Надо будет либо делать отдельную цепочку со списком сетей, считающихся локальными,
и делать все проверки по этой цепочке, либо делать список этих сетей в таблице ipset
и вставлять проверку по этой таблице  в нужные правила вместо ! -d/s 192.168.0.0/16. 
</p>
<p>
Далее, можно во всех цепочках *_ALLOW,*_SKIP_OR_DENY, *_SKIP_OR_DENY заменить 
последовательности правил с проверкой IP-адресов и портов на правила с проверкой по таблицам ipset.
Фактически, c использованием таблиц ipset становятся лишними сами цепочки 
*_ALLOW,*_SKIP_OR_DENY, *_SKIP_OR_DENY: переходы в соответствующие цепочки можно 
заменить на правила с проверкой по соответствующим таблицам ipset.
Такой вариант приведет к значительному уменьшению общего количества правил.  
При этом изменение настроек фаервола будет сводиться, в основном, к изменению ipset-таблиц,
а не к удалению/добавлению правил iptables.
</p>

<p>
В рассматриваемом варианте ipset не используется в связи с тем, что в Debian
модуль ipset не входит в стандартное ядро.
</p>

<p>
Для установки модулей ядра ipset можно воспользоваться пакетом <b>module-assistant</b>
и с помощью него автоматически скомпилировать и установить пакет <b>xtables-addons-source</b>,
в который входит и модуль ipset.
</p>

<p>
К сожалению для компиляции <b>xtables-addons-source</b> устанавливается компилятор,
заголовки ядра и много еще чего, совершенно не нужного на шлюзе и вообще на сервере, 
находящемся в эксплуатации. Лучше иметь отдельную машину с архитектурой идентичной шлюзу
для компиляции с последующим копированием скомпилированого пакета не шлюз. Процедура не очень удобна. 
</p>

<p>
Использование -m multiport для нескольких tcp/udp портов.
</p>

<br>
<br>
<h2 id="iptables-restore"><b>iptables-restore</b>: атомарная загрузка набора правил</h2>
<p>
Одна из особенностей NETFILTER и iptables заключается в следующем: при внесении
любых изменений в активный (находящийся в ядре) набор правил с помощью команды iptables 
происходит копирование существующего набора правил из kernelspace в userspace, 
далее вносятся изменения и измененный набор загружается обратно в ядро.
Поэтому при таких, инкрементальных обновлениях правил, возможны следующие негативные эффекты.
</p>

<p>
Первый эффект заключается в замедлении загрузке новых и обновлении уже существующих правил:
1-ая команда iptables обрабатывается и загружается в ядро. При выполнении второй команды,
сначала из ядра загружается существующий набор правил (состоящий пока из одного правила),
этот набор парсится, в него добаляется второе правило и новый набор из двух правил загружается в ядро.
При выполнении 3-ей команды происходит то же, но из ядра уже загружается и обрабатывается больше
информации: два правила. Далее, к этим двум правилам добавляется третье и новый набор правил снова загружается в ядро.
И с каждой следующей командой между kernelspace и userspace надо прогнать и пропарсить всё больше информации. 
Это занимает какое-то время. Поэтому когда правил становится много, то замедление загрузки
и обновления правил (время выполнения всего набора команд iptables) становится ощутимым.
</p>

<p>
Второй отрицательный эффект конфигурирования фаервола инкрементальными командами заключается в
следующем.
Нередко, набор правил NETFILTER имеет такую структуру, что при загрузке только части 
правил из всего логически связанного набора, фаервол может работать неполноценно.
Так как правила добавляются по одному, а в промежутках времени между загрузками 
каждого из правила ядро продолжает обрабатывать пакеты согласно частично 
загруженному, но логически неполному набору правил, то фаервол может, например, 
пропускать нежелательный (<i>разрешащие правила уже загружены, 
а запрещающие исключения из этих правил - еще нет</i>) или не пропускать разрешённый трафик 
(<i>удалили все правила в цепочке/таблице для полного их перестроения согласно новой конфигурации</i>).
</p>

<p>
Чтобы избежать  указанных выше проблем необходимо
для начальной загрузки и обновления правил в NETFILTER использовать вместо iptables команду iptables-restore.
Команда iptables-restore загружает всю группу команд <b>для одной таблицы</b>, переданную ей на стандартный вход,
за один приём (в транзакции).
Поэтому исключаются ситуации, когда только часть правил загружена в таблицу, 
а оставшася часть правил не загружена из-за возникшей в процессе обработки правил 
синтаксической или другой ошибки. Набор правил в котором обнаруженая та или иная ошибка
просто не загружается в соответсвующую таблицу и имеющийся в ядре набор правил остаётся нетронутым.
</p>

<div class="excl">
<b>Однако надо не забывать, что iptables-restore атомарно загружает правила только в одну таблицу!</b><br>
В NETFILTER четыре таблицы: <b>raw</b>, <b>filter</b>, <b>mangle</b>, <b>nat</b> и все они загружаются независимо друг от друга.
Возникновение ошибки при загрузке в одну из них не вызывает отката изменений в остальных. 
</div>

<p>
Синтаксис команд, которые принимает на вход iptables-restore слегка отличается от
синтаксиса iptables.
</p>

<p>
Структура входных данных для iptables-restore:
<pre class="cmd">
*таблица #1
команда #1
...
команда #N
COMMIT
*таблица #2
команда #2
...
команда #N
COMMIT
EOF
</pre>
Сначала указывается таблица, затем перечисляются правила, вносимые в эту таблицу 
и завершается блок командой COMMIT.
</p>

<p>
Команда создания новых цепочек выглядит так: 
<pre class="cmd">
:имя_цепочки - [кол-во пакетов:кол-во байт]
</pre>

Остальные команды (добавление, удаление, вставка и т.п.) идентичны по синтаксису 
параметрам команды iptables.
Т.о. файл с командами для iptables-restore может выглядеть так:
<pre class="cmd">
*filter
:DMZ_INET - [0:0]
-A DMZ_INET -m state --state RELATED,ESTABLISHED -j ACCEPT
-A DMZ_INET -p tcp --dport 80 -m state --state NEW -j ACCEPT
COMMIT
*nat
:LAN_INET - [0:0]
-A LAN_INET -o eth1 -j SNAT --to-source 44.44.44.44
COMMIT

</pre>
</p>

<p>
По умолчанию iptables-restore полностью очищает указанные таблицы от имеющихся 
там правил, однако если нам надо внести изменения, не очищая существующие в ядре 
правила, то надо указать ключ <i>--noflush</i>.
</p>

<br>
<br>
<h2 id="nftables"><b>nftables</b>: преемник iptables и будущее Linux firewall</h2>
<p>
<b>nftables</b> - написанная "с нуля" система управления фильтрацией и классификацией трафика, призванная заменить iptables.
Изначально nftables разрабатывал Patrick McHardy, сопровождающий подситему NETFILTER.
Презентация проекта (включая реализацию подсистемы ядра) состоялась на Netfilter Workshop в сентябре 2008 года.
18 марта 2009 Patrick анонсировал первый публичный релиз nftables
(<a href="http://marc.info/?l=linux-netdev&m=123735060618579">http://marc.info/?l=linux-netdev&m=123735060618579</a>),
содержащий реализацию пространства ядра (kernelspace) и утилиту пространства пользователя для администрирования.
Официальная страница проекта
<a href="http://netfilter.org/projects/nftables/index.html">http://netfilter.org/projects/nftables/index.html</a>
По логу git'а (<a href="http://git.netfilter.org/nftables">http://git.netfilter.org/nftables</a>)
можно судить о том, что релизов не было уже 4 года, но работа над проектом продолжается.
На данный момент ведущим разработчиком, продолжившим развитие nftables является Pablo Neira Ayuso.
Уже начата интеграция с кодом ядра в отдельном репозитарии (см. 
<a href="http://git.kernel.org/cgit/linux/kernel/git/pablo/nftables.git">
http://git.kernel.org/cgit/linux/kernel/git/pablo/nftables.git</a>).
 <a href="http://lwn.net/Articles/531752/">
Xtables2 vs. nftables</a>
<br><a href="http://kernelnewbies.org/Linux_3.13">http://kernelnewbies.org/Linux_3.13</a>
</p>

<p>
При разработке nftables учтены недостатки iptables.
Например, "одно правило - одна цель", когда для логирования (-j LOG) и последующего 
игнорирования (-j DROP) пакета надо создавать два отдельных идентичных правила.
При добавлении новых расширений в iptables их надо реализовать и в userspace и в коде ядра. 
Код ядра, отвечающий за фильтрацию содержит много дублирущегося функционала.  
Неэффективность инкрементальных изменений: iptables загружает из ядра весь набор правил
в userspace, модифицирует его, и затем загружает весь набор обратно в ядро. <b>Это происходит 
при каждом вызове iptables</b>: в скрипте с 300-ми вызовами команды iptables описаная операция
будет выполнена 300 раз, при этом с каждым разом будет расти количество информации, 
передаваемое между ядром и утилитой iptables. 
Для решения этих и множества других проблем nftables имеет следующую архитектуру.
</p>

<p>
В<b> ядре реализована простая виртуальная машина (интерпретатор)</b>, которая 
имеет инструкции для загрузки данных из пакета, поддерживает выражения для сравнения различных частей 
пакета (по смещениям) с чем-либо, инструкции отслеживания состояний (limit, quota, ...) и т.д.
Это освобождает от "зашитой" в код ядра информации о протоколах и о том как их анализировать:
мы просто программируем ВМ ядра на выполнение тех или иных программ по обработке пакетов прямо
из userspace. Подобным же образом в BSD-системах давно и успешно реализован <b>BSD Packet Filter</b>.
Это позволит, не затрагивая ядро Linux, разрабатывать новые (исправлять существующие) 
правила обработки для новых протоколов, что очень сильно упрощает процесс и избавляет от дублирования кода в kernelspace и userspace.
</p>
<p>
<b>nftables поддерживает такие структуры данных как множества и словари</b>, что <b>позволяет в одном правиле
выполнять различные проверки по спискам и диапазонам значений</b> (аналогично ipset, но с любыми данными).  
Код ядра полностью свободен от блокировок (locks). Счетчики пакетов и байтов, в 
отличие от iptables, опциональны, т.е. их нет по умолчанию в каждом правиле, их надо создавать явно.
Это положительно влияет на производительность. Нет встроенных таблиц - все таблицы надо явно объявлять.
Ядро выполняет минимальные проверки и не проверяет (смысловую) корректность загружаемых 
в ВМ программ, лишь бы программа не повредила ядро: все синтаксические и семантические 
проверки дожны проводиться утилитами userspace. 
</p>

<p>
Соответственно, вместо последовательности команд iptables с ключами, для nftables существует 
полноценный высокоуровневый язык, который компилируется в низкуровневые инструкции для ВМ ядра.
С помощью userspace утилиты nft можно выполнять как отдельные команды этого языка,
для инкрементальных изменений, так и целые программы, написанные на этом языке.
Т.о. сложной последовательности команд iptables по созданию цепочек и правил в этих цепочках,
в nftables будет соответствовать программа состоящая из одного или нескольких файлов.
Утилита nft проверит синтаксис этой программы, сообщит об ошибках, откомплирует её в инструкции
ВМ и загрузит откомпилированую программу в ВМ ядра, используя протокол netlink. 
</p>

<div class="excl">
Т.о. образом, если и когда проект созреет и будет включен в ядро, в Linux появится гибкая и мощная
система фильтрации. Она во многом перекроет возможности, предоставляемые <b>ipset</b> и 
такими надстройками над <b>iptables</b> как <b>shorewall</b>, <b>ferm</b>, <b>ufw</b>, <b>uif</b> и т.п.
</div>

<br><h1 id="part2">Часть 2. Реализация</h1>
<p>
Эта часть посвящена описанию реализации фаервола, написанного с учетом принципов 
и требований первой части. Естественно рассматриваемый вариант является лишь одним 
из возможных подходов к решению этой задачи со своими преимуществами и недостатками.
</p>

<p>
Описанный вариант фаервола можно использовать непосредственно, без переделок, либо
скорректировать его под свои нужды (в первую очередь по кол-ву интерфейсов на шлюзе и используемых сетей).
</p>

<p>
При реализации фаервола возникли вопросы вроде:
<ul>
<li>на каком языке писать?</li>
<li>как реализовать настройку (формат, назначение, кол-во файлов)?</li>
<li>как структурировать сам код и др. файлы?</li>
</ul>
</p>

<p>
Будет рассмотрено как эти вопросы были решены в данной реализации.
</p>

<br><h2 id="part2_changes">Изменения относительно предыдудщего варианта</h2>
<p>
Добавлены: возможность использования ipset, задание значений правил по умолчаниию.
<br>
В процессе добавления поддержки ipset выяснилось, что формат файла <i>hosts.conf</i>
плохо приспособлен для реализации данной возможности и что логика описания
правил не обладала эээ... ясностью в целом.
Поэтому был переработан формат файла <i>hosts.conf</i> и практически полностью переписан модуль <b>firewall.pm</b>. 
</p>

<p>
В предыдущем варианте логика описания правил в <i>hosts.conf</i> в кратце сводилась к 
приниципу "один ко многим": подразумевалось, что основная единица cети - хост и 
для каждого IP-адреса данного хоста перечисляются наборы различных правил.
Т.о. правила в конфигурационном файле группировались по IP-адресам хоста:
<pre class="cmd">
{name => hostname, ipv4 => host_default_ip, ...
  [
    {srcip/dstip => host_ip0, ...  # <--- хэш хоста 
      deny => [dst_ip1,dst_ip2,...],
      allow => [dst_ip1,dst_ip2,...]
    },
    {srcip/dstip => host_ip1, ...
      deny => [dst_ip1,dst_ip2,...],
      allow => [dst_ip1,dst_ip2,...]
    }
  ]
}
</pre>
Служило это для "визуальной привязки" правил к хосту и для сокращения записи. 
Ключ ipv4 был введён позже, для ситуаций, когда хост имеет один IP и для него 
надо прописать входящие/исходящие разрешения для сети.
В этом случае удобно не повторять IP в каждом наборе правил, а сделать его свойством хоста,
а не конкретного набора правил. 
При добавлении поддержки ipset выяснилось, что данный вариант синтаксиса плохо подходит,
т.к. хотелось иметь возможность задавать правила iptables и описывать наборы ipset 
с помощью единого синтаксиса. А со старым синтаксисом задавать наборы ipset 
пришлось бы примерно следующим образом:  
<pre class="cmd">
host {
  host_ip0 {
    ...
    ipset => "ipset_name"
  },
  host_ip1 {
    ...
    ipset => "ipset_name"
  }
}
</pre>
хотя здесь как раз удобнее было бы вот так:
<pre class="cmd">
host {
   ipset_name {
    ip => "host_ip0",
    ip => "host_ip1"
  }
}
</pre>
</p>
<p>
Следующая проблема, связанная с ipset, это собственно описание match-правила.
Т.к. уже присутсвуют src, dst, ipv4 и  пришлось бы добавлять еще один ключ для ipset,
то вместе с обязательным заданием имен сетей "откуда-куда" это не 
добавило бы этой конструкции ясности. Кроме того, отсутствовал системный подход
в том, что выносить в "наружные" хэши, для сокращения записи.   
</p>

<p>
Т.о., в идеале, было бы удобно иметь возможность произвольно группировать
и использовать произвольную явную вложенность различных частей описания, 
но был выбран другой подход. 
</p>

<p>
Возникла мысль "вернуться к истокам" - к синтаксису без явных группировок
и вложенностей - аналогично синтаксису iptables: в одном правиле описаны все его элементы,
без их выноса во внешние объекты.  
Необходимость же указывать в каждом правиле большое количество повторяющейся информации
решено было сгладить макроподстановкой на уровне элементов хэшей. Макроподстановка же
позволяет как-то визуально сымитировать группировку правил произвольным образом.  
</p>
<p>
Кроме того, анализ синтаксиса nftables, показал что в нем используется тот же 
подход, что и в iptables: линейный набор правил.
Так что, новый подход к организации правил в файле hosts.conf
обещает быть более универсальным.
</p>

<br><h2 id="shorewall">Чем не устроил Shorewall</h2>
<p>
На момент проб Shorewall был 3-ей версии.
Пробовал я его на "Web server #1" в DMZ.
В частности необходимо было задать различные наборы правил
для клиентов из сети INET и cети LAN2, подключенных через <i>eth3</i>. 
Из INET надо было ничего не разрешать, а из LAN2 дать доступ к DMZ. 
Но я так и не смог настроить Shorewall
так, чтобы он "понимал" что за интерфейсом <i>eth3</i> у меня две сети, для
каждой из которых я хочу настроить разные правила. На все мои варианты Shorewall 
сообщал об ошибке в конфигурации: что именно было уже не помню - 
что-то про нестыковку в сетях (файл hosts) и интерфейсах (файл interfaces).
Т.е. добиться того что мне надо от Shorewall я так и не смог, и чем мучаться
и догадываться что там не так я вручную написал эти несчастные несколько нужных мне правил.
</p>
<p>
Также мне не понравился формат конфигурационных файлов: в частности большой 
объем правил в rules читается с трудом. И в файл не предусматривает встраивания 
метаданных об хостах сети для последующего из использования в отчетах.
Можно было метаданные вставлять в виде комментариев, но мне этот вариант не понравился. 
</p>

<p>
Потом я поверхностно посмотрел, какие правила и цепочки генерировал Shorewall, в надежде
понять как же добиться от него того, что мне надо, однако никакой системы на тот 
моментя там не уловил - все правила "валились в кучу": в INPUT, FORWARD, OUTPUT. 
Помню была некая специальная цепочка "all2all" и всё.
</p>

<p>
Вообщем смысл вышексказанного сводится к тому, что написать нексолько правил для 
той задачи было гораздо проще, чем тратить время на разбор того, почему у меня не 
получается добиться того же результата от Shorewall и я плюнул на него, в том числе чтобы 
знание устройства Shorewall не повлияло на то, как я буду делать свой велосипед.
</p>

<p>
На 2013 год Shorewall имеет 4 версию, которая переписана на Perl вместо Bash,
но синтаксис конфигурационных файлов остался прежним. Интересно, изменилось ли 
что-то в логике построения цепочек и правил.  
</p>

<br><h2 id="history">Предыдущие варианты реализации или "как не надо делать"</h2>
<p>
В своё время, я не знал о существовании, например, shorewall и поэтому 
"изобретал велосипед" заново, не зная на каких принципах построены другие firewall'ы.
Позже я попробовал использовать shorewall 3-ей версии на веб-сервере в DMZ, но так и не добился
того, что мне было нужно и не стал разбираться - возможно ли это в приниципе. Поэтому
изобретение велосипеда продолжилось. 
</p>

<p>
Ниже описаны варианты фаервола, предшествововашие рассматриваемому варианту.
Это сделано с целью поделиться опытом и показать "как не надо делать".
Условно, рассматриваемый в данном тексте вариант фаервола можно считать "четвертой версией".
А вот предыдущие три:
</p>

<p>
<b>"Версия №1"</b> - это просто bash-скрипт, содержащий линейную 
последовательность команд <i>iptables...</i>, выполнение которых и конфигурировало NETFILTER.
Скрипт не имел настроек или параметров и для вненсения изменений, надо было исправлять 
непосредственно сам скрипт. По мере добавления новых интерфейсов и расширения 
функционала, рос и объем скрипта. Скрипт становился плохо управляемым и нечитаемым.
Управления трафиком не было в принципе - вообще непонятно было как это делается.
Назрела переделка.   
</p>

<p>
<b>"Версия №2"</b>. Первоначальный bash-скрипт был переписан: за конфигурацию 
правил каждого из потоков теперь отвечала отдельная функция в этом скрипте.
Имелся файл настройки: 
отдельный bash-скрипт, в котором были описаны одномерные массивы в которых 
указывались списки (через пробелы) разрешённых IP- адресов и/или портов.
Массивов было много - для каждого из потоков и для каждого типа задач 
(глобальные/индивидульные/адреса/порты/протоколы разрешения, запреты, исключения) приходилось заводить
свой массив. Индексами в этих массивах были последние октеты IP-адреса хостов
внутренней сети (напр. LAN_IP_INET_IP_ALLOW[131]="0.0.0.0/0" описывал разрешение 
хосту 192.168.0.131 на выход в Интернет).
В рабочем скрипте использовалось множество однотипных циклов по этим 
массивам: так строились наборы правил для каждого из потоков.
В скрипте, в достаточном количестве, присутствовали "ручные вставки кода" для 
"для решения особых случаев".
Такая система была неуниверсальна, привязана к сети класса С и громоздка
(для каждой из сетей приходилось заводить свои наборы массивов).
</p>
<p>
На этом этапе возникла необходимость в управлении трафиком и отчетности.
Т.е. в конфигурации надо было увязать IP-адреса и некие метаданные (ФИО сотрудников, 
названия серверов) и как-то интегрировать это с управлением трафиком.
Вопрос интеграции метаданных был решен примитивно - так как срипт отчетности был 
написан на perl'е, то для него просто вёлся отдельный perl-скрипт  c хэшем IP-адресов,
в котором хранились нужные данные. Ясно что возникло дублирование данных (IP-адресов)
в двух файлах: в конфигурационном bash-скрипте, предназначенном для фаервола и 
в конфигурационном perl-срипте предназначенном для отчетности.
</p>
<p>
Управление трафиком было реализовано также примитивно: отдельный фиксированный 
bash-скрипт, в котором указывалось download-скорость из Интернета в LAN.
Для ppp-клиентов был отдельный скрипт. Дублирование конфигурационной информации нарастало.  
Позже эта версия была слегка модифицирована: один единый файл конфигурации (bash-скрипт)
был разбит на отдельные файлы (по кол-ву потоков), в каждом из которых содержались 
настройки, относящиеся только к данномоу потоку. Единый основной скрипт также был
"расчленён" по тому же принципу - "каждому потоку - свой скрипт", в каждом файле 
две функции: для правил трафика "туда" и для правил "в обратном направлении".
В результате получилась довольно негибкая "смесь" из различных подходов, которая 
не обладала нужной "полноценностью" (управление трафиком, ау!) и снова была слабоуправляемой.
Необходимо было превратить эту "кашу" в нечто более стройное и единообразное.
<b>Основные направления: унификация формата настройки, его расширямость, 
читаемость и некая универсальность. И как следствие - неизбежный уход от bash, 
в сторону языка, с поддержкой болеес сложных структур данных.</b> 
</p>

<p>
<b>"Версия №3"</b>. В этой версии файлы настроек были уже написаны на perl как и 
сам firewall. Однако ж снова, на каждую пару потоков было по своему собственному 
скрипту и для каждого потока был свой настроечный файл. Предполагалось, что при 
необходимости, нужные скрипты и конфигурационные файлы будут дописаны, используя 
существующие, как шаблоны.    
</p>
<p>
Однако с конфигурационными файлами возникла следущая проблема - их было два вида: 
один для описания потоков типа "шлюз-сеть"и другой для потоков тип "сеть-сеть".
И кроме того, был конфигурационный файл, описывающий хосты (фактически пользователей) сети LAN,
со своими особенностями (метаданными).
А хотелось иметь один универсальный "формат". Остановлюсь на этом моменте подробнее.
<span style="color:red">Кроме того, "специализированные" скрипты для каждого из потоков сами по себе плохое решение.</span> 
</p>

<p>
<b>Что должны описывать конфигурационные файлы.</b>
<p>
Поэтому встал вопрос о том, как организовать информацию в конфигурационных файлах.
Здесь усматривается два типа информации, которую надо хранить:
<ul>
<li>информация непосредственно об хостах сетей: адреса, имена, скорости и прочая вспомогательная информация, характеризующая данный узел;</li>
и
<li>информация о связях между хостами какой-либо пары сетей: какой между ними трафик разрешён, запрещен и т.п. (т.е.
правила, говорящие что может передавать один узел другому)</li>
</ul>    


</p>
<p>
<center><img src="img/config_hosts_links.png"/></center>
<br>
<center>Рис. 17 Конфигурационные файлы</center>
</p>

<p>
Следующий вопрос: в какое количество файлов и каким именно образом сгруппировать данную информацию?
Варианты:
<ul>
<li><b>два типа конфигурационных файлов</b>: один тип описывает непосредственно хосты некоей сети и их метаданные, а другой - 
	только "связи" между ними хостами этих сетей ("что разрешено или запрещено"); тогда, например для двух сетей 
	небохидимы три файла: первый описывает хосты одной сети, второй описывает хосты другой сети и третий файл описывает связи между ними;
	общее кол-во файлов равно сумме числа всех хостов и числа всех связей</li>
<li><b>один тип конфигурационных файлов, описывающий "связи"</b> между хостами двух произвольных сетей; 
при этом информация о самих хостах хранится в описании самой "связи"; в данном варианте 
общее кол-во файлов равно сумме числа всех свзяей между хостами</li>
<li><b>один тип конфигурационных файлов, совмещающий описание хостов и связей</b> в 
виде некоей иерархической структуры;</li>
</ul>    
</p>

<p>
У первого варианта два существенных недостатка: необходимость в двух форматах 
файлов конфигурации и неудобство управления двумя видами файлов.
При изменениях конфигурации надо будет вручную сверять эти файлы на наличие
несоответствий между наличием (отсутствием) связей в одном файле и наличием (отсутствием)
описания соответсвующих этим связям хостов в другом файле.
</p>

<p>
Недостаток второго подхода - дублирование информации об хостах: так как нет 
специального файла, в котором один раз были бы описаны хосты, то эту 
информацию надо будет в описании каждой "связи" между разными сетями, даже если 
с одной стороны связи один и тот же узел. 
</p>

<p>
Третий вариант хорош, но возникает вопрос: неясно, в каком из двух файлов, описывающий хосты, 
размещать информацию о связях для каждой пары сетей? Решение оказалось простым: 
правила для трафика надо размещать в том файле, где это удобно и наглядно.
Настройка должна позволять размещать правила произвольно в произвольном количестве файлов:
для пары сетей можно разместить одну часть правил в одном файле, а остальные правила - в другом,
а можно все правила сгруппировать в один файл. 
</p>

<p>
Т.о. версия №3 фаервола просуществовала недолго и оказалась опытной площадкой для перехода
на следующую, четвертую версию с унифицированным форматом конфигурационных файлов, который и рассматривается далее.
</p>

<br><h2 id="sources">Исходные тексты фаервола</h2>
<p>
<a href="hlf.tar.gz">Исходный код hlf.tar.gz</a>
</p>

<br><h2 id="why_perl">Почему perl, а не bash, python и т.д.?</h2>
<p>
Для написания фаервола рассматривались языки <b>bash</b>, <b>perl</b> и <b>python</b>.  
Для описания конфигурации и собственно кодинга выбран язык perl и вот почему.
</p>

<dl>
<dt><b>bash</b></dt>
<dd>
В bash 3.x нет сложных структур данных, а именно хэшей (словарей, ассоциативных массивов).
И хотя в bash 4.x словари появились, но этого недостаточно, потому как невозможно 
создавать вложенные струтктуры данных: многомерные массивы, массивы хэшей, хэши массивов и т.п. 
</dd>

<dt><b>python</b></dt>
<dd>
Python не является обязательным для Linux и не устанавливается по умолчанию (как минимум в Debian без GUI).
Использование Python приводит к зависимости фаервола от необязательного компонента системы и 
необходимости устанавливать Python, потенциально, только ради работоспособности фаервола.
</dd>

<dt><b>perl</b></dt>
<dd>
Является неотьемлемой частью любого (серверного) дистрибутива. Мало того, без него 
невозможно скомпилировать Linux систему из исходных текстов (смотри напр. <a target="_blank" href="http://www.linuxfromscratch.org/lfs/view/stable/">Linux From Scratch</a>). 
Т.е. perl обязательно будет в любом десктопном или серверном дистрибутиве по умолчанию.
В perl, в отличии от bash, есть удобные структуры данных и их легко сделать вложенными.
</dd>
</dl>
<p>
Т.о. был выбран perl: за присутствие в системе по умолчанию, наличие удобных 
структур данных и за общую гибкость в решении задач администрирования. 
</p>

<h2 id="structure_db">Почему конфигурация не хранится в СУБД или LDAP</h2>
<p>
При размещении конфигурации в СУБД или LDAP возникает зависимость от наличия 
этих компонентов на том хосте, где планируется использовать фаервол. 
Соответственно их надо установить и сконфигурировать, что может стать источником 
ошибок и дополнительной траты времени.
Так как я стремился к абсолютному минимуму зависимостей и к наглядному, легко редактируемому
формату конфигурации, то для хранения конфигурации были выбраны обычные текстовые файлы. 
Их леко копировать и для внесения изменений ничего, кроме текстового редактора, не требуется.
<b>Это позволяет просто скопировать фаервол на новую машину и не думать о зависимостях,
установке и настройке СУБД и т.п.</b>
</p>

<br><h2 id="structure">Устройство фаервола</h2>
<h3>Общие замечания</h3>
<ol>
<li>
Код и настройки фаервола имеют модульную структуру. Идея заключается в том, чтобы
иметь в наличии некий набор базовых действий (функций), используя которые, 
можно "собирать" более высокоуровневые скрипты, выполняющие уже какие-то конкретные задачи.
Конечно, это далеко не абсолютно универсальные функции, позволяющие строить фаервол
произвольной структуры, а функции, реализующие вполне определенный подход, описанный в первой части.
(С течением времени идея выродилась, так как набора слабосвязанных скриптов сделать 
не получилось и все превратилось в набор функций в модуле <b>firewall.pm</b>) 
</li>
<li>
Код написан, а настройки расположены в файловой системе таким образом,
чтобы легко было иметь несколько различный конфигураций одновременно и переключаться
между ними одной командой. К тому же за счёт того, что есть возможность разбивать
конфигурацию на произвольное кол-во файлов, при продуманном подходе к их написанию, возможно 
их повторное использование в различный конфигурациях. Например, если с 9 до 18 часов,
должны действовать какие-то глобальные запреты, а в другое время эти запреты не действуют, то в 
основной (с 9 до 18) конфигурации можно вычленить в отдельный файл эту запрещающую часть.
Далее создать новую конфигурацию, в которую слинковать часть файлов из основной конфигурации
и убрать запуск обработки конфигурационного файла с запретами.
И, наконец, перезапускать фаервол в нужное время с помощью crontab, указывая нужную конфигурацию.
</li>
<li>
Не все команды для конфигурации фаервола 
создаются автоматически на основании файлов конфигурации. Часть команд содержится
в явном виде непосредственно скриптах.
Это связанно с тем что не для всех случаев легко и очевидно на данный момент
или целесообразно придумывать синтаксис конфигурации и писать её обработку - иногда проще написать несколько 
команд, выполняющих нужные действия.
В рассматриваемом варианте это относится, например, к правилам NAT: их немного, 
меняются они нечасто и "городить" ради них настройку и её обработку просто сложнее
чем написать скрипт с несколькими  командами iptables который и будет запускаться 
в нужный момент. Пометка трафика для целей policy routing также сделана отдельным 
скриптом с нужным набором правил, так как на данный момент для меня неочевидно как
это гибко "вписать" в выбранную концепцию конфигурационных файлов. Но и данный 
вариант более чем жизнеспособен.
</li>
<li>
Фаервол не сохраняет какой-либо метаинформации о своём текущем состоянии на диске или
в памяти. После каждого изменения конфигурации требуется полная обработка конфигурации,
построение структур в памяти и формирование нового набора правил.   
</li>
<li>
Сначала на основании конфигурации генерируется полная последовательность команд iptables и tc,
и только затем эти команды выполняются. При возникновении оишибок во время генерации
команд они просто не выполняются.  
</li>
<li>
Фаервол позволяет описать настройку как шлюза с произвольным кол-вом интерфесов 
и сетей к ним подключенным, так и, например, сервер или рабочую станцию, не выполняющие роль шлюза.
</li>
<li>
Фаервол не проверяет смысловую корректность файлов настройки и конфигурации в целом.
Реализация таких проверок в полном объеме - сложная и трудоёмкая задача и для проекта,
используемого только в личных целях, не приоритетная. 
В данном случае, с помощью возможностей интерпретатора perl, проверяется только 
синтаксическая корректность файлов конфигурации.
По этой же причине выдача различных дигностических сообщений минимальна. 
</li>
<li>
<b>По умолчанию для каждой связи трафик потоков идущих в одном направлении, 
проверяется по одной и той же пользовательской цепочке.
Например потоки INET>eth1-eth0>LAN, INET>eth3-eth0>LAN, INET>eth1-pppX>LAN, INET>eth3-pppX>LAN
по умолчанию будут проходить проверку по одной и той же цепочке.</b> 
Данное поведение изменяется доп. конфигурированием.
</li>
<li>
Фаервол работает только с IPv4. Для IPv6 необходима соответствующая доработка 
(как минимум, для IPv6 надо использовать ip6tables). 
</li>
</ol>

<br><h3 id="structure_fs">Структура каталогов</h3>
<p>
<b>В приведенных выше исходных файлах примере</b> весь код и настройки фаервола расположены в каталоге /etc/hlf.<br>
<b>Код написан так, что работает по относительным путям и поэтому его можно 
размещать где угодно в файловой системе.</b><br>
Поэтому далее по тексту каталог в котором расположены файлы фаервола будет 
обозначаться как <b>/FW</b>. Если для файлов и каталогов не указан полный путь, 
то подразумевается их размещение относительно каталога <b>/FW</b>.
<br>Итак, в каталоге <b>/FW</b> находятся:
<ul>
<li><b>conf.d</b> - каталог, в котором хранятся конфигурации (каждая конфигурация в своём отдельном подкаталоге);</li>
  <ul>
  <li><b>conf.d/allow_all</b> - каталог с конфигурацией фаервола, которая разрешает прохождение любых пакетов;</li>
  <li><b>conf.d/block_all</b> - каталог с конфигурацией фаервола, которая запрещает прохождение любых пакетов;</li>
  <li><b>conf.d/default</b> - символическая ссылка на каталог с конфигурацией, используемой по умолчанию;</li>
  <li><b>conf.d/main</b> - каталог с основной конфигурацией фаервола, которая рассматривается в данном тексте;</li>
  <li><b>conf.d/outbound_only</b> - каталог с конфигурацией фаервола, которая разрешает 
													 прохождение пакетов только для соединений, установленных
													 самим шлюзом;</li>
  </ul>
<li><b>modules</b> - каталог, в котором размещаются файлы, с кодом, реализующем функционал, 
общий для всех конфигураций (т.е. общие алгоритмы) и другие вспомогательные скрипты:</li>
  <ul>
  <li><b>modules/firewall.pm</b> - модуль Perl с функциями разбора файлов конфигурации и 
												 генерации последовательности команд</li>
  <li><b>modules/fw-sysv</b> - bash скрипт запуска для системы инициализации SysV</li>
  <li><b>modules/init</b> - bash скрипт начальной инициализации фаервола (очистка всех таблиц)</li>
  <li><b>modules/load_modules</b> - bash скрипт с командами загрузки необходимых модулей ядра</li>
  <li><b>modules/save_counters</b> - bash скрипт, являющийся шаблоном, по которому создаётся файл для записи счетчиков iptables в файлы</li>
  <li><b>modules/wait_dns</b> - bash скрипт ожидающий установления связи ADSL-модемом 
											посредством попыток открытия порта на DNS-сервере провайдера;
											необходим для разрешения DNS-имен в правилах iptables, если таковые будут</li>
  </ul>
<li><b>start</b> - головной скрипт запуска фаервола.</li>
</ul> 

<h4 id="structure_multiconfig">Поддержка нескольких конфигураций (<b>/FW</b>/conf.d)</h4>
<p>
В каталоге <b>conf.d</b> может находиться произвольное количество различных конфигураций
фаервола - каждая в своем отдельном подкаталоге. При этом имя подкаталога фактически 
является именем расположенной в нём конфигурации.
Также в <b>conf.d</b> может находиться символическая ссылка <b>conf.d/default</b>,
указывающая на каталог, содержащий конфигурацию фаервола, используемую по умолчанию. 
</p>

<p>
Для запуска фаервола предназначен скрипт <b>/FW/start</b>.
Если в качестве параметра скрипту <b>start</b> передано имя подкаталога 
из каталоге <b>conf.d</b> (напр. outbound_only), то будет запущен скрипт <b>start</b>,
находящийся в указанном каталоге. 
<br>В свою очередь, запускаемый скрипт <i>conf.d/<b>имя_конфигурации</b>/start</i> 
и должен выполнить всю последовательность действий по конфигурированию фаервола для
выбранного варианта конфигурации.
Если скрипту <b>start</b> не передано имя конфигурации, то этот скрипт пытается
запустить скрипт <i>conf.d/<b>default</b>/start</i>, где default может быть как обычным
подкаталогом с конфигурацией по умолчанию, так и символической ссылкой на некий существующий
подкаталог <i>conf.d/<b>имя_конфигурации</b></i>.
</p>

<p>
Данный механизм жёстко задан и не может быть изменён без правки кода в <b>/FW/start</b>.
</p>

<h4 id="structure_modules">Общие модули (/FW/modules)</h4>
<p>
Собственно здесь находятся общие "строительные блоки" - файлы с кодом, которые можно 
использовать в скриптах каждой из конфигураций. 
Основной из них - perl-модуль <b>firewall.pm</b>. 
Именно в нём реализованы почти все функции по конфигурированию фаервола.
Его внутреннее устройство будет рассмотрено ниже.
</p>

<h3 id="structure_cfg">Конфигурационные файлы</h3>
<p>
Как уже говорилось выше, фаервол поддерживает произвольное кол-во независимых 
конфигураций. 
<br><b>Минимальная конфигурация</b> (каталог <i>conf.d/<b>имя_конфигурации</b></i>) <b>включает в себя три элемента</b>:
<ul>
<li><b>start</b> - исполняемый perl-скрипт из которого вызываются функции модуля 
<b>firewall.pm</b> <i>в необходимой вам в данной конфигурации</i> последовательности, 
а так же выполняются любые другие необходимые действия;<br> 
<b>Написание данного срипта является частью процесса конфигурирования!</b></li>
<li><b>один конфигурационный файл</b>, описывающий <b>интерфейсы шлюза</b> 
и <b>сЕти, подключенные к этим интерфейсам непосредственно</b> или 
которые доступны через эти сети; далее по тексту такие файлы будут обозначаться
как <b>networks.conf</b></li>
<li><b>конфигурационный файл (один или несколько)</b>, описывающий <b>метаданные, разрешающие 
и запрещающие правила, скоростные ограничения</b> и т.п. как <i>для сетей</i>, 
перечисленных в <b>networks.conf</b>, так <i>и для отдельных хостов</i> в этих сетях;
далее по тексту такие файлы будут обозначаться как <b>hosts.conf</b>.</li>
</ul>  
</p>

<p>
В конфигурации <b>фиксированное имя имеет только</b> скрипт <b>start</b>.
Так как содержимое скрипта <b>start</b> пишется индивидуально для каждой 
конфигруации, то и для файлов <b>networks.conf</b> и <b>hosts.conf</b> можно 
выбрать любые имена и расположение внутри каталога конфигурации.
Кроме того, не обязательно иметь один файл типа <b>hosts.conf</b> - конфигурацию
правил для хостов/сетей можно разбить произвольным образом на любое кол-во файлов,
естественно с разными именами.  
</p>

<p>
В случае необходимости используются как конфигурационные файлы, 
так и вспомогательные скрипты. Т.е. конфигруация может придставлять собой произвольный
набор скриптов и конфигурационных файлов определенной структуры. Скрипты, в определенной 
последовательности, запускают  какие-либо команды ОС, вызывают вспомогательные функции из общих 
модулей <b>conf.d/modules</b>, которые в свою очередь разбирают файлы конфигурации
и формируют на их основе последовательность команд iptables, tc и т.п. для создания нужных
цепочек, правил, задания правил управления трафиком и т.д.
</p>

<h4 id="structure_cfg_net">Файл описания шлюза и сетей (аkа networks.conf)</h4>
<p>
Назначение данного файла - описать интерфейсы шлюза и сЕти, подключенные к этим интерфейсам 
непосредственно или которые доступны через эти сети.
<b>В конфигурации этот файл должен быть в единственном экземпляре</b>.
На основании этого файла определяются все возможные соединения между каждой парой сетей,  
между шлюзом и сетями; для соединений определяются прямые и обратные потоки, именуются цепочки
и строится общий "каркас" цепочек NETFILTER (т.е. набор цепочек и переходов между ними).
Также, на основании информации из этого файла строятся "головные" правила для цепочек
INPUT, OUTPUT, FORWARD, с помощью которых, на основании входящих и исходящих интерфейсов и сетей,
трафик относится к тому или иному потоку.
</p>
<p>
С точки зрения синтаксиса данный файл представляет собой Perl-скрипт,
настройки в котором описаны в виде хэша <i>%networks</i>.
Использование конфигурационных файлов, которые по сути являются фрагментами 
исполняемого языка программирования имеет недостатки:
<ul>
<li>так как файл конфигурации фактически исполняется интерпретатором (в данном случае Perl), 
да еще с правами root, то в файл конфигурации можно случайно или намеренно вставить код, выполнение которого 
будет будет иметь побочные, возможно даже деструктивные, последствия;</li>
<li>синтаксис файлов не является нейтральным по отношению к языку, на котором написан фаервол:
при смене языка придется переписывать файлы конфигурации согласно новому синтаксису;</li>   
</ul>
Плюс один - простота реализации и удобство внесения изменений: без всякого 
нудного синтаксического разбора файлов и без зависимостей от сторонних модулей мы сразу
в коде имеем нужные структуры данных. Это крайне удобно, пока вам до конца не 
ясен оптимальный формат файла и набор параметров в нём. 
</p>

<div class="excl">
В данном варианте подразумевается, что администратор шлюза (root):
<ul>
<li>владелец каталога /FW и всего его содержимого;</li> 
<li>только root может изменять и запускать файлы в /FW;</li> 
<li>и что он достаточно разумен, чтобы не вписать в файлы конфигурации что-то 
вроде `unlink '/etc/passwd'` или `system('rm -R /')`.</li> 
</ul>
</div>

<br>
<p id="structure_cfg_net_syntax">
<b>Далее описан синтаксис файла описания шлюза и сетей.</b><br>
В качестве примера смотри файл <b>/etc/hlf/conf.d/main/conf/networks.conf</b> в исходниках.
</p>

<p>
Файл должен иметь синтаксис perl-скрипта.
В файле должен быть один раз описан хэш с зарезервированным именем <i>%networks</i>.
Каждый элемент хэша <i>%networks</i> описывает одну сеть.
Ключ в <i>%networks</i> является именем сети и на него возможны ссылки 
из этого и других конфигурационных файлов. Имя сети выбирается произвольно и должно быть уникальным. 
В <i>%networks</i> должны быть описаны все сети, трафик которых проходит через шлюз
и трафиком которых вы хотите управлять.
<b>Сам шлюз синтаксически трактуется как сеть и должен быть описан в данном файле.</b>
</p>

<pre class="cmd">
%networks = (
    сеть№1 => описание_сети,
    сеть№2 => описание_сети,
    сеть№3 => описание_сети,
    ...
    сеть№N => описание_сети,
}
</pre>

<p>
<b>Описание_сети</b> представляет собой хэш из четырех элементов со следующими ключами:
<ul>
<li><b>name</b> - наименование сети - используется при генерации имён цепочек; обязательный параметр;</li>
<li><b>type</b> - задает тип сети. Обязательный параметр. Возможные значения: 
<ul>
<li><b>host</b>: служит для указания, что данное <b>описание_сети</b> описывает сам шлюз. <b>Должно втречаться в файле только единожды!</b></li>
<li><b>net</b>: служит для указания, что данное <b>описание_сети</b> описывает некую сеть;</li>
</ul>
</li>
<li><b>ifaces</b> - для <b>type => 'host'</b> описывает интерфейсы с их адресами.<br>
Для <b>type => 'net'</b> описывает адреса сетей (т.е. можно несколько IP-сетей трактовать как одну) и интерфейсы шлюза,
через которые эти сети доступны непосредственно или косвенно. Обязательный параметр;</li>
<li><b>flows_options</b> - описывает различные параметры потоков
между данной сетью и остальными сетями. Необязательный параметр.</li>
</ul>
</p>

<pre class="cmd">
%networks = (
    сеть№1 => {
        name => наименование_сети, type => тип_сети,
        ifaces => описание_интерфейсов,
        flows_options => опции_потоков
    }
)
</pre>

<p>
<b>Описание_интерфейсов</b> представляет собой хэш в котором каждый элемент описывает
один сетевой интерфейс. Ключом хэша является имя интерфейса в формате команд iptables. 
В описании шлюза (тип "host") в <b>ifaces</b> должны быть перечислены сетевые интерфейсы шлюза.
В описании сети (тип "net") должны быть перечислены сетевые интерфейсы, 
через который данная сеть доступна непосредственно или далее через маршрутизаторы (next hops).
При использовании алиасов (интерфейс с несколькими ip-адресами) алиас описывается
как отдельный интерфейс.
</p>

<pre class="cmd">
    ifaces => {
        'eth0'   => параметры_интерфейса,
        'eth0:0' => параметры_интерфейса,
        'eth1'   => параметры_интерфейса,
        'ppp+'   => параметры_интерфейса,
        ...
    },
</pre>

<p>
<b>Параметры_интерфейса</b> представляют собой хэш из элементов со следующими ключами.
<ul>
<li><b>addr</b> - для шлюза задает IPv4-адрес(а) интерфейса. Для сети задает IPv4-адрес(а) сети.
Указание "!" в начале имени означает логическое отрицание аналогично iptables.
Несколько адресов перечисляются через запятую и заключаются в квдратные скобки. 
При указании более одного адреса используется штатная возможность iptables по генерации
нужного кол-ва правил;</li>  
<li><b>ipset</b> - задает имя набора ipset, который описывает <b>адреса сетей</b>, 
доступных через данный интерфейс.
Указание "!" в начале имени означает логическое отрицание аналогично iptables;</li>
<li><b>tc_iface</b> - имя IFB-интерфейса, на который будет перенаправлен трафик 
данного интерфейса. Собственно перенаправление должно осуществляться дополнительно
утилитой tc в пользовательском скрипте;
<li><b>tc_root_handle</b> - номер major для дисциплины данного интерфейса -
имеет смысл только в описании шлюза;</li>
<li><b>qd_start_minor</b> - номер minor с которого начнется нумерация 
автоматически создаваемых классов для дисциплины данного интерфейса.
Имеет смысл только в описании шлюза</li>
</ul>
Обязателен только параметр <b>addr</b>.
</p>

Пример для шлюза:
<pre class="cmd">
gw => {
    name => 'GW', type => 'host',
    ifaces => {
        'eth0'   => {addr => '192.168.0.1', tc_iface => 'ifb0', tc_root_handle => '0'},
        'eth0:0' => {addr => '192.168.4.1'},
        'eth1'   => {addr => <b>['203.0.113.2','192.168.46.1']</b>},
        'ppp+'   => {addr => '192.168.3.1', tc_iface => 'ifb0', tc_root_handle => '3', qd_start_minor => '30'}
    }
},
lan => {
    name   => 'LAN', type => 'net',
    ifaces => {
        'eth0'   => {addr => '192.168.0.0/24'}
        'ppp+'   => {addr => '192.168.1.0/24'}
    }
},
inet => {
    name   => 'INET', type => 'net',
    ifaces => {
        'eth6' => {ipset => '! private_net'},
        'eth7' => {ipset => '! private_net'}    
}
</pre>
Пример для хоста в сети DMZ:
<pre class="cmd">
# у dmz-хоста все сети доступны через один и тот же интерфейс
dmz-host => {
    name => 'dmz', type => 'host',
    ifaces => {
        'eth0' => {addr => '203.0.113.1'}
    }
},
lan => {
    name   => 'LAN', type => 'net',
    ifaces => {
        'eth0' => {addr => <b>['192.168.0.0/24','192.168.1.0/24']</b>}
    }
},
</pre>

<div class="excl">
Как видно из примера, для ppp-туннеля адрес интерфейса не обязан совпадать с адресом
сети - поэтому адрес сети не вычисляется на основании адреса интерфейса,
а задается явно. На факт "подключения" сети <b>lan</b> к шлюзу <b>gw</b>
указывает <b>только</b> наличие "общего" интерфейса <b>ppp+</b>. Т.е., фактически, описывается
через какие интерфейсы шлюза доступны те или иные сети.  
</div>

<p>
<b>Опции_потоков</b> (ключ <b>flows_options</b>) представляет собой хэш в котором 
каждый элемент описывает параметры одного потока или группы потоков между 
данной сетью и другими сетями.
Понятие потока смотрите в разделе
<a href="#conception">Основные концепции: потоки, линки, цепочки</a>, в частности
<b>рис. 3</b>.
</p>

<pre class="cmd">
сеть№1 => {
    ifaces => {
        ...
        flows_options => {
            'сеть№2'           => параметры_потока,
            'сеть№3/ppp+/tun+' => параметры_потока,
            'сеть№4'           => параметры_потока,
            'сеть№4//eth7'     => параметры_потока,
            'сеть№5/ppp+'      => параметры_потока,
        }
    }
}
</pre>

<p>
Так как правила и имена цепочек генерируются автоматически, то необходимо иметь
некий механизм управления параметрами этой генерации - <b>опции_потоков</b>.
<br>Основное использование опций потоков:
<ul>
<li>влияние на порядок головных правил в цепочках <b>INPUT, FORWARD, OUTPUT</b>;</li>
<li>явный запрет на генерацию правил для потока(ов);</li>
<li>автоматическая генерация правил-счетчиков на основе 
сгенерированных разрешающих правил;</li>
<li>создание в таблицах <i>filter</i> и <i>mangle</i> отдельных цепочек для определённого 
потока или группы потоков. <b>Данная возможность необходима</b> в связи с тем, что
<b>по умолчанию <i>трафик всех потоков между двумя сетями</i> "группируется"</b> - 
его проверка происходит по одним и тем же правилам в одной сгенерированной
для этой цели цепочке потока.</li>
</ul>
</p>

<p>
Ключ в опции потока идентифицирует поток или группу потоков согласно следующему синтаксису:<br>
<b>сеть_назначения/входящий_интерфейс/исходящий_интерфейс</b>.
Указание <b>сети_назначения</b> обязательно.
Отсутствие в ключе интерфейса означает "любой интерфейс" из тех, что "связывает" две сети. 
</p>

<p>
Пример для потоков INET->LAN на рис. 3. Их будет четыре: INET>eth1-eth0>LAN,
INET>eth1-ppp+>LAN, INET>eth7-eth0>LAN, INET>eth1-ppp+>LAN.
Смотрим, как можно указать эти потоки:
</p>

<pre class="cmd">
inet => {
    name = "INET",
    ...
    ifaces => {
        ...
        flows_options => {
(1)         'lan'              => параметры_потока,
(2)         'lan/eth1'         => параметры_потока,
(3)         'lan//ppp+'        => параметры_потока,
(4)         'lan/eth7/eth0'    => параметры_потока,
        }
    }
}
</pre>
<p>
Вариант (1) задаст параметры для всех четырёх потоков.
Вариант (2) задаст параметры для двух из четырёх потоков: INET><b>eth1</b>-eth0>LAN,
INET><b>eth1</b>-ppp+>LAN.
Вариант (3) задаст параметры для других двух потоков, у которых выходной интерфейс <b>ppp+</b>:
INET>eth1-<b>ppp+</b>>LAN, INET>eth7-<b>ppp+</b>>LAN.
Вариант (4) задаст параметры для только для одного потока:
INET><b>eth7</b>-<b>eth0</b>>LAN.
</p>


<p>
<b>Параметры_потока</b> представляют собой хэш из элементов со следующими возможными ключами:
<ul>
<li><b>order</b> - строка, добавляемая в начало имени цепочки, генерируемой для 
данного потока(ов). Используется в случае необходимости задать порядок правил, 
генерирумых для цепочек INPUT, FORWARD, OUTPUT. Правила будут созданы в алфавитном порядке имен цепочек;</li>
<li><b>ban</b> - явный запрет на генерацию правил потока. Допустимы два значения:
<ul>
<li><b>flow</b> - запрещает генерацию любых цепочек и правил для данного потока(ов).
Пример. Между сетями VPN и LAN связь возможна через два LAN-интерфейса: eth0 и ppp+.
Известно что трафик между VPN и LAN через ppp+ не нужен. Т.е. необходимо полностью
запретить потоки VPN>tun1-ppp+>LAN и LAN>ppp+-tun1>VPN; 
</li>
<li><b>rules</b> - разрешает генерацию цепочки потока и stateful-правил RELATED,ESTABLISHED,
но запрещает генерацию любых пользовательских правил для данного потока(ов);
Пример. Новые соединения между сетями DMZ и LAN должны быть возможны только в
одном направлении: из сети LAN в DMZ, но не наоборот. 
Если для потоков DMZ->LAN указать  ban => 'flow',
то любые правила для этих потоков будут проигнорированы, что застрахует
от случайного разрешения новых соединений из DMZ в LAN;  
</li>
</ul> 
<li><b>filter_chain</b> - задает имя цепочки в таблице filter, которое будет 
использовано при генерации правила для данного потока(ов). В частности, 
таким образом можно задать разный набор правил фильтрации для потоков, 
которые по умолчанию проверяются по одной и той же цепочке.
Пример. Все потоки LAN>INET проверяются по одной цепочке 05_LAN_INET.
Чтобы часть потоков идущих через eth7 проверялась по набору правил, отличному
от правил для потоков идущих через eth5, можно для потоков LAN>*-eth3>INET задать
имя цепочки потока, отличное от автоматически сгенерированного '05_LAN_INET'.
А в правилах для этого потока дополнительно уточнять исходящий интерфейс eth7. 
</li>
<li><b>mangle_chain</b> - задает имя цепочки в таблице mangle, которое будет 
использовано при генерации правила для данного потока(ов).
Использование аналогично <b>filter_chain</b>.
Чтобы выполнить классификацию трафика для потоков LAN>*-eth5>INET не так
как для LAN>*-eth7>INET можно задать мия для этих потоков.
</li>
<li><b>count</b> - разрешает автоматическую генерацию счетчиков на основе 
разрешающих правилам потока(ов). Допустимы три значения:
  <ul>
  <li><b>direct</b> - вызывает генерацию правил-счётчиков <b>в данном потоке</b> 
  по разрешающим правилам <b>для данного потока</b>; 
  </li>
  <li><b>back</b> -  - вызывает генерацию правил-счётчиков <b>в обратном потоке</b> 
  по разрешающим правилам <b>для данного потока</b>;  
  </li>
  <li><b>both</b> - вызывает генерацию правил-счётчиков <b>в данном и обратном 
  потоках</b> по разрешающим правилам <b>для данного потока</b>;  
  </li>
  </ul> 
</li>
</ul>
</p>

<p>
Пример, чтобы было понятнее про параметр <b>count</b>.<br>
В сети <b>LAN</b> есть хост с адресом <i>192.168.0.13</i> и ему разрешён доступ в сеть <b>INET</b> (Интернет).
Для этого в цепочку потока <b>05_LAN_INET</b> добавляется ALLOW-правило с адресом источника
<i>192.168.0.13</i>. 
</p>

<p>
Теперь необходимо посчитать сколько данный хост получает трафика из сети Интернет
(входящий трафик).
Трафик к этому хосту из сети Интернет идет в обратном направлении и проверяется по
цепочке <b>06_INET_LAN</b> обратной для <b>05_LAN_INET</b>. Естественно правила-счетчики для подсчёта 
необходимо вставить в цепочку <b>06_INET_LAN</b> (а не <b>05_LAN_INET</b>), да еще и адрес 
<i>192.168.0.13</i> указать в назначении, а не в источнике. Вот такие правила и 
генерируются при  <b>count => 'back'</b>. Другими словами: "сгенерируй для адреса, которому
мы разрешили выход в сеть, правила для подсчета трафика, получаемого им из этой сети".
</p>

<p>
Если необходимо подсчитать исходящий трафик для <i>192.168.0.13</i>, то правила
естественно надо размещать в <b>05_LAN_INET</b>, т.е. в той же цепочке, где и само разрешающее
правило. Для этого используется <b>count => 'direct'</b>. 
</p>

<p>
И наконец, если для хоста <i>192.168.0.13</i> необходимо подсчитать и исходящий 
и входящий трафик, то ставим <b>count => 'both'</b> и счетчики для <i>192.168.0.13</i>
генерируются для <i>192.168.0.13</i> в обоих цепочках: в <b>05_LAN_INET</b> с src <i>192.168.0.13</i>,
а в <b>06_INET_LAN</b>  с dst <i>192.168.0.13</i>. 
</p>

<br>
<h4 id="structure_cfg_hosts">Файл описания правил для хостов и сетей (аkа hosts.conf)</h4>
<p>
Назначение данного файла - описать метаданные, разрешающие и запрещающие правила, 
скоростные ограничения и т.п. как для отдельных хостов какой-либо сети из указанных 
в <b>networks.conf</b>, так и, возможно, для сети в целом.
<b>В конфигурации количество файлов hosts.conf может быть произвольным</b> и не обязательно
совпадать с количеством сетей, описанных в <b>networks.conf</b>: код конфигурации
написан так, что правила можно группировать и размещать в произвольном количестве
файлов типа hosts.conf.
</p>
<p>
Например, можно использовать один файл net.conf, в котором будут описаны абсолютно все правила,
можно использовать несколько файлов net.conf - по одному на сеть,
можно сгруппировать правила в разные net.conf по отделам, кабинетам или по как-либо другим признакам. 
</p>

<p>
С точки зрения синтаксиса данный файл также представляет собой Perl-скрипт,
настройки в котором описаны в массиве <i>@items</i>.
О плюсах и минусах такого подхода см. выше.
</p>

<br>
<p id="structure_cfg_hosts_syntax">
<b>Далее описан синтаксис файла описания правил для хостов и сетей.</b><br>
В качестве примера смотри файл <b>/etc/hlf/conf.d/main/conf/lan.conf</b> в исходниках.
</p>

<p>
Файл должен иметь синтаксис perl-скрипта.
В файле должен быть один раз описан массив с зарезервированным именем <i>@items</i>.
Каждый элемент массива <i>@items</i> описывает метаданные, правила фильтрации и 
управления трафиком для хоста или некоей логически связанной группы ip-адресов
одной из сетей, описанных в  networks.conf.
Порядок элементов в <i>@items</i> важен и напрямую влияет на порядок генерируемых правил. 
</p>

<p>
Настройки из файла обрабатываются на уровне структур данных perl'а в runtime,
а не путем его синтаксического разбора. Поэтому, в принципе,  массив <i>@items</i>
может быть создан любым способом, а не только указанным ниже объявлением массива.  
</p>

<p>
По этой же причине задание формального синтаксиса данного файла несколько нелепо, 
но с другой стороны позволяет кратко описать все поддерживаемые настройки: 
</p>

<pre class="cmd">
(* hosts.conf EBNF *)

qot = '"'|"'"; (* кавычки для perl-строк *)

(* общая структура файла и элементов описания *)
file = [ code ], items, [ code ];
code = ? perl code ?;
items = "@items", "=", "(", [entries], ")";
entries = entry, {",", entry};
entry = "{", [id-records, [",",  rules]], "}";

(* идентификационные метаданные *)
id-records = id-record, {"," , id-record};
id-record = id-name, "=>", id-value;
id-name = "name" | "ou" | "ln" | "fn" | "mn" | "email";
id-value = ? perl string ?; 

(* набор правил *)
rules = "rules", "=>", "[", [ruleset], "]"; 
ruleset = rule, {"," , rule};
rule = "{", default-entry | rule-entries, "}";
default-entry = ( "dfltg" | "dflt" | "dflt0" ), "=>", "{", rule-entries, "}";
rule-entries = match-part | ipset, [",", match-part]; 

(* проверка на совпадение и вердикт *)
match-part = match-net, [",", match-entries], [",", verdict], [",", classify],
                   [",", speed], [",", target], [",", targopts])]
match-net = "srcnet", "=>", source-net, ",", "dstnet", "=>", destination-net; 
match-entries = match-entry, {"," match-entry};
match-entry = match-key, "=>", match-value;
match-key = "srcip" | "srcset" | "dstip" | "dstset" | "proto" | "sport" | "dport" | "ext" | "extp";
match-value = ? perl string ?; 
verdict = "'", "allow" | "drop" | "deny" | "exclude" | "limit", "'";

(* дополнительная конфигурация *)
(* ipset *)
ipset = "ipset =>", qot, ? ipset name ?, qot ;

(* classify *)
classify = "classify", "=>", "{", (classid | class-script) ,"}";
classid = "classid", "=>", qot, ? tc htb classid ?, qot;
class-script = "script", "=>", script-name, [",", "params", "=>", "{", param-list, "}"];
script-name = ? perl string ?;
param-list = ? perl hash ?;

(* speed *)
speed = "speed", "=>", "{", speed-entry ,"}"; 
speed-entry = dir, [",", in-out], [",", parent], [",", classid], [",", rate], 
              [",", ceil], [",", script];
dir = "dir", "=>", qot, ("up" | "down"), qot;
in-out = ("in" | "out"), "=>", qot, ? interface name ?, qot              
parent = "parent", "=>", qot, ? tc htb parent class ?, qot;
classid = "classid", "=>", qot, ? tc htb classid ?, qot;
rate = "rate", "=>", qot, ? tc htb rate value ?, qot;
ceil = "ceil", "=>", qot, ? tc htb ceil value ?, qot;

(* target *)
target = "target =>", qot, ? iptables targets ?, qot;
targopts = "targopts =>", qot,? iptables targets options ?, qot 
</pre>

<br>
<h5>Идентификационные метаданные</h5>
<p>
Элемент <b>id-records</b>: позволяет задать наименование правила/хоста,
организацию, фамилию, имя, отчество, почтовый адрес пользователя хоста.
Данный элементы используются только для информационных целей, в отчетах и
их значения никак не влияют на конфигурацию фаервола;
Все элементы являются необязательными. Например:
</p>

<pre class="cmd">
{name => 'boss',
    ou    => 'Organization',
    ln    => 'LastName',
    fn    => 'FirstName',
    mn    => 'MiddleName',
    email => 'box\@host.domain',
    ...
}
</pre>

<br>
<h5>Набор правил</h5>
<p>
Элемент <b>rules</b> описывает некий произвольный набор правил (<b>ruleset</b>).
Каждое правило может задавать либо значения по умолчанию (<b>default-match-entries</b>),
либо собственно значения параметров для данного правила (<b>match-rule</b>). Порядок генерации 
правил совпадает с порядком их записи.
</p>

<p>
<b>default-entry</b> предназначены для сокращение записи правил и по сути 
являются специализированными макросами - их элементы подставляются в правила. 
При этом как-то явно указывать в правилах использование этих макросов не надо: 
после объявления значения соответствующих макросов 
подставляются автоматически в каждое последующее правило. По сути происходит 
объединение элементов текущего хэша (правила) с элементами другого хэша (макроса).
</p>
<div class="excl">Объявление макроса не вызывает генерацию правил(а)!</div>  

<p>
Данные макросы могут быть трех видов:
<ul>
<li><b>dfltg</b> - область видимости данного макроса: от места объявления до конца файла.
   Значение по умолчанию: пустой perl-хэш;</li>
<li><b>dflt</b> - область видимости данного макроса: от места объявления до конца <b>ruleset</b>.
   Значение по умолчанию равно макросу (хэшу) <b>dfltg</b>; Макрос аддитивный, 
   т.е. при каждом последующем объявлении значения предыдущего объявления 
   объединяются со значениями текущего, а не переопределяются;</li>
<li><b>dflt0</b> - обнуляет макрос <b>dflt</b> и присваивает ему указанные значения.</b>.
   Эквивалентно последовательности:
   <pre class="cmd">
   {dflt => {}},
   {dflt => {key => new-value, ...}};
   </pre>
</li>
</ul>
В следующем примере в комментариях указаны эффективные значения макросов и правил:
</p>
<pre class="cmd">
{name => 'default rule`s entries',
    rules => [{dfltg => {srcnet => 'lan', dstnet => 'inet'}}]
}, 
{ ...
  rules => [
      {dflt => {verdict => 'allow'}},            # <b>dflt</b> теперь = {srcnet => 'lan', dstnet => 'inet', verdict => 'allow'} 
          {srcip => '192.168.7.10'},             #               {srcnet => 'lan', dstnet => 'inet', verdict => 'allow', srcip => '192.168.7.10'}
          {srcip => '192.168.7.14'},             #               {srcnet => 'lan', dstnet => 'inet', verdict => 'allow', srcip => '192.168.7.14'}
      {dflt => {proto => 'tcp', dport => '25'}}, # <b>dflt</b> теперь = {srcnet => 'lan', dstnet => 'inet', verdict => 'allow', proto => 'tcp', dport => '25'}
          {srcip => '192.168.8.88'},             #               {srcnet => 'lan', dstnet => 'inet', verdict => 'allow', proto => 'tcp', dport => '25', srcip => '192.168.8.88'}
          {srcip => '192.168.8.89'},             #               {srcnet => 'lan', dstnet => 'inet', verdict => 'allow', proto => 'tcp', dport => '25', srcip => '192.168.8.89'}
          {srcip => '192.168.8.90'},             #               {srcnet => 'lan', dstnet => 'inet', verdict => 'allow', proto => 'tcp', dport => '25', srcip => '192.168.8.90'}
      {dflt0 => {verdict => 'allow', srcnet => 'vpn', dstnet => 'lan',
                 srcip => '192.168.255.5'}},     # <b>dflt</b> теперь = {verdict => 'allow', srcnet => 'vpn', dstnet => 'lan', srcip => '192.168.255.5'}
          {dstip => '192.168.7.10'},             #               {verdict => 'allow', srcnet => 'vpn', dstnet => 'lan', srcip => '192.168.255.5', dstip => '192.168.7.10'}
          {dstip => '192.168.7.14'},             #               {verdict => 'allow', srcnet => 'vpn', dstnet => 'lan', srcip => '192.168.255.5', dstip => '192.168.7.14'}
          {}                                     #               {verdict => 'allow', srcnet => 'vpn', dstnet => 'lan'}
  ]
}
</pre>
<p>
<b>rule-entries</b> позволяет задать элемент списка ipset, и/или правило 
соответствия (<b>match-part</b>). 
</p>

<br>
<h5>Проверка на совпадение и вердикт</h5>
<p>
<b>match-part</b> позволяет задать сети (<b>match-net</b>),  
вердикт (<b>verdict</b>), правило классификации пакетов (<b>classify</b>),
скоростные ограничения (<b>speed</b>), а также явно указать iptables target 
с опциями (<b>target</b> и <b>targopts</b>).
</p>
<p>
В EBNF <b>match-net</b> выделен отдельно из <b>match-part</b> только потому что является
обязательной частью match-правила:  
<li><b>srcnet</b> - задает имя сети-источника;</li>
<li><b>dstnet</b> - задает имя сети-назначения.</li>
</p>
<p>
<b>match-entries</b> по сути позволяют задать базовые параметры и параметры расширений 
правила iptables. Причем, если для базовых параметров предусмотрены отдельные
элементы хэша (<b>srcip, srcset, dstip, dstset, proto, sport, dport</b>), 
то параметры расширений задаются прямо строковым фрагментом правила
iptables в одном элементе хэша <b>ext</b>. Элемент же <b>extp</b> служит для сборки
строки расширения из нескольких правил: в начало <b>extp</b> данного правила добавляется
содержимое <b>ext</b> или <b>extp</b> из предыдущего.<br>
Пример: 
<pre class="cmd">
    {dflt => {proto => 'tcp', dport => '25', <b>ext</b> => '-m conntrack --ctstate NEW'}},
(1) {verdict => 'limit', target => 'RETURN', <b>extp</b> => '-m hashlimit --hashlimit-upto 2/m --hashlimit-burst 5 '.
                                                       '--hashlimit-mode srcip,dstip --hashlimit-name laninet25'
    },
(2) {verdict => 'limit', target => 'WHATEVER', <b>extp</b> => 'bla-bla-bla'},
(3) {verdict => 'limit', target => 'WHATEVER', <b>ext</b> => 'stop it'},

в (1) мы будем уже иметь ext = '-m conntrack --ctstate NEW -m hashlimit --hashlimit-upto 2/m --hashlimit-burst 5 --hashlimit-mode srcip,dstip --hashlimit-name laninet25' 
в (2) ext = '-m conntrack --ctstate NEW -m hashlimit --hashlimit-upto 2/m --hashlimit-burst 5 --hashlimit-mode srcip,dstip --hashlimit-name laninet25 bla-bla-bla'  
в (3) ext = 'stop it'  
</pre>
<ul>
<li><b>srcip</b> - задает IPv4-адрес источника в сети <b>srcnet</b>;</li>
<li><b>srcset</b> - задает имя набора ipset по которому проводится проверка источника пакета;</li>
<li><b>dstip</b> - задает IPv4-адрес назначения в сети <b>dstnet</b>;</li>
<li><b>dstset</b> - задает имя набора ipset по которому проводится проверка приёмника пакета;</li>
<li><b>proto</b> - протокол (см. iptables --protocol);</li>
<li><b>sport</b> - порт источника (см. iptables --sport);</li>
<li><b>dport</b> - порт назначения (см. iptables --dport);</li>
<li><b>ext</b> - позволяет задать match-часть команды iptables. Задумана для задания match-расширений iptables;</li>
<li><b>extp</b> - то же, что и <b>ext</b>, но значение ключа конкатенируется со значением 
<b>ext/extp</b> предыдущего правила, если таковое существует.</li>
</ul>
</p>
<p>
Назначение <b>verdict</b> двояко: 
<ul>
<li>задает смысловую суть правила и как следствие специализированную цепочку в которую 
оно будет размещено;
</li>
<li>
для части вердиктов одновременно определяет и "судьбу" пакета в том же смысле что и iptables targets.
</li>
</ul>
</p>

<p>
Таким образом:
<ul>
<li>
<b>allow , drop, deny</b> - задают для данного правила iptables target ACCEPT,
DROP, DENY соответственно. Генерация правил происходит в предназначенные для этого цепочки;
</li>
<li>
<b>exclude</b> позволяет указать, что генерация правил должна осуществляться в цепочку, 
предназначенную для исключений из запретов. 
В отличие от предыдущего варианта <b>exclude только</b> указывает функциональное
назначение правила, но не указывает исключением из каких именно запрещающих правил 
является данное исключение. Это задается собственно параметрами как запрещающего так
и исключающего правила по необходимой вам логике. Другими словами запрет и исключение 
из него теперь связаны только логически содержимым правил - можете писать что угодно. 
<br>Ранее было:
</li>
<pre class="cmd">
    deny  => [{proto => 'tcp', dport => '25',
               'log-prefix' => 'LAN_INET_DENY_SMTP: ', 'limit' => '60/hour', 'limit-burst' => 5,
                exclude => [
                    {srcip => '192.168.1.1', proto => 'tcp', dport => '25'},
                    {dstip => 'smtp.yandex.ru', proto => 'tcp', dport => '25'}
                ] 
              }
    ] 
</pre>
,а теперь:
<pre class="cmd">
(1)    {dflt => {proto => 'tcp', dport => '25'}},
(2)        {verdict => 'drop', target => 'LOG', targopts => '--log-prefix LAN_INET_DENY_SMTP:',
                                        ext =>  '-m limit --limit 60/hour --limit-burst 5'},
(3)        {verdict => 'drop'},
(4)    {dflt => {verdict => 'exclude'}},
(5)        {srcip => '192.168.1.1'},
(6)        {dstip => 'smtp.yandex.ru'}

(1) - задаем для дальнейших правил tcp/25 (SMTP), (2) логируем пакеты с ограничением
на частоту логирования и (3) игнорируем, за (4) исключением пакетов 
(5)с адреса 192.168.1.1 на dport tcp/25 и (6) на адрес smtp.yandex.ru dport tcp/25.   
</pre>
<li>
<b>limit</b> позволяет указать, что генерация правил должна осуществляться в цепочку, 
предназначенную для лимитирования темпа создания новых соединений. 
В отличие от предыдущего варианта <b>limit только</b> указывает фунциональное
назначение правила, но не вызывает генерацию последовательности лимитирующих и 
логирующих правил. Собственно правила необходимо описывать явно. 
<br>Ранее было одно правило:
<pre class="cmd">
  (1) {srcnet => 'lan', dstnet => 'inet', name => '',
          limit => [{proto => 'tcp', dport => '25',
              'hashlimit' =>'2/m', 'hashlimit-burst' => 5, 'hashlimit-mode' => 'srcip,dstip',
              'hashlimit-name' => 'laninet25', 'loglimit' => '6/h', 'loglimit-burst' => 5,
              'log-prefix' => 'LAN->INET_SMTP_limit:'},
          ]
      }
</pre>
,а теперь:
<pre class="cmd">
  {dflt => {proto => 'tcp', dport => '25', ext => '-m conntrack --ctstate NEW'}},

(1)   {verdict => 'limit', target => 'RETURN', extp => '-m hashlimit --hashlimit-upto 2/m --hashlimit-burst 5 '.
                                                       '--hashlimit-mode srcip,dstip --hashlimit-name laninet25'
      },
(2)   {verdict => 'limit', target => 'LOG', targopts => '--log-prefix LAN->INET_SMTP_limit:', extp => '-m limit --limit 6/h --limit-burst 5'},
(3)   {verdict => 'limit', target => 'DROP'}

(1) возвращаем на дальнейшие проверки, все что не превысило лимит, (2) логируем 
и (3) игнорируем все пакеты, превысившее лимит
</pre>
Т.е. если ранее одно правило <b>limit</b> автоматически разворачивалось в 
последовательность правил iptables, зашитую в коде, то теперь правила должны быть 
явно прописаны вручную. 
</li>
</ul>
Для избежания повторов, в будущем предполагется использование макроподстановки с параметрами.
</p>

<br>
<h5>Дополнительная конфигурация</h5>

<p>
<b>ipset</b> позволяет задать элемент набора ipset с именем "ipset name".
Набор ipset создается автоматически. Можно задавать элементы которые будут отображаться
на разные типы наборов ipset: кол-во и типы наборов определятся автоматически исходя
из типа и сочетания указанных элементов.
<b>На данный момент автоматическая генерация поддерживается только комбинации <i>ip/protocol/port</i>.</b> 
<div class="excl">
Поэтому правила, в которых используются наборы ipset должны следовать после задания 
всех элемнтов используемого набора ipset. Только в этом случае можно однозначно
определить сколько наборов и каких типов необходимо сгенерировать и 
для скольки наборов нужно сгенерировать правила iptables в том или ином месте. 
<br>
Пример:  
</div>
<pre class="cmd">
{name => 'one',
    rules => [
        {ipset => 'lan-usr', srcip => '192.168.1.23'}
    ]
}, 
{name => 'two',
    rules => [
        {ipset => 'lan-usr', srcip => '192.168.1.24', proto => 'udp'}
    ]
}, 
{name => 'allow lan-usr',
    rules => [
        {verdict => 'allow', srcset => 'lan-usr'}, # разрешаем 'lan-usr' и все 'поднаборы' 
    ]
} 
автоматически сгенерирует два набора: 'lan-usr' с 192.168.1.23 и 'lan-usr-pr' c 192.168.1.24/udp,
а так же сгенерирует в цепочке *_ALLOW два (а не одно) разрешающих правила для обоих наборов: 

ACCEPT     all  --  *      *       0.0.0.0/0            0.0.0.0/0            match-set lan-usr src ctstate NEW 
ACCEPT     all  --  *      *       0.0.0.0/0            0.0.0.0/0            match-set lan-usr-pr src ctstate NEW 
</pre>
</p>

<p>
Элемент <b>classify</b> позволяет либо задать HTB-класс (<b>classid</b>) для пакета,
соответствующей комбинации<b> match-net</b>+<b>match-entries</b>, либо имя пользовательского perl-скрипта (<b>class-script</b>),
который генерирует нужный набор классифицирующих правил
в восходящем или нисходящем направлениях. Посредством хэша <b>params</b> возможна 
передача параметров пользовательскому скрипту.  
</p>

<p>
Элемент <b>speed</b> позволяет задать скоростные ограничения для трафика соответствующего
<b>match-net</b>+<b>match-entries</b> данного правила в восходящем или нисходящем направлениях:
<ul>
<li><b>dir</b> - направление, для которого происходит ограничение. Обязательный параметр. Допустимы два значения:</li>
<ul>
<li><b>down</b> - генерируются классифицирующие правила для нисходящего потока;</li>
<li><b>up</b> - генерируются классифицирующие правила для восходящего потока;</li>
</ul>
<li><b>in-out</b> - наименование исходящего интерфейса, на котором будет осуществляться данное ограничение:</li>
<li><b>rate</b> - гарантированная скорость (см. HTB rate). Обязательный параметр;</li>
<li><b>ceil</b> - максимальная скорость (см. HTB ceil). Обязательный параметр;</li>
<li><b>parent</b> - идентификатор родительского класса, к кторому будет прикреплен
данный (см. tc class parent). Обязательный параметр;</li>
<li><b>classid</b> - идентификатор данного класса (см. tc class classid);</li>
<li><b>script</b> - имя пользовательского perl-скрипта для генерации классифицирующих правил.
При задании скрипта <b>не происходит генерации</b> правила классификации.</li>
</ul>
</p>
<p>
Подразумевается использование HTB-дисциплины на целевом интерфейсе с заранее 
сконфигурированной (напр. отдельным скриптом) статической иерархией классов, в
которую и будут добавляться генерируемые правила. Соответственно поддерживается
управление трафиком только на исходящем интерфейсе, policing на входящем не поддерживается. 
</p>
<p>
Каждое скоростное правило вызывает генерацию одного классифицирущего правила для
для таблицы mangle iptables и одного правила создания класса для команды tc. 
</p>

<p>
Элементы <b>target</b> и <b>targopts</b> позволяют явно задать или переопределить
параметр target и его опции в генерируемой для данного правила iptables-команде.
</p>


<h3 id="structure_cfg_main">Конфигурация main</h3>
<p>
Конфигурация <i>main</i> собственно и является конфигурацией фаервола, в которой
реализованы подходы, рассмотренные в первой части.
</p>

<p>
Основная конфигурация, рассматриваемая в данном тексте называется <b>main</b> и 
расположена она соответственно в <b>/FW/conf.d/<i>main</i></b>.
В этом же каталоге, наряду с обязательным скриптом start расположены и другие 
вспомогательные скрипты, задающие некоторые наборы правил, перезапускающие firewall по частям и т.д.
Конфигурационные же файлы, для удобства, вынесены в отдельный подкаталог /FW/conf.d/main/conf.
</p>
<p>
Данное разделение является произвольным и определяется, тем какой именно код вы напишите в 
скрипте start и других скриптах (если таковые будут). Другими словами количество
скриптов, конфигурационных файлов (кроме обязательных) и их расположение внутри каталога
конфигруции абсолютно произвольное.
</p>

<p>
Конфигурация <i>main</i> расположена в каталоге <b>/FW/conf.d/main</b> и состоит из скриптов
и конфигурационных файлов, расположенных в каталоге <b>/FW/conf.d/main/conf</b> .
</p>

<h4 id="structure_cfg_mian_startup">Процесс загрузки конфигурация <b>main</b></h4>
<p>
После загрузки ядра система инициализации выполняет скрипты для текущего <i>runlevel</i>.
Для запуска фаервола при загрузке ОС добавляем символическую ссылку на <b>/FW/modules/fw-sysv</b>,
например в <i>/etc/rc3.d</i>, который собственно и запускает скрипт <b>/FW/start</b>.
Так как при этом скрипту <b>/FW/start</b> никакого имени конфигурации
не передается, то <b>/FW/start</b> запускает скрипт 
<b>/FW/conf.d/<i>default</i>/start</b>.
При этом ссылка <i>default</i> указывает на <b>/FW/conf.d/<i>main</i></b>.
</p>

<p>
Perl-скрипт <b>/FW/conf.d/main/start</b> уже является специфичным для конфигурации
main и через вызовы вспомогательных функций конфигурирует фаервол под конкретные задачи. 
Если вы захотите написать свою собственную конфигурацию, отличную по количеству
сетей, интерфейсов, конфигурационных файлов, то в этот файл надо будет вносить изменения.
</p>

<p>
В скрипте <b>/FW/conf.d/main/start</b>, после подготовительных действий и объявления переменных идет вызов
функции <b>check_confs</b> из <b>/FW/modules/firewall.pm</b>.
Эта функция просто проверят правильность синтаксиса всех perl-скриптов, находящихся
в <b>/FW/conf.d/main/conf/</b>. Файлы проверяются только на соответствие 
синтаксису языка - никакой смысловой проверки не проводится.  
</p>
<p>
Далее выполняется функция <b>init</b> из того же firewall.pm, которая должна инициализировать
конфигурацию интерфейсов шлюза и сетей, подключенных к ним. Для этого ей передается имя файла конфигурации сетей,
в данном случае <b>/FW/conf.d/main/conf/networks.conf</b>. 
Так как подразумевается что это просто perl-скрипт (см. <a href="#structure_cfg_net">Файл, описывающий шлюз и сети</a>), 
то данный файл просто выполняется как обычный perl-скрипт. 
Т.о. создается и инициализиурется хэш %networks.	
</p>
<p>
Следующим выполняется bash-скрипт <b>из другой конфигурации</b>
<b>/FW/conf.d/<i>outbound_only</i>/start</b>, который 
загружает временный набор правил, позволяющих шлюзу делать исходящие соединения.
Это необходимо, например, для разрешения DNS-имён.
</p>
<p>
Затем запускается скрипт <b>/FW/modules/wait_dns</b>, который задерживает
дальнейшее выполнение кода до момента, когда станет доступен DNS-сервер провайдера.
Подразумевается, что доступность DNS-сервера провайдера означает, что xDSL-модем №1
загрузился и установил соединение. Это необходимо в случае если запуск фаервола происходит после
сбоя питания и модем включился одновременно с шлюзом (шлюз загружается быстрее, 
чем модем устанавливает соединение). После этого становится возможным разрешение 
имён хостов и дальнейшее выполнение скрипта фаервола. Если после некоторого числа
попыток DNS-сервер так и остается недоступным выполнение скрипта продолжается.
</p>

<p>
<div class="excl">
Так как фаервол конфигурируется с использованием iptables-restore, любая ошибка,
возникшая в момент загрузки набора правил, приведет к отмене загрузки данного набора правил.
<b>В том числе и при ошибках (невозможности) разрешения доменных имён, если таковые
есть в правилах вместо IP адресов.</b>
Т.е. недоступность DNS-сервера или какие-то другие ошибки разрешения имён 
(ошибки в имени, например) в момент загрузки правил, приведут к тому, что фаервол
не будет сконфигурирован как положено, что чревато нерабоспособностью шлюза.   
Поэтому лучше не использовать в правилах доменные имена вместо IP адресов.
</div>
</p>

<p>
Далее bash-скрипт <b>/FW/modules/load_modules</b> загружает необходимые модули
ядра, а <b>/FW/conf.d/main/sysctl</b> устанавливает значения некоторых переменных ядра,
необходимые для данной конфигурации.  
</p>

<p>
И затем выполняется bash-скрипт <b>/FW/conf.d/main/via_isp</b>, который 
настраивает маршрутизацию через первого или второго провайдера, в зависимости 
от значения переменной в <b>/FW/conf.d/main/conf/via_isp.conf</b>.
</p>

<p>
Вслед за этим четыре раза вызывается функция <b>flow</b> - для каждого из имеющихся 
conf-файлов, описывающих хосты и правила для них. В результате вызовов данной функции
в массивы <i>@filter_rules, @mangle_rules, @nat_rules, @raw_rules</i> сгенерированными строчками 
команд для iptables-restore.
</p>

<p>
Затем, с помощью функции <b>add_rules_from_file</b> к списку уже сгенерированных на данном
этапе правил добавляются правила из файлов <b>/FW/conf.d/main/conf/rules_nat_redirect</b>, 
<b>rules_nat_snat</b> и <b>rules_route</b>. Правила в этих файлах написаны вручную.
</p>

<p>
Функция <b>gen_main_rules</b> генерирует все правила переходов в цепочки потоков
из встроенных цепочек <b>INPUT, OUNPUT, FORWARD</b> для таблиц <i>filter</i> и
<i>mangle</i>.
</p>

<p>
После этого из файла <b>rules_main_extra</b> добавляются правила, написаные вручную для
цепочек <b>INPUT, OUNPUT, FORWARD</b> таблиц <i>filter</i> и <i>mangle</i>.
</p>

<p>
Далее, в фукнции <b>finalize</b> генерируются правила DROP, добавляемые в конец 
каждой ALLOW-цепочки и правила переходов в DROP-цепочки,добавляемые в конец 
каждой SKIP-цепочки. Отдельно генерируются переходы в цепочки счетчиков *_CNT.  
</p>

<p>
К этому моменту полный набор сгенерированных правил iptables находится в 
массивах <i>@filter_rules, @mangle_rules, @nat_rules, @raw_rules</i>.
Вызов функции <b>load_rules</b> загружает эти правила в ядро при помощи команды 
iptables-restore. 
</p>

<p>
Далее вызов bash-скрипта <b>tc-ifb0</b> конифигурирует HTB_дисциплину и 
основную иерархию классов на интерфейсе <i>ifb0</i>,
где происходит управление трафиком в нисходящем направлении.
Вызовы <b>tc-eth0</b> и <b>tc-eth2</b> перенаправляют
трафик с интерфейсов <i>eth0</i> и <i>eth2</i> на <i>ifb0</i>. 
Следом, аналогичная последовательность вызовов <b>tc-ifb1</b>,
<b>tc-eth1</b>, <b>tc-eth3</b> конфигурирует <i>ifb1</i>,
где происходит управление трафиком в восходящем направлении.
</p>

<p>
Ранее, при вызовах <b>flow</b> генерировались не только правила для iptables,
но и в массив <i>@tc_rules</i> генерировались tc-команды индивидуальных ограничений
для хостов сетей.
Теперь функция <b>load_tc_rules()</b> выполняет эти команды в дополнение к командам 
предыдущего пункта.  
</p>

<p>
Последняя функция <b>generate_save_counters</b> по настройкам в <i>networks.conf</i> 
генерирует из шаблона <b>/FW/modules/save_counters</b> скрипт сохранения значения счетчиков 
<b>/FW/conf.d/main/var/save_counters</b>, который далее регулярно вызывается через <i>cron</i>.  
</p>

<h4 id="structure_cfg_mian_funcs">Подробнее о функциях <b>firewall.pm</b></h4>

<h2 id="tc_faq">Вопросы без ответов</h2>
<p>
* В команде <i>tc filter ... police ... flowid major:minor</i> параметр flowid по документации
является обязательным (хотя на практике оказывается его можно не указывать).
Для egress задание flowid имеет смысл - задаем класс для трафика, который соответствует этому фильтру.
Но фильтр то можно привязать и к ingress!
В чём смысл flowid в фильтре, созданном в ingress !? 
</p>
<p>
* Major во всех командах для одной qdisc должен быть одинаковым?
</p>
<p>
* После <i>tc filter add dev eth0...action mirred egress redirect dev ifb0</i> весь трафик
из egress(?) <i>eth0</i> перенаправляется на ingress(?) <i>ifb0</i>.
А какой интерфейс в этом случае является "выходным", например для iptables?
По логике <i>ifb0</i>, хотя все таки, похоже <i>eth0</i>. 
</p>
<p>
* Если посмотреть в исходники tc (<i>m_police.c</i>) то там можно увидеть следующее 
недокументированное actions:
<pre class="cmd">
...
else if (matches(arg, "shot") == 0)
  res = TC_POLICE_SHOT;
</pre>
</p>

<p>
В <i>m_mirred.c</i> можно увидеть следующее:
<pre class="cmd">
fprintf(stderr, "Usage: mirred <DIRECTION> <ACTION> [index INDEX] <dev DEVICENAME> \n");
fprintf(stderr, "where: \n");
fprintf(stderr, "\tDIRECTION := <ingress | egress>\n");
fprintf(stderr, "\tACTION := <mirror | redirect>\n");
fprintf(stderr, "\tINDEX  is the specific policy instance id\n");
fprintf(stderr, "\tDEVICENAME is the devicename \n"); 
</pre>
Как это работает (т.е. смысл) нигде недокументировано.
Есть только заученная мантра 
'tc filter add dev ... action mirred egress redirect dev ifb0' 
для использования одной дисциплины на IFB.
Это что, можно "полисить" входящий трафик с нескольких интерфейсов одной дисциплиной?
И это реально работает? 
</p>

<p>
* Безклассовая (согласно любой документации) дисциплина TBF однако имеет класс:


<pre class="cmd">
tc qdisc add dev ifb0 root handle 4: tbf limit 1500 rate 160kbps burst 1600
tc qdisc show dev ifb0
tc class show dev ifb0 

получаем:

qdisc tbf 4: root refcnt 2 rate 1280Kbit burst 1600b lat 4295.0s 
<b>class tbf</b> 4:1 parent 4:  
</pre>
Что означает наличие класса в безклассовой дисциплине - непонятно. Например прикрепить
<b>к TBF-классу дочерние дисциплины</b> PRIO (как в примере http://blog.edseek.com/~jasonb/articles/traffic_shaping/scenarios.html#guarprio
) или HTB не получается. <b>Прикрепить дочерний класс</b> тоже непонятно можно ли, какой и зачем.  
</p>

<h2 id="todo">TODO</h2>
<h3>Текст</h3>
<p>
<s>- описать из-за stateful уже созданные соединения будут существовать несмотря на 
последующие запреты и их надо явно удалять (conntrack -D)</s>;<br />
<s>- упомянуть почему в реализации INET_SKIP_OR_DENY идут до RELATED,ESTABLISHED;</s><br />
- возможность подсчета трафика с помощью conntrackd?;<br />
- упомянуть про поток типа LAN>GW>LAN (маршрутизация через шлюз в одну и ту же сеть);<br />
- Резервирование и переключение - разаобраться, что можно сделать;<br />
- Policing закончить. Использование policing для входяещего трафика не дало нужных результатов;<br />
</p>
<h3>Код</h3>
<p>
- удаление возможных существующих соединений (conntrack -D) при отсутствии хотя бы одного разрешающего правила;<br />
- добавить возможность использования ipset. В штатном ядре Debian 7 уже есть ipset,
так что можно начинать прикручивать;<br />
- --multiport;<br />
- рефакторинг длинных процедур?;<br />
- реализовать _default;<br />
- в случае отсутствия DNS явно не загружать правила с именами вместо адресов - надо ли?;<br />
- откат к предыдущему состоянию фаервола в случае неудачной загрузки команд iptables;
  необходимо так как правила для отдельных таблиц загружаются независимо и возможна загрузка в таблицу filter 
  и затем ошибка при загрузке в mangle, но filter так и останется в новом состоянии<br /> 
<h4>Оптимизация</h4>
- оптимизация цепочек.  По возможности вставлять allow-правила прямо в цепочу потока;<br>
</p>
</div>	
</div>	
</body>
</html>